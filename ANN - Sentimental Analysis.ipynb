{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "FTq57oFRd76C"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "\n",
        "In this project, we sought to perform sentiment analysis, predicting the quality and difficulty ratings for professors on https://www.ratemyprofessors.com/ from the corresponding comment. A variety of approaches were taken, a summary of which is provided in the \"Accuracy Comparison Table\" at the end of this document. With respect to difficulty, the highest performing model employed GloVe embeddings and deep bidirectional LSTM cells, achieving an accuracy of ~38.03% on a validation subset. With respect to quality, the highest performing model employed GloVe embeddings and deep bidirectional GRU cells, achieving an accuracy of ~47.06% on a validation subset, with most models within 5% of this benchmark. The problem was also approached as a regression problem, with the lowest test loss using mean squared error on difficulty being ~.8127 from a model utilizing GloVe and deep bidirectional GRU cells and the lowest test loss on quality under the same terms being ~1.2845 from a model utilizing a bespoke embedding and a bidirectional GRU cell. In both cases, it was harder to predict difficulty rather than quality, possibly as a result of the former only spanning 5 values, 1 through 5, while the latter spanned 9, 1 through 5 in .5 increments, though fewer models were dedicated to difficulty in any case. Another interesting feature is that in many cases models tended to very quickly overfit on the training data, with validation accuracy quickly lowering after only one or two epochs. Overall, models spanned a variety of embeddings, dimensions, architectures, and data sources with varying results."
      ],
      "metadata": {
        "id": "xwYVM6tMtKd_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **APPROACH - 1 :**\n",
        "---\n",
        "## **RNN Model for Sentiment Analysis of Student Comments using GloVe Embeddings and Keras**\n",
        "---\n",
        "### This code is performing sentiment analysis on a dataset of student comments about professors, in order to predict the professor's rating and level of difficulty. It begins by importing necessary libraries, including pandas, numpy, and the Natural Language Toolkit (NLTK), and loading the data into a pandas dataframe from a CSV file.\n",
        "\n",
        "### The code then loads pre-trained GloVe embeddings to use in creating an embedding matrix for the tokenizer's word index. It cleans the text data by removing stopwords, numerical values, and symbols, and converts the text to sequences. It creates target variables for the professor's rating and level of difficulty, and splits the data into training and testing sets.\n",
        "\n",
        "### The code builds a recurrent neural network (RNN) model using the Keras API with an embedding layer, LSTM layer, and a dense softmax output layer. It trains the model on the augmented training data, and evaluates its performance on the augmented test data. Finally, it prints the test accuracy of the model.\n",
        "\n",
        "### Overall, this code is a machine learning pipeline for predicting professor ratings and levels of difficulty based on student comments, using an RNN model and pre-trained embeddings."
      ],
      "metadata": {
        "id": "-d7haG6KIA5Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8eMInuIzb4Cq",
        "outputId": "c6523017-19e0-40fe-cf04-f1befe1de535"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the required libraries.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding, Dropout\n"
      ],
      "metadata": {
        "id": "c1luwTieEKV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the data from .csv to data frame.\n",
        "df = pd.read_csv(r'/content/drive/My Drive/GROUP_PROJECT_2_ANN/Big Data Set from RateMyProfessor.com for Professors Teaching Evaluation/RateMyProfessor_Sample data.csv')"
      ],
      "metadata": {
        "id": "YNDWQR2ihvMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained GloVe embeddings\n",
        "embeddings_index = {}\n",
        "with open(r'/content/drive/My Drive/GROUP_PROJECT_2_ANN/glove.6B.100d.txt', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs"
      ],
      "metadata": {
        "id": "Njr1Qk3wNL12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove 'nan' values from 'comments' column\n",
        "df = df.dropna(subset=['comments'])\n",
        "\n",
        "# Remove stopwords, numerical values, and symbols from 'comments' column and convert to lowercase\n",
        "nltk.download('stopwords')\n",
        "stop_words = stopwords.words('english')\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = \" \".join([word for word in text.split() if word not in stop_words])\n",
        "    return text\n",
        "\n",
        "df['comments'] = df['comments'].apply(clean_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZ6npV2WEeDE",
        "outputId": "a0ce6b2b-c674-4553-96a2-195160e9fdc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set parameters for model\n",
        "MAX_NB_WORDS = 19114\n",
        "MAX_SEQUENCE_LENGTH = 250\n",
        "EMBEDDING_DIM = 100\n",
        "\n",
        "# Tokenize words in 'comments' column\n",
        "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True)\n",
        "tokenizer.fit_on_texts(df['comments'].values)\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "# Convert text to sequences\n",
        "X = tokenizer.texts_to_sequences(df['comments'].values)\n",
        "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "# Create target variables\n",
        "y_star = pd.get_dummies(df['student_star']).values\n",
        "y_difficult = pd.get_dummies(df['student_difficult']).values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9PKjxr2eEfod",
        "outputId": "89cc3143-1ae5-43d8-a13f-ecb9560fe43f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 19113 unique tokens.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an embedding matrix for the tokenizer's word index\n",
        "num_words = len(tokenizer.word_index) + 1\n",
        "embedding_dim = 100\n",
        "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "id": "6KqjfbSENYZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into training and testing sets\n",
        "VALIDATION_SPLIT = 0.2\n",
        "indices = np.arange(X.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "X = X[indices]\n",
        "y_star = y_star[indices]\n",
        "y_difficult = y_difficult[indices]\n",
        "num_validation_samples = int(VALIDATION_SPLIT * X.shape[0])\n",
        "\n",
        "X_train = X[:-num_validation_samples]\n",
        "y_train_star = y_star[:-num_validation_samples]\n",
        "y_train_difficult = y_difficult[:-num_validation_samples]\n",
        "X_test = X[-num_validation_samples:]\n",
        "y_test_star = y_star[-num_validation_samples:]\n",
        "y_test_difficult = y_difficult[-num_validation_samples:]"
      ],
      "metadata": {
        "id": "3aouemkEEkVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_star.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUCXV0jyC-To",
        "outputId": "73714bd8-01a3-44fd-fd27-88415171aae9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15995, 9)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build RNN model\n",
        "model = Sequential()\n",
        "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False))\n",
        "model.add(LSTM(64, dropout=0.2))\n",
        "model.add(Dense(9, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZtnfeshzEnxY",
        "outputId": "17d35954-b2e9-4321-caa1-d122512865a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 250, 100)          1911400   \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 64)                42240     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 9)                 585       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,954,225\n",
            "Trainable params: 42,825\n",
            "Non-trainable params: 1,911,400\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model on augmented data\n",
        "model.fit(X_train, y_train_star, validation_data=(X_test, y_test_star), epochs=20, batch_size=128)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMxTW-8WEskE",
        "outputId": "9429a1d9-3d7f-43f5-cd3b-70ae0ea7a361"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "125/125 [==============================] - 20s 27ms/step - loss: 1.8575 - accuracy: 0.3757 - val_loss: 1.7126 - val_accuracy: 0.4175\n",
            "Epoch 2/20\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 1.6825 - accuracy: 0.4241 - val_loss: 1.6671 - val_accuracy: 0.4217\n",
            "Epoch 3/20\n",
            "125/125 [==============================] - 2s 14ms/step - loss: 1.6282 - accuracy: 0.4299 - val_loss: 1.6071 - val_accuracy: 0.4312\n",
            "Epoch 4/20\n",
            "125/125 [==============================] - 2s 14ms/step - loss: 1.6033 - accuracy: 0.4345 - val_loss: 1.5913 - val_accuracy: 0.4342\n",
            "Epoch 5/20\n",
            "125/125 [==============================] - 2s 14ms/step - loss: 1.5850 - accuracy: 0.4424 - val_loss: 1.5837 - val_accuracy: 0.4365\n",
            "Epoch 6/20\n",
            "125/125 [==============================] - 2s 14ms/step - loss: 1.5726 - accuracy: 0.4463 - val_loss: 1.5871 - val_accuracy: 0.4367\n",
            "Epoch 7/20\n",
            "125/125 [==============================] - 2s 14ms/step - loss: 1.5594 - accuracy: 0.4478 - val_loss: 1.5871 - val_accuracy: 0.4362\n",
            "Epoch 8/20\n",
            "125/125 [==============================] - 2s 16ms/step - loss: 1.5423 - accuracy: 0.4559 - val_loss: 1.5744 - val_accuracy: 0.4407\n",
            "Epoch 9/20\n",
            "125/125 [==============================] - 2s 16ms/step - loss: 1.5291 - accuracy: 0.4579 - val_loss: 1.5789 - val_accuracy: 0.4427\n",
            "Epoch 10/20\n",
            "125/125 [==============================] - 2s 14ms/step - loss: 1.5164 - accuracy: 0.4588 - val_loss: 1.5732 - val_accuracy: 0.4412\n",
            "Epoch 11/20\n",
            "125/125 [==============================] - 2s 14ms/step - loss: 1.5074 - accuracy: 0.4648 - val_loss: 1.5799 - val_accuracy: 0.4337\n",
            "Epoch 12/20\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 1.4925 - accuracy: 0.4683 - val_loss: 1.5743 - val_accuracy: 0.4360\n",
            "Epoch 13/20\n",
            "125/125 [==============================] - 2s 14ms/step - loss: 1.4777 - accuracy: 0.4728 - val_loss: 1.5754 - val_accuracy: 0.4400\n",
            "Epoch 14/20\n",
            "125/125 [==============================] - 2s 14ms/step - loss: 1.4716 - accuracy: 0.4694 - val_loss: 1.5729 - val_accuracy: 0.4422\n",
            "Epoch 15/20\n",
            "125/125 [==============================] - 2s 15ms/step - loss: 1.4627 - accuracy: 0.4791 - val_loss: 1.5689 - val_accuracy: 0.4392\n",
            "Epoch 16/20\n",
            "125/125 [==============================] - 2s 16ms/step - loss: 1.4502 - accuracy: 0.4820 - val_loss: 1.5708 - val_accuracy: 0.4402\n",
            "Epoch 17/20\n",
            "125/125 [==============================] - 2s 14ms/step - loss: 1.4320 - accuracy: 0.4868 - val_loss: 1.5715 - val_accuracy: 0.4397\n",
            "Epoch 18/20\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 1.4298 - accuracy: 0.4869 - val_loss: 1.5845 - val_accuracy: 0.4402\n",
            "Epoch 19/20\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 1.4157 - accuracy: 0.4905 - val_loss: 1.5788 - val_accuracy: 0.4392\n",
            "Epoch 20/20\n",
            "125/125 [==============================] - 2s 14ms/step - loss: 1.4065 - accuracy: 0.4935 - val_loss: 1.5940 - val_accuracy: 0.4452\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f45ce390eb0>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate model on augmented test data\n",
        "score, acc = model.evaluate(X_test, y_test_star, batch_size=32)\n",
        "print('Test accuracy :', acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AzLAyMEMEuTP",
        "outputId": "cecb09e2-a82f-4260-9a8b-22f2dd4a26e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "125/125 [==============================] - 1s 7ms/step - loss: 1.5940 - accuracy: 0.4452\n",
            "Test accuracy : 0.4452226161956787\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **--- END OF SECTION 1 ---**"
      ],
      "metadata": {
        "id": "Z26nepr8hgj-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Aproach 2:**\n",
        "---\n",
        "## **Sentiment Analysis using GloVe Embeddings and GRU Model on RateMyProfessor Data**\n",
        "---\n",
        "### This code is a machine learning model built using Keras and TensorFlow to predict student ratings based on their comments about a professor. The code starts by importing necessary libraries and loading the data from a CSV file. It then loads pre-trained GloVe embeddings and uses them to create an embedding matrix for the tokenizer's word index. The text data is preprocessed by cleaning and tokenizing the comments column of the DataFrame. The data is then split into train and test sets, tokenized, and padded to have the same length. A GRU (Gated Recurrent Unit) model is created with two GRU layers, a dropout layer, and a dense layer. The model is then compiled with a different optimizer and trained on the train set for 5 epochs. Finally, the model is evaluated on the test set, and the accuracy is printed."
      ],
      "metadata": {
        "id": "E0uW0I0BIF2u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the required default libraries.\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import pad_sequences\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "3j6UfzDtII3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the data from .csv to data frame.\n",
        "data_frame = pd.read_csv(r'/content/drive/My Drive/GROUP_PROJECT_2_ANN/Big Data Set from RateMyProfessor.com for Professors Teaching Evaluation/RateMyProfessor_Sample data.csv')"
      ],
      "metadata": {
        "id": "FibB2Rb1IMC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained GloVe embeddings\n",
        "embeddings_index = {}\n",
        "with open(r'/content/drive/My Drive/GROUP_PROJECT_2_ANN/glove.6B.100d.txt', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs"
      ],
      "metadata": {
        "id": "V5gqqzKAIUYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows with null values in 'comments' column\n",
        "data_frame.dropna(subset=['comments'], inplace=True)\n",
        "\n",
        "# Select only the 'student_star' and 'comments' columns\n",
        "data_frame = data_frame[['student_star', 'comments']]"
      ],
      "metadata": {
        "id": "Mle4ZhH4ItHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "# Define a regex pattern to match only alphanumeric characters\n",
        "pattern = r'[^a-zA-Z0-9]'\n",
        "\n",
        "def clean_text(text):\n",
        "    # Remove non-word characters using the regex pattern\n",
        "    text = re.sub(pattern, ' ', text)\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "    # Split text into individual words\n",
        "    words = text.split()\n",
        "    # Remove stopwords using NLTK\n",
        "    words = [word for word in words if word not in stopwords.words('english')]\n",
        "    return words\n",
        "\n",
        "# Apply the clean_text function to the 'comments' column of the DataFrame\n",
        "data_frame['comments'] = data_frame['comments'].apply(lambda x: clean_text(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXF9IyF-JDLR",
        "outputId": "d93b55af-f45e-4ccf-e859-df3d715cafa4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_frame.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4BW352LwIfnz",
        "outputId": "d2782708-4134-4a24-da94-56d951049577"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(19993, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data_frame['comments'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdGT325UKlFw",
        "outputId": "b0665c3b-4228-4c6a-e17e-589d2071db19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0        [class, hard, two, one, gen, ed, knockout, con...\n",
            "1        [definitely, going, choose, prof, looney, clas...\n",
            "2        [overall, enjoyed, class, assignments, straigh...\n",
            "3        [yes, possible, get, definitely, work, content...\n",
            "4        [professor, looney, great, knowledge, astronom...\n",
            "                               ...                        \n",
            "19995               [great, sense, humor, love, parasites]\n",
            "19996    [really, nice, guy, really, funny, however, bi...\n",
            "19997    [parasitology, class, lot, work, makes, extrem...\n",
            "19998    [way, much, work, 1, credit, class, shegnoski,...\n",
            "19999    [extremely, easy, lab, teacher, quizzes, littl...\n",
            "Name: comments, Length: 19993, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into train and test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = data_frame['comments']\n",
        "y = data_frame['student_star']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "id": "34eL5VTOKmFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the comments column of the train and test sets\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "# Pad the sequences of the train and test sets to have the same length\n",
        "max_length = max([len(seq) for seq in X_train_seq])\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=max_length)\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=max_length)"
      ],
      "metadata": {
        "id": "9AZf7bWGLq3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an embedding matrix for the tokenizer's word index\n",
        "num_words = len(tokenizer.word_index) + 1\n",
        "embedding_dim = 100\n",
        "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "id": "EsHkf6phM78m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the GRU model\n",
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.Embedding(num_words, embedding_dim, weights=[embedding_matrix], input_length=max_length, trainable=False))\n",
        "model.add(keras.layers.GRU(128, return_sequences=True))\n",
        "model.add(keras.layers.Dropout(0.2))\n",
        "model.add(keras.layers.GRU(64))\n",
        "model.add(keras.layers.Dense(32, activation='relu'))\n",
        "\n",
        "# Output layer for 'student_star'\n",
        "model.add(keras.layers.Dense(6, activation='softmax', name='student_star'))\n",
        "\n",
        "# Compile the model with a different optimizer\n",
        "optimizer = keras.optimizers.RMSprop(lr=0.001)\n",
        "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcbxpKXwK2Bg",
        "outputId": "88f81ba0-94fa-408d-94c5-6d17b4f6f580"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.RMSprop.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 72, 100)           1370300   \n",
            "                                                                 \n",
            " gru (GRU)                   (None, 72, 128)           88320     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 72, 128)           0         \n",
            "                                                                 \n",
            " gru_1 (GRU)                 (None, 64)                37248     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 32)                2080      \n",
            "                                                                 \n",
            " student_star (Dense)        (None, 6)                 198       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,498,146\n",
            "Trainable params: 127,846\n",
            "Non-trainable params: 1,370,300\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model on the train set for more epochs\n",
        "history = model.fit(X_train_pad, y_train, epochs=5, batch_size=128, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "loss, accuracy = model.evaluate(X_test_pad, y_test)\n",
        "print('Test accuracy:', accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFbrgYSyLFb4",
        "outputId": "482f241c-568d-40e9-d77e-48f7c1aee1e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "88/88 [==============================] - 6s 26ms/step - loss: 1.4536 - accuracy: 0.3492 - val_loss: 1.3236 - val_accuracy: 0.4191\n",
            "Epoch 2/5\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 1.2943 - accuracy: 0.3972 - val_loss: 1.2479 - val_accuracy: 0.4334\n",
            "Epoch 3/5\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 1.2405 - accuracy: 0.4063 - val_loss: 1.2232 - val_accuracy: 0.4041\n",
            "Epoch 4/5\n",
            "88/88 [==============================] - 1s 11ms/step - loss: 1.2089 - accuracy: 0.4163 - val_loss: 1.2205 - val_accuracy: 0.4280\n",
            "Epoch 5/5\n",
            "88/88 [==============================] - 1s 11ms/step - loss: 1.1815 - accuracy: 0.4219 - val_loss: 1.2372 - val_accuracy: 0.3823\n",
            "188/188 [==============================] - 1s 6ms/step - loss: 1.2075 - accuracy: 0.3710\n",
            "Test accuracy: 0.37095698714256287\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **--- END OF SECTION 2 ---**"
      ],
      "metadata": {
        "id": "r6RKnWr2hbUN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Approach 3**\n",
        "\n",
        "---\n",
        "\n",
        "## **Sentiment Analysis using LSTM Model on RateMyProfessor Data**\n",
        "\n",
        "---\n",
        "\n",
        "### The code loads a dataset of professor reviews from RateMyProfessor.com and performs sentiment analysis using a deep learning LSTM model. The dataset is initially read from a CSV file and then augmented with additional reviews loaded from a JSON file. The reviews are preprocessed by removing stopwords, numerical values, and symbols from the 'comments' column and converting it to lowercase. The pre-trained GloVe embeddings are loaded and an embedding matrix is created for the tokenizer's word index. The data is then split into training and testing sets, and an LSTM model is built and trained on the training set using the categorical cross-entropy loss function and the Adam optimizer. The model's performance is evaluated on the testing set, and the accuracy is displayed.\n",
        "\n",
        "### Overall, this code implements a basic sentiment analysis on RateMyProfessor data, providing insights into the quality and difficulty of various professors based on student reviews."
      ],
      "metadata": {
        "id": "5FT1OzOicaeZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "wlMvQMGha9ir",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f72d56fa-17d2-4fed-9f88-ab2166a97df9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the required libraries.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding, Dropout"
      ],
      "metadata": {
        "id": "60UE5jPcdxWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#For more info on parsing json in pandas, see https://pandas.pydata.org/docs/reference/api/pandas.read_json.html\n",
        "# df_json = pd.read_json(r'/content/drive/My Drive/GROUP_PROJECT_2_ANN/all_reviews.json')\n",
        "# print(df_json[0][2])\n",
        "df = pd.read_csv(r'/content/drive/My Drive/GROUP_PROJECT_2_ANN/Big Data Set from RateMyProfessor.com for Professors Teaching Evaluation/RateMyProfessor_Sample data.csv')"
      ],
      "metadata": {
        "id": "JzmxVsZMlBoY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import json\n",
        "# myfile = open('/content/drive/My Drive/GROUP_PROJECT_2_ANN/all_reviews.json')\n",
        "# mydict = json.load(myfile)\n",
        "# print(mydict[0][0])\n",
        "# myfile.close()\n",
        "# #Total review count: 3374"
      ],
      "metadata": {
        "id": "TDFtY7KrxDCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "my_file = open('/content/drive/My Drive/GROUP_PROJECT_2_ANN/all_reviews.json')\n",
        "json_dict = json.load(my_file)\n",
        "my_file.close()\n",
        "lastindex = 20000\n",
        "for i in range(len(json_dict)):\n",
        "  for j in range(len(json_dict[i])):\n",
        "    d = json_dict[i][j]\n",
        "    tempDataFrame = pd.DataFrame({\n",
        "        \"comments\": d[\"Comment\"],\n",
        "        \"student_star\": float(d[\"Quality\"]),\n",
        "        \"student_difficult\": float(d[\"Difficulty\"]),\n",
        "        \"professor_name\": d[\"professor\"]\n",
        "    }, index = [lastindex])\n",
        "    df = pd.concat([df, tempDataFrame])\n",
        "    lastindex = lastindex + 1"
      ],
      "metadata": {
        "id": "L8KvTyYXrdzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['comments'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3EcmM6d9NWh",
        "outputId": "425c4bfd-8f83-41e1-90bb-12b65c5803b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0        This class is hard, but its a two-in-one gen-e...\n",
            "1        Definitely going to choose Prof. Looney\\'s cla...\n",
            "2        I overall enjoyed this class because the assig...\n",
            "3        Yes, it\\'s possible to get an A but you\\'ll de...\n",
            "4        Professor Looney has great knowledge in Astron...\n",
            "                               ...                        \n",
            "23369    don't take this call unless you love memorizin...\n",
            "23370    Really funny guy. He gives ridiculous (but app...\n",
            "23371    Only teaches from powerpoints and gives tons o...\n",
            "23372    Awesome prof. Gives out homework and short ess...\n",
            "23373    Great teacher great course. Funny guy, gives o...\n",
            "Name: comments, Length: 23374, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first 10 professors and their corresponding stars, difficulty, and comment\n",
        "prof_index = 0\n",
        "prof_miniset = {}\n",
        "for i in range(10):\n",
        "  prof_name = df['professor_name'][prof_index]\n",
        "  prof_miniset[prof_name] = [\"Stars: \" + str(df['student_star'][prof_index]), \"Difficulty: \" + str(df['student_difficult'][prof_index]), df['comments'][prof_index], ]\n",
        "  while(prof_name == df['professor_name'][prof_index]):\n",
        "    prof_index = prof_index + 1\n",
        "for k,v in prof_miniset.items():\n",
        "  print(k, v)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gRduxdHbdzXC",
        "outputId": "46454549-fcb2-48fa-9696-efdbc9da457b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Leslie  Looney ['Stars: 5.0', 'Difficulty: 3.0', 'This class is hard, but its a two-in-one gen-ed knockout, and the content is very stimulating. Unlike most classes, you have to actually participate to pass. Sections are easy and offer extra credit every week. Very funny dude. Not much more I can say.']\n",
            "Jans  Wager ['Stars: 5.0', 'Difficulty: 2.0', 'Dr. Wager is a great professor. Her expectations were clear and she really wants to help you succeed. She was always entertaining and knew what she was talking about. I took her hybrid course which i would definitely recommend.']\n",
            "Robert  Warden ['Stars: 1.5', 'Difficulty: 4.0', 'This guy is a quack! You never understand what he is saying!']\n",
            "Bryan  Eldredge ['Stars: 3.0', 'Difficulty: 5.0', 'I took his online class as an elective and regretted taking it by the end. The information is interesting but he is a tough grader. His tests were hard and very confusing.']\n",
            "William  Hollinrake ['Stars: 1.0', 'Difficulty: 5.0', 'Took online course. Day after Midterms I was the *only* one who turned in the next assignment. Rest of the year was 3 people turning in their homework. Took off points for incorrect COMMENTS, not just incorrect code. hyper-specific. bad teacher. wouldnt even help me install the software properly, gave no link to online instructions either.']\n",
            "Lauren  Baumbach ['Stars: 4.0', 'Difficulty: 4.0', 'Knows alot, the class is long and boring but she gives good notes.']\n",
            "Beverly  Faunce ['Stars: 4.5', 'Difficulty: 2.0', 'I loved the course I took with Beverly. I definitely got some practical information I could actually use.']\n",
            "Deborah  Pluss ['Stars: 4.0', 'Difficulty: 3.0', \"Pluss is hysterical and made COMM Research Methods bearable. There are no surprises here and the final is just the four quizzes she gave throughout the semester. Of those four, she drops the lowest one. There were 5 assignments, 5 quizzes (including the final). The material can be dry but that\\\\'s not on her. she made a boring class fun, take her.\"]\n",
            "Lauren  Bold ['Stars: 5.0', 'Difficulty: 2.0', nan]\n",
            "Mary  Richardson ['Stars: 4.5', 'Difficulty: 1.0', \"She\\\\'s cool. sometime laughs upsettingly loud but that\\\\'s not from an evil intention. she enjoys teaching. class is about critical writing and provides fun discussion time. it goes in appropriate pace so it\\\\'s easy to keep up. hw amount is moderate.\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained GloVe embeddings\n",
        "embeddings_index = {}\n",
        "with open(r'/content/drive/My Drive/GROUP_PROJECT_2_ANN/glove.6B.100d.txt', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs"
      ],
      "metadata": {
        "id": "NubYtccId3yM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove 'nan' values from 'comments' column\n",
        "df = df.dropna(subset=['comments'])\n",
        "\n",
        "# Remove stopwords, numerical values, and symbols from 'comments' column and convert to lowercase\n",
        "nltk.download('stopwords')\n",
        "stop_words = stopwords.words('english')\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = \" \".join([word for word in text.split() if word not in stop_words])\n",
        "    return text\n",
        "\n",
        "df['comments'] = df['comments'].apply(clean_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnsM0Cwod7KW",
        "outputId": "99ff31ba-25e1-4f77-be96-4ed74f14f247"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set parameters for model\n",
        "MAX_NB_WORDS = 20979\n",
        "MAX_SEQUENCE_LENGTH = 250\n",
        "EMBEDDING_DIM = 100\n",
        "\n",
        "# Tokenize words in 'comments' column\n",
        "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True)\n",
        "tokenizer.fit_on_texts(df['comments'].values)\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "# Convert text to sequences\n",
        "X = tokenizer.texts_to_sequences(df['comments'].values)\n",
        "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "# Create target variables\n",
        "y_star = pd.get_dummies(df['student_star']).values\n",
        "y_difficult = pd.get_dummies(df['student_difficult']).values"
      ],
      "metadata": {
        "id": "xTBHQ_a5eoMt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d45a1e6-b96b-4786-e71e-e447c2687b5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20978 unique tokens.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an embedding matrix for the tokenizer's word index\n",
        "num_words = len(tokenizer.word_index) + 1\n",
        "embedding_dim = 100\n",
        "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "id": "89PtEvHjetVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into training and testing sets\n",
        "VALIDATION_SPLIT = 0.2\n",
        "indices = np.arange(X.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "X = X[indices]\n",
        "y_star = y_star[indices]\n",
        "y_difficult = y_difficult[indices]\n",
        "num_validation_samples = int(VALIDATION_SPLIT * X.shape[0])\n",
        "\n",
        "X_train = X[:-num_validation_samples]\n",
        "y_train_star = y_star[:-num_validation_samples]\n",
        "y_train_difficult = y_difficult[:-num_validation_samples]\n",
        "X_test = X[-num_validation_samples:]\n",
        "y_test_star = y_star[-num_validation_samples:]\n",
        "y_test_difficult = y_difficult[-num_validation_samples:]"
      ],
      "metadata": {
        "id": "rCu_mDKLe1AT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_train_star.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWFPrGNK21CG",
        "outputId": "050bdccd-7237-413f-db61-a14535aa3588"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(18694, 9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build RNN model\n",
        "model = Sequential()\n",
        "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH))\n",
        "model.add(LSTM(64, dropout=0.2))\n",
        "model.add(Dense(9, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())"
      ],
      "metadata": {
        "id": "55l_ZV9ve7ah",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0c98dab-dc0a-474e-931c-c1b45ca8b57c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_3 (Embedding)     (None, 250, 100)          2097900   \n",
            "                                                                 \n",
            " lstm_3 (LSTM)               (None, 64)                42240     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 9)                 585       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,140,725\n",
            "Trainable params: 2,140,725\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "model.fit(X_train, y_train_star, batch_size=32, epochs=10, verbose=1, validation_data=(X_test, y_test_star))"
      ],
      "metadata": {
        "id": "8Q3AApPne-rx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b493c187-bdf3-4095-f5ea-adee069fdbe2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "585/585 [==============================] - 43s 72ms/step - loss: 1.3550 - accuracy: 0.5104 - val_loss: 1.5716 - val_accuracy: 0.4601\n",
            "Epoch 2/10\n",
            "585/585 [==============================] - 14s 23ms/step - loss: 1.2642 - accuracy: 0.5451 - val_loss: 1.5962 - val_accuracy: 0.4449\n",
            "Epoch 3/10\n",
            "585/585 [==============================] - 10s 17ms/step - loss: 1.1240 - accuracy: 0.5955 - val_loss: 1.6969 - val_accuracy: 0.4250\n",
            "Epoch 4/10\n",
            "585/585 [==============================] - 10s 18ms/step - loss: 1.0090 - accuracy: 0.6436 - val_loss: 1.7711 - val_accuracy: 0.4186\n",
            "Epoch 5/10\n",
            "585/585 [==============================] - 9s 15ms/step - loss: 0.9033 - accuracy: 0.6872 - val_loss: 1.8801 - val_accuracy: 0.4141\n",
            "Epoch 6/10\n",
            "585/585 [==============================] - 9s 16ms/step - loss: 0.8024 - accuracy: 0.7227 - val_loss: 2.0178 - val_accuracy: 0.3989\n",
            "Epoch 7/10\n",
            "585/585 [==============================] - 8s 14ms/step - loss: 0.7135 - accuracy: 0.7559 - val_loss: 2.1151 - val_accuracy: 0.4017\n",
            "Epoch 8/10\n",
            "585/585 [==============================] - 8s 14ms/step - loss: 0.6382 - accuracy: 0.7822 - val_loss: 2.2412 - val_accuracy: 0.3912\n",
            "Epoch 9/10\n",
            "585/585 [==============================] - 9s 15ms/step - loss: 0.5762 - accuracy: 0.8024 - val_loss: 2.3053 - val_accuracy: 0.3882\n",
            "Epoch 10/10\n",
            "585/585 [==============================] - 7s 13ms/step - loss: 0.5268 - accuracy: 0.8195 - val_loss: 2.4923 - val_accuracy: 0.3813\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f45c5ed9910>"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate model\n",
        "scores = model.evaluate(X_test, y_test_star, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "metadata": {
        "id": "tX5HxMjNfClp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa5c1b93-cc66-474f-94d0-cffbbe4ca33d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 38.13%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **--- END OF SECTION 3 ---**"
      ],
      "metadata": {
        "id": "o-hMmPPThUK4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Approach 4**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## **Building an RNN Model to Predict Professor Rating from Student Reviews using Bidirectional LSTM layers in Keras.**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### The code provided loads a dataset of professor ratings and comments from a CSV file and a JSON file, preprocesses the text data by cleaning and tokenizing it, and trains a Bidirectional LSTM neural network model to predict the star rating and difficulty level of a professor based on the comments given by the students.\n",
        "\n",
        "### Here's a summary of what the code does:\n",
        "\n",
        "### 1. Mounts the Google Drive to Colab and imports the necessary libraries.\n",
        "### 2. Reads the professor rating and comments data from a CSV file and a JSON file.\n",
        "### 3. Cleans and tokenizes the text data by removing stopwords, numerical values, and symbols, and converts the text to lowercase.\n",
        "### 4. Tokenizes the comments using the Keras tokenizer and converts the text to sequences, which are then padded to a maximum sequence length.\n",
        "### 5. Creates target variables for the star rating and difficulty level using one-hot encoding.\n",
        "### 6. Loads pre-trained GloVe embeddings and creates an embedding matrix for the tokenizer's word index.\n",
        "### 7. Splits the data into training and testing sets.\n",
        "### 8. Builds a Bidirectional LSTM neural network model with an embedding layer, two LSTM layers, two dense layers, and a softmax output layer.\n",
        "### 9. Trains and evaluates the model on the training and testing sets."
      ],
      "metadata": {
        "id": "W91EnGrpdkXm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "outputId": "137648ca-714c-49ae-ec29-a1e8a3757fdb",
        "id": "BMw5aCjtdkXo"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m     case = d.expect([\n\u001b[0m\u001b[1;32m    260\u001b[0m         \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pexpect/spawnbase.py\u001b[0m in \u001b[0;36mexpect\u001b[0;34m(self, pattern, timeout, searchwindowsize, async_, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0mcompiled_pattern_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_pattern_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m         return self.expect_list(compiled_pattern_list,\n\u001b[0m\u001b[1;32m    344\u001b[0m                 timeout, searchwindowsize, async_)\n\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pexpect/spawnbase.py\u001b[0m in \u001b[0;36mexpect_list\u001b[0;34m(self, pattern_list, timeout, searchwindowsize, async_, **kw)\u001b[0m\n\u001b[1;32m    370\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mexpect_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpect_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m     def expect_exact(self, pattern_list, timeout=-1, searchwindowsize=-1,\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pexpect/expect.py\u001b[0m in \u001b[0;36mexpect_loop\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0mincoming\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspawn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_nonblocking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelayafterread\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m                     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelayafterread\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m                 \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mincoming\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0;31m# Keep reading until exception or return.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the required libraries.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding, Dropout"
      ],
      "metadata": {
        "id": "JcMhko58dkXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#For more info on parsing json in pandas, see https://pandas.pydata.org/docs/reference/api/pandas.read_json.html\n",
        "# df_json = pd.read_json(r'/content/drive/My Drive/GROUP_PROJECT_2_ANN/all_reviews.json')\n",
        "# print(df_json[0][2])\n",
        "df = pd.read_csv(r'/content/drive/My Drive/GROUP_PROJECT_2_ANN/Big Data Set from RateMyProfessor.com for Professors Teaching Evaluation/RateMyProfessor_Sample data.csv')"
      ],
      "metadata": {
        "id": "5lCLkL2wdkXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import json\n",
        "# myfile = open('/content/drive/My Drive/GROUP_PROJECT_2_ANN/all_reviews.json')\n",
        "# mydict = json.load(myfile)\n",
        "# print(mydict[0][0])\n",
        "# myfile.close()\n",
        "# #Total review count: 3374"
      ],
      "metadata": {
        "id": "5HOTIwJQdkXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "my_file = open('/content/drive/My Drive/GROUP_PROJECT_2_ANN/all_reviews.json')\n",
        "json_dict = json.load(my_file)\n",
        "my_file.close()\n",
        "lastindex = 20000\n",
        "for i in range(len(json_dict)):\n",
        "  for j in range(len(json_dict[i])):\n",
        "    d = json_dict[i][j]\n",
        "    tempDataFrame = pd.DataFrame({\n",
        "        \"comments\": d[\"Comment\"],\n",
        "        \"student_star\": float(d[\"Quality\"]),\n",
        "        \"student_difficult\": float(d[\"Difficulty\"]),\n",
        "        \"professor_name\": d[\"professor\"]\n",
        "    }, index = [lastindex])\n",
        "    df = pd.concat([df, tempDataFrame])\n",
        "    lastindex = lastindex + 1"
      ],
      "metadata": {
        "id": "6piXe4GBdkXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['comments'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a5e5601-2c38-4a6c-afe4-6014ee6ab3c9",
        "id": "2JqgK1_bdkXs"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0        This class is hard, but its a two-in-one gen-e...\n",
            "1        Definitely going to choose Prof. Looney\\'s cla...\n",
            "2        I overall enjoyed this class because the assig...\n",
            "3        Yes, it\\'s possible to get an A but you\\'ll de...\n",
            "4        Professor Looney has great knowledge in Astron...\n",
            "                               ...                        \n",
            "23369    don't take this call unless you love memorizin...\n",
            "23370    Really funny guy. He gives ridiculous (but app...\n",
            "23371    Only teaches from powerpoints and gives tons o...\n",
            "23372    Awesome prof. Gives out homework and short ess...\n",
            "23373    Great teacher great course. Funny guy, gives o...\n",
            "Name: comments, Length: 23374, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first 10 professors and their corresponding stars, difficulty, and comment\n",
        "prof_index = 0\n",
        "prof_miniset = {}\n",
        "for i in range(10):\n",
        "  prof_name = df['professor_name'][prof_index]\n",
        "  prof_miniset[prof_name] = [\"Stars: \" + str(df['student_star'][prof_index]), \"Difficulty: \" + str(df['student_difficult'][prof_index]), df['comments'][prof_index], ]\n",
        "  while(prof_name == df['professor_name'][prof_index]):\n",
        "    prof_index = prof_index + 1\n",
        "for k,v in prof_miniset.items():\n",
        "  print(k, v)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58cb58c3-968a-4657-8b07-5503cd7b67e4",
        "id": "5yvSvRosdkXt"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Leslie  Looney ['Stars: 5.0', 'Difficulty: 3.0', 'This class is hard, but its a two-in-one gen-ed knockout, and the content is very stimulating. Unlike most classes, you have to actually participate to pass. Sections are easy and offer extra credit every week. Very funny dude. Not much more I can say.']\n",
            "Jans  Wager ['Stars: 5.0', 'Difficulty: 2.0', 'Dr. Wager is a great professor. Her expectations were clear and she really wants to help you succeed. She was always entertaining and knew what she was talking about. I took her hybrid course which i would definitely recommend.']\n",
            "Robert  Warden ['Stars: 1.5', 'Difficulty: 4.0', 'This guy is a quack! You never understand what he is saying!']\n",
            "Bryan  Eldredge ['Stars: 3.0', 'Difficulty: 5.0', 'I took his online class as an elective and regretted taking it by the end. The information is interesting but he is a tough grader. His tests were hard and very confusing.']\n",
            "William  Hollinrake ['Stars: 1.0', 'Difficulty: 5.0', 'Took online course. Day after Midterms I was the *only* one who turned in the next assignment. Rest of the year was 3 people turning in their homework. Took off points for incorrect COMMENTS, not just incorrect code. hyper-specific. bad teacher. wouldnt even help me install the software properly, gave no link to online instructions either.']\n",
            "Lauren  Baumbach ['Stars: 4.0', 'Difficulty: 4.0', 'Knows alot, the class is long and boring but she gives good notes.']\n",
            "Beverly  Faunce ['Stars: 4.5', 'Difficulty: 2.0', 'I loved the course I took with Beverly. I definitely got some practical information I could actually use.']\n",
            "Deborah  Pluss ['Stars: 4.0', 'Difficulty: 3.0', \"Pluss is hysterical and made COMM Research Methods bearable. There are no surprises here and the final is just the four quizzes she gave throughout the semester. Of those four, she drops the lowest one. There were 5 assignments, 5 quizzes (including the final). The material can be dry but that\\\\'s not on her. she made a boring class fun, take her.\"]\n",
            "Lauren  Bold ['Stars: 5.0', 'Difficulty: 2.0', nan]\n",
            "Mary  Richardson ['Stars: 4.5', 'Difficulty: 1.0', \"She\\\\'s cool. sometime laughs upsettingly loud but that\\\\'s not from an evil intention. she enjoys teaching. class is about critical writing and provides fun discussion time. it goes in appropriate pace so it\\\\'s easy to keep up. hw amount is moderate.\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained GloVe embeddings\n",
        "embeddings_index = {}\n",
        "with open(r'/content/drive/My Drive/GROUP_PROJECT_2_ANN/glove.42B.300d.txt', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs"
      ],
      "metadata": {
        "id": "Y_Jynw3adkXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove 'nan' values from 'comments' column\n",
        "df = df.dropna(subset=['comments'])\n",
        "\n",
        "# Remove stopwords, numerical values, and symbols from 'comments' column and convert to lowercase\n",
        "nltk.download('stopwords')\n",
        "stop_words = stopwords.words('english')\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = \" \".join([word for word in text.split() if word not in stop_words])\n",
        "    return text\n",
        "\n",
        "df['comments'] = df['comments'].apply(clean_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9421834-c5b0-4d4d-9a1b-db0d5b72376c",
        "id": "nk_s7aKSdkXu"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set parameters for model\n",
        "MAX_NB_WORDS = 20979\n",
        "MAX_SEQUENCE_LENGTH = 250\n",
        "EMBEDDING_DIM = 300\n",
        "\n",
        "# Tokenize words in 'comments' column\n",
        "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True)\n",
        "tokenizer.fit_on_texts(df['comments'].values)\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "# Convert text to sequences\n",
        "X = tokenizer.texts_to_sequences(df['comments'].values)\n",
        "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "# Create target variables\n",
        "y_star = pd.get_dummies(df['student_star']).values\n",
        "y_difficult = pd.get_dummies(df['student_difficult']).values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95c45aea-d353-4399-d337-1c4347786171",
        "id": "G6APdfYBdkXv"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20978 unique tokens.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an embedding matrix for the tokenizer's word index\n",
        "num_words = len(tokenizer.word_index) + 1\n",
        "embedding_dim = EMBEDDING_DIM\n",
        "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "id": "cKgepCsydkXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into training and testing sets\n",
        "VALIDATION_SPLIT = 0.2\n",
        "indices = np.arange(X.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "X = X[indices]\n",
        "y_star = y_star[indices]\n",
        "y_difficult = y_difficult[indices]\n",
        "num_validation_samples = int(VALIDATION_SPLIT * X.shape[0])\n",
        "\n",
        "X_train = X[:-num_validation_samples]\n",
        "y_train_star = y_star[:-num_validation_samples]\n",
        "y_train_difficult = y_difficult[:-num_validation_samples]\n",
        "X_test = X[-num_validation_samples:]\n",
        "y_test_star = y_star[-num_validation_samples:]\n",
        "y_test_difficult = y_difficult[-num_validation_samples:]"
      ],
      "metadata": {
        "id": "DLROmH9YdkXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_train_star.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19f31f8c-618e-4b65-dd0b-3cd3262c8f84",
        "id": "bfCaS0FwdkXx"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(18694, 9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build RNN model with Bidirectional LSTM layers\n",
        "# Train/test models on only either the quality or difficulty metrics\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.layers import Bidirectional\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH))\n",
        "model.add(Bidirectional(LSTM(64,return_sequences=True, dropout=0.2)))\n",
        "model.add(Bidirectional(LSTM(64,dropout=0.2)))\n",
        "model.add(Dense(64, activation=\"tanh\"))\n",
        "model.add(Dense(9, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "\n",
        "model.save(\"candidate.keras\")\n",
        "\n",
        "star_candidate = load_model(\"candidate.keras\")\n",
        "difficulty_candidate = load_model(\"candidate.keras\")\n",
        "difficulty_candidate.pop()\n",
        "difficulty_candidate.add(Dense(5, activation='softmax'))\n",
        "difficulty_candidate.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(difficulty_candidate.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4063e0b6-4c0f-4259-a2a8-ad055f13bce1",
        "id": "TA4Pd0aRdkXx"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 250, 300)          6293700   \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 250, 128)         186880    \n",
            " l)                                                              \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirectio  (None, 128)              98816     \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 9)                 585       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6,588,237\n",
            "Trainable params: 6,588,237\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 250, 300)          6293700   \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 250, 128)         186880    \n",
            " l)                                                              \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirectio  (None, 128)              98816     \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 5)                 325       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6,587,977\n",
            "Trainable params: 6,587,977\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the models\n",
        "print(\"Model trained on star alone\")\n",
        "star_candidate.fit(X_train, y_train_star, validation_data=(X_test, y_test_star), epochs=10, batch_size=128)\n",
        "print(\"Model trained on difficulty alone\")\n",
        "difficulty_candidate.fit(X_train, y_train_difficult, validation_data=(X_test, y_test_difficult), epochs=10, batch_size=128)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "441e4385-54e4-4f4b-a49b-438e93ec3b89",
        "id": "IiqkaZgYdkXy"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model trained on star alone\n",
            "Epoch 1/10\n",
            "147/147 [==============================] - 44s 213ms/step - loss: 1.6719 - accuracy: 0.4239 - val_loss: 1.5812 - val_accuracy: 0.4434\n",
            "Epoch 2/10\n",
            "147/147 [==============================] - 19s 131ms/step - loss: 1.4851 - accuracy: 0.4679 - val_loss: 1.5108 - val_accuracy: 0.4697\n",
            "Epoch 3/10\n",
            "147/147 [==============================] - 14s 96ms/step - loss: 1.3930 - accuracy: 0.4965 - val_loss: 1.5221 - val_accuracy: 0.4601\n",
            "Epoch 4/10\n",
            "147/147 [==============================] - 14s 96ms/step - loss: 1.3095 - accuracy: 0.5213 - val_loss: 1.5759 - val_accuracy: 0.4490\n",
            "Epoch 5/10\n",
            "147/147 [==============================] - 13s 88ms/step - loss: 1.2049 - accuracy: 0.5533 - val_loss: 1.6205 - val_accuracy: 0.4423\n",
            "Epoch 6/10\n",
            "147/147 [==============================] - 11s 76ms/step - loss: 1.0900 - accuracy: 0.6004 - val_loss: 1.7280 - val_accuracy: 0.4415\n",
            "Epoch 7/10\n",
            "147/147 [==============================] - 12s 82ms/step - loss: 0.9769 - accuracy: 0.6451 - val_loss: 1.8413 - val_accuracy: 0.4164\n",
            "Epoch 8/10\n",
            "147/147 [==============================] - 11s 76ms/step - loss: 0.8809 - accuracy: 0.6844 - val_loss: 1.9629 - val_accuracy: 0.3920\n",
            "Epoch 9/10\n",
            "147/147 [==============================] - 11s 72ms/step - loss: 0.7867 - accuracy: 0.7227 - val_loss: 2.0788 - val_accuracy: 0.4017\n",
            "Epoch 10/10\n",
            "147/147 [==============================] - 11s 75ms/step - loss: 0.6840 - accuracy: 0.7625 - val_loss: 2.2340 - val_accuracy: 0.3822\n",
            "Model trained on difficulty alone\n",
            "Epoch 1/10\n",
            "147/147 [==============================] - 35s 185ms/step - loss: 1.4906 - accuracy: 0.3246 - val_loss: 1.4158 - val_accuracy: 0.3687\n",
            "Epoch 2/10\n",
            "147/147 [==============================] - 17s 114ms/step - loss: 1.3730 - accuracy: 0.3930 - val_loss: 1.3983 - val_accuracy: 0.3803\n",
            "Epoch 3/10\n",
            "147/147 [==============================] - 13s 88ms/step - loss: 1.2774 - accuracy: 0.4515 - val_loss: 1.4492 - val_accuracy: 0.3719\n",
            "Epoch 4/10\n",
            "147/147 [==============================] - 12s 82ms/step - loss: 1.1513 - accuracy: 0.5159 - val_loss: 1.5069 - val_accuracy: 0.3758\n",
            "Epoch 5/10\n",
            "147/147 [==============================] - 12s 83ms/step - loss: 0.9934 - accuracy: 0.6006 - val_loss: 1.6434 - val_accuracy: 0.3687\n",
            "Epoch 6/10\n",
            "147/147 [==============================] - 10s 71ms/step - loss: 0.8208 - accuracy: 0.6809 - val_loss: 1.7760 - val_accuracy: 0.3606\n",
            "Epoch 7/10\n",
            "147/147 [==============================] - 11s 73ms/step - loss: 0.6841 - accuracy: 0.7362 - val_loss: 1.9146 - val_accuracy: 0.3563\n",
            "Epoch 8/10\n",
            "147/147 [==============================] - 10s 71ms/step - loss: 0.5879 - accuracy: 0.7744 - val_loss: 2.0886 - val_accuracy: 0.3432\n",
            "Epoch 9/10\n",
            "147/147 [==============================] - 11s 76ms/step - loss: 0.5112 - accuracy: 0.8078 - val_loss: 2.3673 - val_accuracy: 0.3482\n",
            "Epoch 10/10\n",
            "147/147 [==============================] - 10s 71ms/step - loss: 0.4544 - accuracy: 0.8256 - val_loss: 2.3961 - val_accuracy: 0.3377\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc866e8e4a0>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Alternate Model - 2:**\n",
        "\n",
        "### **The code defines a neural network model that can classify text into one of nine categories. The model uses pre-trained word embeddings and two layers of bidirectional LSTM cells. The first LSTM layer has twice as many parameters as before, and the embedding layer is no longer trainable. The model is trained using categorical cross-entropy loss and the Adam optimizer. It is evaluated on a validation set and trained for 10 epochs with a batch size of 128.**\n",
        "\n"
      ],
      "metadata": {
        "id": "FTq57oFRd76C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Same as before, only we no longer train the embedding layer and doubled the number of parameters in the first LSTM layer\n",
        "alt_model = Sequential([\n",
        "    Embedding(MAX_NB_WORDS, EMBEDDING_DIM, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False),\n",
        "    Bidirectional(LSTM(128,return_sequences=True, dropout=0.2)),\n",
        "    Bidirectional(LSTM(64,dropout=0.2)),\n",
        "    Dense(64, activation=\"tanh\"),\n",
        "    Dense(9, activation='softmax')\n",
        "])\n",
        "\n",
        "alt_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(alt_model.summary())\n",
        "alt_model.fit(X_train, y_train_star, validation_data=(X_test, y_test_star), epochs=10, batch_size=128)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jT1aNcVGrvdN",
        "outputId": "7ef77076-870b-469f-bebb-ccfcccaa23e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 250, 300)          6293700   \n",
            "                                                                 \n",
            " bidirectional_2 (Bidirectio  (None, 250, 256)         439296    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " bidirectional_3 (Bidirectio  (None, 128)              164352    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 9)                 585       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6,906,189\n",
            "Trainable params: 612,489\n",
            "Non-trainable params: 6,293,700\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            "147/147 [==============================] - 21s 94ms/step - loss: 1.6751 - accuracy: 0.4193 - val_loss: 1.5520 - val_accuracy: 0.4601\n",
            "Epoch 2/10\n",
            "147/147 [==============================] - 13s 86ms/step - loss: 1.5521 - accuracy: 0.4520 - val_loss: 1.5243 - val_accuracy: 0.4590\n",
            "Epoch 3/10\n",
            "147/147 [==============================] - 13s 86ms/step - loss: 1.5216 - accuracy: 0.4577 - val_loss: 1.5227 - val_accuracy: 0.4629\n",
            "Epoch 4/10\n",
            "147/147 [==============================] - 12s 83ms/step - loss: 1.4830 - accuracy: 0.4704 - val_loss: 1.5156 - val_accuracy: 0.4631\n",
            "Epoch 5/10\n",
            "147/147 [==============================] - 12s 85ms/step - loss: 1.4611 - accuracy: 0.4743 - val_loss: 1.5195 - val_accuracy: 0.4526\n",
            "Epoch 6/10\n",
            "147/147 [==============================] - 13s 86ms/step - loss: 1.4277 - accuracy: 0.4840 - val_loss: 1.5241 - val_accuracy: 0.4644\n",
            "Epoch 7/10\n",
            "147/147 [==============================] - 14s 95ms/step - loss: 1.3915 - accuracy: 0.4944 - val_loss: 1.5565 - val_accuracy: 0.4526\n",
            "Epoch 8/10\n",
            "147/147 [==============================] - 13s 88ms/step - loss: 1.3732 - accuracy: 0.5084 - val_loss: 1.5850 - val_accuracy: 0.4402\n",
            "Epoch 9/10\n",
            "147/147 [==============================] - 13s 89ms/step - loss: 1.3104 - accuracy: 0.5259 - val_loss: 1.5889 - val_accuracy: 0.4526\n",
            "Epoch 10/10\n",
            "147/147 [==============================] - 13s 88ms/step - loss: 1.2630 - accuracy: 0.5398 - val_loss: 1.5996 - val_accuracy: 0.4513\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc864c8e0e0>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Alternate Model - 3:**\n",
        "\n",
        "### **This code creates a neural network model with two LSTM layers, two dense layers with dropout, and an output layer with softmax activation. The number of parameters in the second LSTM layer is doubled compared to the first model. The model is trained on a dataset with nine categories and evaluated using accuracy.**"
      ],
      "metadata": {
        "id": "uHYp5E7keiAm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Add an additional dense layer and dropout, and double the parameter count in the second LSTM layer\n",
        "alt2_model = Sequential([\n",
        "    Embedding(MAX_NB_WORDS, EMBEDDING_DIM, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False),\n",
        "    Bidirectional(LSTM(128,return_sequences=True, dropout=0.2)),\n",
        "    Bidirectional(LSTM(128,dropout=0.2)),\n",
        "    Dense(64, activation=\"tanh\"),\n",
        "    Dropout(0.4),\n",
        "    Dense(64, activation=\"tanh\"),\n",
        "    Dropout(0.4),\n",
        "    Dense(9, activation='softmax')\n",
        "])\n",
        "\n",
        "alt2_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(alt2_model.summary())\n",
        "alt2_model.fit(X_train, y_train_star, validation_data=(X_test, y_test_star), epochs=10, batch_size=128)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BB3cx3Yztl4W",
        "outputId": "510a728a-1bab-413e-c536-c24084d7388f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (None, 250, 300)          6293700   \n",
            "                                                                 \n",
            " bidirectional_4 (Bidirectio  (None, 250, 256)         439296    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " bidirectional_5 (Bidirectio  (None, 256)              394240    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 64)                16448     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 64)                0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 64)                4160      \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 9)                 585       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 7,148,429\n",
            "Trainable params: 854,729\n",
            "Non-trainable params: 6,293,700\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            "147/147 [==============================] - 26s 117ms/step - loss: 1.7399 - accuracy: 0.4070 - val_loss: 1.6095 - val_accuracy: 0.4430\n",
            "Epoch 2/10\n",
            "147/147 [==============================] - 15s 104ms/step - loss: 1.6021 - accuracy: 0.4410 - val_loss: 1.5733 - val_accuracy: 0.4440\n",
            "Epoch 3/10\n",
            "147/147 [==============================] - 15s 103ms/step - loss: 1.5587 - accuracy: 0.4526 - val_loss: 1.5395 - val_accuracy: 0.4584\n",
            "Epoch 4/10\n",
            "147/147 [==============================] - 15s 100ms/step - loss: 1.5304 - accuracy: 0.4575 - val_loss: 1.5606 - val_accuracy: 0.4541\n",
            "Epoch 5/10\n",
            "147/147 [==============================] - 15s 99ms/step - loss: 1.5116 - accuracy: 0.4629 - val_loss: 1.5124 - val_accuracy: 0.4657\n",
            "Epoch 6/10\n",
            "147/147 [==============================] - 15s 102ms/step - loss: 1.4784 - accuracy: 0.4726 - val_loss: 1.5311 - val_accuracy: 0.4539\n",
            "Epoch 7/10\n",
            "147/147 [==============================] - 15s 101ms/step - loss: 1.4546 - accuracy: 0.4828 - val_loss: 1.5170 - val_accuracy: 0.4609\n",
            "Epoch 8/10\n",
            "147/147 [==============================] - 15s 101ms/step - loss: 1.4238 - accuracy: 0.4879 - val_loss: 1.5444 - val_accuracy: 0.4479\n",
            "Epoch 9/10\n",
            "147/147 [==============================] - 15s 101ms/step - loss: 1.3946 - accuracy: 0.4946 - val_loss: 1.5490 - val_accuracy: 0.4477\n",
            "Epoch 10/10\n",
            "147/147 [==============================] - 15s 101ms/step - loss: 1.3617 - accuracy: 0.5005 - val_loss: 1.5576 - val_accuracy: 0.4517\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc860d3d7b0>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Alternate Model - 4:**\n",
        "### **The code defines a deep learning model to classify text based on star ratings. It uses an embedding layer followed by two bidirectional LSTM layers and several dense layers with dropout. The difference with the previous models is that the number of parameters in the LSTM layers and the dense layers have been modified, and a sigmoid activation function has been used in the first two dense layers. The model is trained using the categorical cross-entropy loss function and the Adam optimizer, and it is evaluated on a validation set.**"
      ],
      "metadata": {
        "id": "n6WueKVYerKA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Cut parameter count in 2nd layer back, double the count in first dense layer, and try sigmoid activation\n",
        "alt3_model = Sequential([\n",
        "    Embedding(MAX_NB_WORDS, EMBEDDING_DIM, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False),\n",
        "    Bidirectional(LSTM(128,return_sequences=True, dropout=0.2)),\n",
        "    Bidirectional(LSTM(64,dropout=0.2)),\n",
        "    Dense(128, activation=\"sigmoid\"),\n",
        "    Dropout(0.4),\n",
        "    Dense(64, activation=\"sigmoid\"),\n",
        "    Dropout(0.4),\n",
        "    Dense(9, activation='softmax')\n",
        "])\n",
        "\n",
        "alt3_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(alt3_model.summary())\n",
        "alt3_model.fit(X_train, y_train_star, validation_data=(X_test, y_test_star), epochs=20, batch_size=128)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0Ec3qJduiyD",
        "outputId": "ead18c1a-5868-4268-d930-2d1db36c4ccc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_3 (Embedding)     (None, 250, 300)          6293700   \n",
            "                                                                 \n",
            " bidirectional_6 (Bidirectio  (None, 250, 256)         439296    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " bidirectional_7 (Bidirectio  (None, 128)              164352    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 128)               16512     \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 9)                 585       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6,922,701\n",
            "Trainable params: 629,001\n",
            "Non-trainable params: 6,293,700\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/20\n",
            "147/147 [==============================] - 22s 94ms/step - loss: 1.8913 - accuracy: 0.3688 - val_loss: 1.6562 - val_accuracy: 0.4423\n",
            "Epoch 2/20\n",
            "147/147 [==============================] - 12s 85ms/step - loss: 1.6727 - accuracy: 0.4303 - val_loss: 1.5832 - val_accuracy: 0.4475\n",
            "Epoch 3/20\n",
            "147/147 [==============================] - 13s 85ms/step - loss: 1.6131 - accuracy: 0.4394 - val_loss: 1.5516 - val_accuracy: 0.4539\n",
            "Epoch 4/20\n",
            "147/147 [==============================] - 12s 84ms/step - loss: 1.5765 - accuracy: 0.4484 - val_loss: 1.5359 - val_accuracy: 0.4554\n",
            "Epoch 5/20\n",
            "147/147 [==============================] - 13s 85ms/step - loss: 1.5454 - accuracy: 0.4534 - val_loss: 1.5276 - val_accuracy: 0.4631\n",
            "Epoch 6/20\n",
            "147/147 [==============================] - 12s 84ms/step - loss: 1.5176 - accuracy: 0.4603 - val_loss: 1.5304 - val_accuracy: 0.4616\n",
            "Epoch 7/20\n",
            "147/147 [==============================] - 13s 88ms/step - loss: 1.5024 - accuracy: 0.4613 - val_loss: 1.5124 - val_accuracy: 0.4652\n",
            "Epoch 8/20\n",
            "147/147 [==============================] - 13s 85ms/step - loss: 1.4732 - accuracy: 0.4718 - val_loss: 1.5138 - val_accuracy: 0.4682\n",
            "Epoch 9/20\n",
            "147/147 [==============================] - 13s 86ms/step - loss: 1.4474 - accuracy: 0.4772 - val_loss: 1.5377 - val_accuracy: 0.4597\n",
            "Epoch 10/20\n",
            "147/147 [==============================] - 12s 84ms/step - loss: 1.4221 - accuracy: 0.4851 - val_loss: 1.5696 - val_accuracy: 0.4635\n",
            "Epoch 11/20\n",
            "147/147 [==============================] - 13s 86ms/step - loss: 1.4026 - accuracy: 0.4848 - val_loss: 1.5275 - val_accuracy: 0.4633\n",
            "Epoch 12/20\n",
            "147/147 [==============================] - 12s 84ms/step - loss: 1.3769 - accuracy: 0.4953 - val_loss: 1.5529 - val_accuracy: 0.4582\n",
            "Epoch 13/20\n",
            "147/147 [==============================] - 12s 84ms/step - loss: 1.3822 - accuracy: 0.4973 - val_loss: 1.5455 - val_accuracy: 0.4479\n",
            "Epoch 14/20\n",
            "147/147 [==============================] - 13s 85ms/step - loss: 1.3471 - accuracy: 0.5026 - val_loss: 1.5582 - val_accuracy: 0.4552\n",
            "Epoch 15/20\n",
            "147/147 [==============================] - 13s 86ms/step - loss: 1.3082 - accuracy: 0.5131 - val_loss: 1.6351 - val_accuracy: 0.4398\n",
            "Epoch 16/20\n",
            "147/147 [==============================] - 12s 84ms/step - loss: 1.2780 - accuracy: 0.5271 - val_loss: 1.6204 - val_accuracy: 0.4571\n",
            "Epoch 17/20\n",
            "147/147 [==============================] - 13s 86ms/step - loss: 1.2420 - accuracy: 0.5384 - val_loss: 1.6929 - val_accuracy: 0.4507\n",
            "Epoch 18/20\n",
            "147/147 [==============================] - 13s 86ms/step - loss: 1.2231 - accuracy: 0.5425 - val_loss: 1.6770 - val_accuracy: 0.4436\n",
            "Epoch 19/20\n",
            "147/147 [==============================] - 13s 85ms/step - loss: 1.1892 - accuracy: 0.5546 - val_loss: 1.7094 - val_accuracy: 0.4237\n",
            "Epoch 20/20\n",
            "147/147 [==============================] - 13s 85ms/step - loss: 1.1844 - accuracy: 0.5557 - val_loss: 1.7591 - val_accuracy: 0.4460\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc867638220>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Alternate Model - 5:**\n",
        "### **The code defines a neural network model that uses GRU layers instead of LSTM layers, with a higher dropout rate to prevent overfitting. It also includes several dense layers with sigmoid activation and dropout layers. The model is trained on the star rating dataset and evaluated using accuracy as a metric.**"
      ],
      "metadata": {
        "id": "WcBdMzfDey7_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#GRU layers with higher dropout rate\n",
        "from tensorflow.keras.layers import GRU\n",
        "\n",
        "alt4_model = Sequential([\n",
        "    Embedding(MAX_NB_WORDS, EMBEDDING_DIM, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False),\n",
        "    Bidirectional(GRU(64,return_sequences=True, dropout=0.4)),\n",
        "    Bidirectional(GRU(64,dropout=0.4)),\n",
        "    Dense(128, activation=\"sigmoid\"),\n",
        "    Dropout(0.4),\n",
        "    Dense(64, activation=\"sigmoid\"),\n",
        "    Dropout(0.4),\n",
        "    Dense(9, activation='softmax')\n",
        "])\n",
        "\n",
        "alt4_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(alt4_model.summary())\n",
        "alt4_model.fit(X_train, y_train_star, validation_data=(X_test, y_test_star), epochs=20, batch_size=128)"
      ],
      "metadata": {
        "id": "tfwT7rGnwDa-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e131a81d-d5c8-4d3e-b99c-d5314c9c484d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_4 (Embedding)     (None, 250, 300)          6293700   \n",
            "                                                                 \n",
            " bidirectional_8 (Bidirectio  (None, 250, 128)         140544    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " bidirectional_9 (Bidirectio  (None, 128)              74496     \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 128)               16512     \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 64)                8256      \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 9)                 585       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6,534,093\n",
            "Trainable params: 240,393\n",
            "Non-trainable params: 6,293,700\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/20\n",
            "147/147 [==============================] - 18s 69ms/step - loss: 1.9411 - accuracy: 0.3496 - val_loss: 1.7105 - val_accuracy: 0.4164\n",
            "Epoch 2/20\n",
            "147/147 [==============================] - 9s 58ms/step - loss: 1.7063 - accuracy: 0.4252 - val_loss: 1.5873 - val_accuracy: 0.4402\n",
            "Epoch 3/20\n",
            "147/147 [==============================] - 9s 59ms/step - loss: 1.6253 - accuracy: 0.4423 - val_loss: 1.5493 - val_accuracy: 0.4513\n",
            "Epoch 4/20\n",
            "147/147 [==============================] - 8s 56ms/step - loss: 1.5968 - accuracy: 0.4431 - val_loss: 1.5445 - val_accuracy: 0.4520\n",
            "Epoch 5/20\n",
            "147/147 [==============================] - 8s 56ms/step - loss: 1.5696 - accuracy: 0.4478 - val_loss: 1.5403 - val_accuracy: 0.4612\n",
            "Epoch 6/20\n",
            "147/147 [==============================] - 8s 55ms/step - loss: 1.5491 - accuracy: 0.4559 - val_loss: 1.5786 - val_accuracy: 0.4565\n",
            "Epoch 7/20\n",
            "147/147 [==============================] - 8s 54ms/step - loss: 1.5397 - accuracy: 0.4559 - val_loss: 1.5045 - val_accuracy: 0.4674\n",
            "Epoch 8/20\n",
            "147/147 [==============================] - 9s 58ms/step - loss: 1.5195 - accuracy: 0.4630 - val_loss: 1.5038 - val_accuracy: 0.4648\n",
            "Epoch 9/20\n",
            "147/147 [==============================] - 8s 56ms/step - loss: 1.5046 - accuracy: 0.4672 - val_loss: 1.4955 - val_accuracy: 0.4699\n",
            "Epoch 10/20\n",
            "147/147 [==============================] - 8s 55ms/step - loss: 1.4900 - accuracy: 0.4702 - val_loss: 1.5008 - val_accuracy: 0.4691\n",
            "Epoch 11/20\n",
            "147/147 [==============================] - 9s 59ms/step - loss: 1.4778 - accuracy: 0.4691 - val_loss: 1.4880 - val_accuracy: 0.4699\n",
            "Epoch 12/20\n",
            "147/147 [==============================] - 8s 54ms/step - loss: 1.4687 - accuracy: 0.4746 - val_loss: 1.4950 - val_accuracy: 0.4682\n",
            "Epoch 13/20\n",
            "147/147 [==============================] - 8s 55ms/step - loss: 1.4553 - accuracy: 0.4769 - val_loss: 1.4873 - val_accuracy: 0.4682\n",
            "Epoch 14/20\n",
            "147/147 [==============================] - 9s 59ms/step - loss: 1.4413 - accuracy: 0.4785 - val_loss: 1.4949 - val_accuracy: 0.4699\n",
            "Epoch 15/20\n",
            "147/147 [==============================] - 8s 56ms/step - loss: 1.4337 - accuracy: 0.4819 - val_loss: 1.4920 - val_accuracy: 0.4706\n",
            "Epoch 16/20\n",
            "147/147 [==============================] - 9s 59ms/step - loss: 1.4246 - accuracy: 0.4829 - val_loss: 1.5365 - val_accuracy: 0.4650\n",
            "Epoch 17/20\n",
            "147/147 [==============================] - 8s 55ms/step - loss: 1.4168 - accuracy: 0.4867 - val_loss: 1.4978 - val_accuracy: 0.4682\n",
            "Epoch 18/20\n",
            "147/147 [==============================] - 8s 55ms/step - loss: 1.3974 - accuracy: 0.4905 - val_loss: 1.5066 - val_accuracy: 0.4704\n",
            "Epoch 19/20\n",
            "147/147 [==============================] - 8s 55ms/step - loss: 1.3896 - accuracy: 0.4924 - val_loss: 1.5175 - val_accuracy: 0.4676\n",
            "Epoch 20/20\n",
            "147/147 [==============================] - 8s 56ms/step - loss: 1.3792 - accuracy: 0.4949 - val_loss: 1.5195 - val_accuracy: 0.4672\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc864947940>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **--- END OF SECTION 4 ---**"
      ],
      "metadata": {
        "id": "cwNvaqnBdt-t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Approach 5**\n",
        "---\n",
        "## **Using Regression Model**\n",
        "---\n",
        "### The model is being trained as a regression problem to predict the star ratings for professors based on the comments left by students.\n",
        "\n",
        "### The model architecture uses pre-trained GloVe embeddings, followed by two layers of Bidirectional GRUs with dropout regularization, and two dense layers with sigmoid activation and dropout regularization. The final dense layer has a single output node, which will output the predicted rating for a given comment.\n",
        "\n",
        "### The data is split into training and testing sets, and the mse loss function is used to evaluate the model's performance during training."
      ],
      "metadata": {
        "id": "syxbtZxWzaEj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e52a36bd-75b3-4eb0-a54a-ab640f89c1e9",
        "id": "l1fA8Z2_zaEk"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the required libraries.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding, Dropout"
      ],
      "metadata": {
        "id": "0KbaiNjOzaE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#For more info on parsing json in pandas, see https://pandas.pydata.org/docs/reference/api/pandas.read_json.html\n",
        "# df_json = pd.read_json(r'/content/drive/My Drive/GROUP_PROJECT_2_ANN/all_reviews.json')\n",
        "# print(df_json[0][2])\n",
        "df = pd.read_csv(r'/content/drive/My Drive/GROUP_PROJECT_2_ANN/Big Data Set from RateMyProfessor.com for Professors Teaching Evaluation/RateMyProfessor_Sample data.csv')"
      ],
      "metadata": {
        "id": "kzqrutBUzaE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "my_file = open('/content/drive/My Drive/GROUP_PROJECT_2_ANN/all_reviews.json')\n",
        "json_dict = json.load(my_file)\n",
        "my_file.close()\n",
        "lastindex = len(df)\n",
        "for i in range(len(json_dict)):\n",
        "  for j in range(len(json_dict[i])):\n",
        "    d = json_dict[i][j]\n",
        "    tempDataFrame = pd.DataFrame({\n",
        "        \"comments\": d[\"Comment\"],\n",
        "        \"student_star\": float(d[\"Quality\"]),\n",
        "        \"student_difficult\": float(d[\"Difficulty\"]),\n",
        "        \"professor_name\": d[\"professor\"]\n",
        "    }, index = [lastindex])\n",
        "    df = pd.concat([df, tempDataFrame])\n",
        "    lastindex = lastindex + 1"
      ],
      "metadata": {
        "id": "jmI9tVMFzaE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2v_Sarbb1Bh5",
        "outputId": "c3ac5e71-377a-4185-880f-f4f100ba4ed4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23374\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['comments'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa31823b-4510-49b3-a081-0409746f3a4b",
        "id": "7Ojr329kzaE9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0        This class is hard, but its a two-in-one gen-e...\n",
            "1        Definitely going to choose Prof. Looney\\'s cla...\n",
            "2        I overall enjoyed this class because the assig...\n",
            "3        Yes, it\\'s possible to get an A but you\\'ll de...\n",
            "4        Professor Looney has great knowledge in Astron...\n",
            "                               ...                        \n",
            "23369    don't take this call unless you love memorizin...\n",
            "23370    Really funny guy. He gives ridiculous (but app...\n",
            "23371    Only teaches from powerpoints and gives tons o...\n",
            "23372    Awesome prof. Gives out homework and short ess...\n",
            "23373    Great teacher great course. Funny guy, gives o...\n",
            "Name: comments, Length: 23374, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first 10 professors and their corresponding stars, difficulty, and comment\n",
        "prof_index = 0\n",
        "prof_miniset = {}\n",
        "for i in range(10):\n",
        "  prof_name = df['professor_name'][prof_index]\n",
        "  prof_miniset[prof_name] = [\"Stars: \" + str(df['student_star'][prof_index]), \"Difficulty: \" + str(df['student_difficult'][prof_index]), df['comments'][prof_index], ]\n",
        "  while(prof_name == df['professor_name'][prof_index]):\n",
        "    prof_index = prof_index + 1\n",
        "for k,v in prof_miniset.items():\n",
        "  print(k, v)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "677ad96a-2426-4cd2-a734-cfd5b6cdae84",
        "id": "F7DQshdbzaE9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Leslie  Looney ['Stars: 5.0', 'Difficulty: 3.0', 'This class is hard, but its a two-in-one gen-ed knockout, and the content is very stimulating. Unlike most classes, you have to actually participate to pass. Sections are easy and offer extra credit every week. Very funny dude. Not much more I can say.']\n",
            "Jans  Wager ['Stars: 5.0', 'Difficulty: 2.0', 'Dr. Wager is a great professor. Her expectations were clear and she really wants to help you succeed. She was always entertaining and knew what she was talking about. I took her hybrid course which i would definitely recommend.']\n",
            "Robert  Warden ['Stars: 1.5', 'Difficulty: 4.0', 'This guy is a quack! You never understand what he is saying!']\n",
            "Bryan  Eldredge ['Stars: 3.0', 'Difficulty: 5.0', 'I took his online class as an elective and regretted taking it by the end. The information is interesting but he is a tough grader. His tests were hard and very confusing.']\n",
            "William  Hollinrake ['Stars: 1.0', 'Difficulty: 5.0', 'Took online course. Day after Midterms I was the *only* one who turned in the next assignment. Rest of the year was 3 people turning in their homework. Took off points for incorrect COMMENTS, not just incorrect code. hyper-specific. bad teacher. wouldnt even help me install the software properly, gave no link to online instructions either.']\n",
            "Lauren  Baumbach ['Stars: 4.0', 'Difficulty: 4.0', 'Knows alot, the class is long and boring but she gives good notes.']\n",
            "Beverly  Faunce ['Stars: 4.5', 'Difficulty: 2.0', 'I loved the course I took with Beverly. I definitely got some practical information I could actually use.']\n",
            "Deborah  Pluss ['Stars: 4.0', 'Difficulty: 3.0', \"Pluss is hysterical and made COMM Research Methods bearable. There are no surprises here and the final is just the four quizzes she gave throughout the semester. Of those four, she drops the lowest one. There were 5 assignments, 5 quizzes (including the final). The material can be dry but that\\\\'s not on her. she made a boring class fun, take her.\"]\n",
            "Lauren  Bold ['Stars: 5.0', 'Difficulty: 2.0', nan]\n",
            "Mary  Richardson ['Stars: 4.5', 'Difficulty: 1.0', \"She\\\\'s cool. sometime laughs upsettingly loud but that\\\\'s not from an evil intention. she enjoys teaching. class is about critical writing and provides fun discussion time. it goes in appropriate pace so it\\\\'s easy to keep up. hw amount is moderate.\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained GloVe embeddings\n",
        "embeddings_index = {}\n",
        "with open(r'/content/drive/My Drive/GROUP_PROJECT_2_ANN/glove.42B.300d.txt', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs"
      ],
      "metadata": {
        "id": "4jOmcswXzaE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove 'nan' values from 'comments' column\n",
        "df = df.dropna(subset=['comments'])\n",
        "\n",
        "# Remove stopwords, numerical values, and symbols from 'comments' column and convert to lowercase\n",
        "nltk.download('stopwords')\n",
        "stop_words = stopwords.words('english')\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = \" \".join([word for word in text.split() if word not in stop_words])\n",
        "    return text\n",
        "\n",
        "df['comments'] = df['comments'].apply(clean_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b61ba34f-b281-42d5-c119-03f0edf66b44",
        "id": "OWjETh--zaE9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set parameters for model\n",
        "MAX_NB_WORDS = 20979\n",
        "MAX_SEQUENCE_LENGTH = 250\n",
        "EMBEDDING_DIM = 300\n",
        "\n",
        "# Tokenize words in 'comments' column\n",
        "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True)\n",
        "tokenizer.fit_on_texts(df['comments'].values)\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "# Convert text to sequences\n",
        "X = tokenizer.texts_to_sequences(df['comments'].values)\n",
        "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "# Create target variables\n",
        "y_star = df['student_star'].values\n",
        "y_difficult = df['student_difficult'].values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7ecd21a-24bf-403a-ff78-93d6d79047a0",
        "id": "m7gRuhInzaE9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20978 unique tokens.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#How exactly does the word_index work? Let's look at the first comment vectorized\n",
        "mylist = df['comments'][0].split(\" \")\n",
        "for word in mylist:\n",
        "  print(f\"{word}:{word_index[word]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcUYR3n94u_m",
        "outputId": "42c89af8-f114-48a8-9c6d-8cbc4fe1d743"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class:1\n",
            "hard:10\n",
            "twoinone:9644\n",
            "gened:3844\n",
            "knockout:7026\n",
            "content:426\n",
            "stimulating:1279\n",
            "unlike:1258\n",
            "classes:34\n",
            "actually:129\n",
            "participate:366\n",
            "pass:143\n",
            "sections:1280\n",
            "easy:7\n",
            "offer:770\n",
            "extra:127\n",
            "credit:137\n",
            "every:57\n",
            "week:155\n",
            "funny:77\n",
            "dude:973\n",
            "much:31\n",
            "say:151\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Next, let's see how the first comment is converted to a sequence\n",
        "print(df['comments'][0])\n",
        "Y = tokenizer.texts_to_sequences(df['comments'].values)\n",
        "Y = pad_sequences(Y, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "print(Y[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9xs47FC6LOP",
        "outputId": "6b4ced93-e6b4-47a4-bb1c-7fe55f94a536"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class hard twoinone gened knockout content stimulating unlike classes actually participate pass sections easy offer extra credit every week funny dude much say\n",
            "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    1   10 9644 3844 7026  426 1279 1258   34  129  366\n",
            "  143 1280    7  770  127  137   57  155   77  973   31  151]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an embedding matrix for the tokenizer's word index\n",
        "num_words = len(tokenizer.word_index) + 1\n",
        "embedding_dim = EMBEDDING_DIM\n",
        "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "id": "JDMC4gd7zaE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into training and testing sets\n",
        "VALIDATION_SPLIT = 0.2\n",
        "indices = np.arange(X.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "X = X[indices]\n",
        "y_star = y_star[indices]\n",
        "y_difficult = y_difficult[indices]\n",
        "num_validation_samples = int(VALIDATION_SPLIT * X.shape[0])\n",
        "\n",
        "X_train = X[:-num_validation_samples]\n",
        "y_train_star = y_star[:-num_validation_samples]\n",
        "y_train_difficult = y_difficult[:-num_validation_samples]\n",
        "X_test = X[-num_validation_samples:]\n",
        "y_test_star = y_star[-num_validation_samples:]\n",
        "y_test_difficult = y_difficult[-num_validation_samples:]"
      ],
      "metadata": {
        "id": "s6ljxDCUzaE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_train_star.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05e902ef-f5f5-4172-e697-787c7e70621f",
        "id": "32iLGuHgzaE-"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(18694,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Treat output as a regression problem\n",
        "from tensorflow.keras.layers import GRU, Bidirectional\n",
        "\n",
        "alt5_model = Sequential([\n",
        "    Embedding(MAX_NB_WORDS, EMBEDDING_DIM, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False),\n",
        "    Bidirectional(GRU(64,return_sequences=True, dropout=0.4)),\n",
        "    Bidirectional(GRU(64,dropout=0.4)),\n",
        "    Dense(128, activation=\"sigmoid\"),\n",
        "    Dropout(0.4),\n",
        "    Dense(64, activation=\"sigmoid\"),\n",
        "    Dropout(0.4),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "alt5_model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
        "print(alt5_model.summary())\n",
        "alt5_model.fit(X_train, y_train_star, validation_data=(X_test, y_test_star), epochs=20, batch_size=128)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bc5e7dc-3e9a-4666-e035-8d6605b02248",
        "id": "L8XPV37QzaE-"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (None, 250, 300)          6293700   \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 250, 128)         140544    \n",
            " l)                                                              \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirectio  (None, 128)              74496     \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               16512     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 128)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6,533,573\n",
            "Trainable params: 239,873\n",
            "Non-trainable params: 6,293,700\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/20\n",
            "147/147 [==============================] - 23s 66ms/step - loss: 3.1372 - accuracy: 0.1214 - val_loss: 2.0056 - val_accuracy: 0.1168\n",
            "Epoch 2/20\n",
            "147/147 [==============================] - 8s 56ms/step - loss: 1.7440 - accuracy: 0.1230 - val_loss: 1.1526 - val_accuracy: 0.1168\n",
            "Epoch 3/20\n",
            "147/147 [==============================] - 8s 57ms/step - loss: 1.3589 - accuracy: 0.1226 - val_loss: 0.9393 - val_accuracy: 0.1168\n",
            "Epoch 4/20\n",
            "147/147 [==============================] - 8s 52ms/step - loss: 1.2663 - accuracy: 0.1226 - val_loss: 0.8821 - val_accuracy: 0.1168\n",
            "Epoch 5/20\n",
            "147/147 [==============================] - 8s 52ms/step - loss: 1.2069 - accuracy: 0.1228 - val_loss: 0.8653 - val_accuracy: 0.1168\n",
            "Epoch 6/20\n",
            "147/147 [==============================] - 8s 56ms/step - loss: 1.1652 - accuracy: 0.1229 - val_loss: 0.9127 - val_accuracy: 0.1168\n",
            "Epoch 7/20\n",
            "147/147 [==============================] - 8s 53ms/step - loss: 1.1168 - accuracy: 0.1232 - val_loss: 0.8442 - val_accuracy: 0.1168\n",
            "Epoch 8/20\n",
            "147/147 [==============================] - 8s 52ms/step - loss: 1.0819 - accuracy: 0.1232 - val_loss: 0.8782 - val_accuracy: 0.1168\n",
            "Epoch 9/20\n",
            "147/147 [==============================] - 8s 57ms/step - loss: 1.0670 - accuracy: 0.1231 - val_loss: 0.8317 - val_accuracy: 0.1168\n",
            "Epoch 10/20\n",
            "147/147 [==============================] - 8s 53ms/step - loss: 1.0358 - accuracy: 0.1233 - val_loss: 0.8303 - val_accuracy: 0.1168\n",
            "Epoch 11/20\n",
            "147/147 [==============================] - 8s 53ms/step - loss: 0.9941 - accuracy: 0.1233 - val_loss: 0.8127 - val_accuracy: 0.1168\n",
            "Epoch 12/20\n",
            "147/147 [==============================] - 8s 53ms/step - loss: 0.9808 - accuracy: 0.1232 - val_loss: 0.8164 - val_accuracy: 0.1168\n",
            "Epoch 13/20\n",
            "147/147 [==============================] - 8s 53ms/step - loss: 0.9789 - accuracy: 0.1233 - val_loss: 0.8464 - val_accuracy: 0.1168\n",
            "Epoch 14/20\n",
            "147/147 [==============================] - 8s 52ms/step - loss: 0.9311 - accuracy: 0.1233 - val_loss: 0.8347 - val_accuracy: 0.1168\n",
            "Epoch 15/20\n",
            "147/147 [==============================] - 8s 53ms/step - loss: 0.9040 - accuracy: 0.1233 - val_loss: 0.8360 - val_accuracy: 0.1168\n",
            "Epoch 16/20\n",
            "147/147 [==============================] - 8s 52ms/step - loss: 0.8911 - accuracy: 0.1233 - val_loss: 0.8223 - val_accuracy: 0.1168\n",
            "Epoch 17/20\n",
            "147/147 [==============================] - 8s 57ms/step - loss: 0.8884 - accuracy: 0.1233 - val_loss: 0.8257 - val_accuracy: 0.1168\n",
            "Epoch 18/20\n",
            "147/147 [==============================] - 8s 53ms/step - loss: 0.8493 - accuracy: 0.1233 - val_loss: 0.8552 - val_accuracy: 0.1168\n",
            "Epoch 19/20\n",
            "147/147 [==============================] - 8s 53ms/step - loss: 0.8475 - accuracy: 0.1233 - val_loss: 0.8423 - val_accuracy: 0.1168\n",
            "Epoch 20/20\n",
            "147/147 [==============================] - 8s 53ms/step - loss: 0.8161 - accuracy: 0.1233 - val_loss: 0.8659 - val_accuracy: 0.1168\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff4602eab80>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Verify that inputs are actually sequences of indices\n",
        "for i in range(20):\n",
        "  print(X[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D344otQb4Ish",
        "outputId": "a5838519-0d3b-4bd0-86ba-e0fb9f6fb308"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0 62]\n",
            "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0 113  68  10 174 249  15 493   2  15 885  18 102  84]\n",
            "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0  124  300\n",
            "   55   76  165   87   61 1464    9   22  431   12  114  342]\n",
            "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0 2411 4022 7623 5213  238  153    1   86   43]\n",
            "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0  30 202 417  45   1  73 517 873 269  14 809 142]\n",
            "[    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "   302    31    64    17   669  5259     1    17 17079    21  3219  1051\n",
            "    54   131  1897   112    12  1904  1059   160   168   239]\n",
            "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0  150   13  196\n",
            "    2   65 3027    2   30   91 2063  859  135   82   36 4967 1066    3\n",
            "    7 1039    1   78  673  286  961 4778 1039  634    3 3027]\n",
            "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    6    2   40   98   18  694  479 1652  230  150   59   40\n",
            "  435   64   40 4689   64   40   98   18  242  135   24    3]\n",
            "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0   13   19  429 1144    4]\n",
            "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    8\n",
            "  188    4    5  116   32   23 1743 1526    9  648   33  252]\n",
            "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0  211  746  131   51 3680   14  838]\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 8 4]\n",
            "[    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0   328\n",
            " 11328   211    25     1     7   185   371     7     3     1]\n",
            "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0 62]\n",
            "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0   81    4    7    1  157   12\n",
            "   38   17  568 2017   47  423   48 1416  157  173  149    3   56   17\n",
            "   10  497   17   47   59   66  152   56  578  170  867  273]\n",
            "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0  427 7286  905 4438  347 5063   56  113  109   96  675]\n",
            "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   2  45  14  17   5   7 160  17  12]\n",
            "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0  39 229]\n",
            "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0  222 1430]\n",
            "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    6\n",
            "  247  258   32  278    6  785   41   32   70  175 4162   78   27    1\n",
            "   16  263    1  499   27    9   10   21   16    3 6604  208]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Sanity check by running on a test case\n",
        "import tensorflow as tf\n",
        "print(X[0])\n",
        "print(X[0][0])\n",
        "print(\"Prediction:\")\n",
        "print(Y[0].shape)\n",
        "y = tf.expand_dims(Y[0],1)\n",
        "print(y.shape)\n",
        "alt5_model.predict(y)"
      ],
      "metadata": {
        "id": "R95YOk-Yyqld",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3606647b-ea8f-499e-88c5-3c0643904407"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0 62]\n",
            "0\n",
            "Prediction:\n",
            "(250,)\n",
            "(250, 1)\n",
            "8/8 [==============================] - 2s 4ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.7170818],\n",
              "       [3.9867063],\n",
              "       [3.9115052],\n",
              "       [3.7170818],\n",
              "       [3.5036733],\n",
              "       [3.8693016],\n",
              "       [3.7451138],\n",
              "       [4.342801 ],\n",
              "       [3.7526822],\n",
              "       [3.9926186],\n",
              "       [4.0642896],\n",
              "       [4.093241 ],\n",
              "       [4.266998 ],\n",
              "       [3.9227715],\n",
              "       [4.5270715],\n",
              "       [4.337521 ],\n",
              "       [4.3131766],\n",
              "       [3.9214077],\n",
              "       [4.188727 ],\n",
              "       [3.9509735],\n",
              "       [4.3546557],\n",
              "       [4.317021 ],\n",
              "       [4.2284527],\n",
              "       [3.773309 ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#It was interesting that the network seemed to have a \"default\" of 3.7170818, so let's do some stats\n",
        "print(df['student_star'][0])\n",
        "print(df['student_star'].mean())\n",
        "print(df['student_star'].mode())\n",
        "print(df['professor_name'].mode())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDVzwatbCYP2",
        "outputId": "98706e3e-2bdf-4fa0-9cb4-20bf1d075f82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5.0\n",
            "3.6573800659049085\n",
            "0    5.0\n",
            "Name: student_star, dtype: float64\n",
            "0    Robert Valenza \n",
            "Name: professor_name, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The difference between the output of the network for an empty embedding and the mean rating is less than .06. This seems to suggest the network is just guessing by assuming the output should be the mean up until actual comments are encountered."
      ],
      "metadata": {
        "id": "rxbFvac55T54"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **--- END OF SECTION 5 ---**"
      ],
      "metadata": {
        "id": "1wafXhJmhAyF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Approach 6**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## **One-Hot Encoding and Sequential Model on RateMyProfessor**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### This code is a machine learning model built using Keras and TensorFlow to predict student ratings based on their comments about a professor. The code starts by importing necessary libraries and loading the data from a CSV file. The text data is preprocessed by cleaning and tokenizing the comments column. The data is then split into train and test sets, tokenized, and padded to have the same length. A Sequential model is created with three dense layers, a dropout layer, and a dense layer. The model is then compiled with a different optimizer and trained on the train set for 10 epochs. Finally, the model is evaluated on the test set, and the accuracy is printed."
      ],
      "metadata": {
        "id": "FsKNSrfKKDsK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Mount google drive in a Google Collab\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "meeSN2ZAKJhh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f79a59ff-2369-4532-fbaf-c5c524edde92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#importing all the necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from numpy import array\n",
        "from tensorflow import keras\n",
        "from numpy import argmax\n",
        "from keras.utils import to_categorical\n",
        "import pandas as pd\n",
        "from keras.utils import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense"
      ],
      "metadata": {
        "id": "yf0uRXjEQXn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#reading the csv file from google drive and saving the data in df using pd\n",
        "df = pd.read_csv(r'/content/drive/My Drive/ANN_Project_2/RateMyProfessor_Sample data.csv')\n",
        "\n",
        "#check the columns\n",
        "print(df['comments'])\n",
        "print(df['student_difficult'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QjSal1QgQdHR",
        "outputId": "ac27d47f-b419-4313-ff7d-0db3b8e9ede9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0        This class is hard, but its a two-in-one gen-e...\n",
            "1        Definitely going to choose Prof. Looney\\'s cla...\n",
            "2        I overall enjoyed this class because the assig...\n",
            "3        Yes, it\\'s possible to get an A but you\\'ll de...\n",
            "4        Professor Looney has great knowledge in Astron...\n",
            "                               ...                        \n",
            "19995     Great sense of humor!!!! Love parasites now!!!!!\n",
            "19996    he is a really nice guy and is really funny..h...\n",
            "19997    His parasitology class is a lot of work but he...\n",
            "19998    He is WAY too much work for a 1 credit class. ...\n",
            "19999    Extremely easy lab teacher, quizzes are a litt...\n",
            "Name: comments, Length: 20000, dtype: object\n",
            "0        3.0\n",
            "1        2.0\n",
            "2        3.0\n",
            "3        3.0\n",
            "4        1.0\n",
            "        ... \n",
            "19995    5.0\n",
            "19996    4.0\n",
            "19997    3.0\n",
            "19998    5.0\n",
            "19999    2.0\n",
            "Name: student_difficult, Length: 20000, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "# Remove 'nan' values from 'comments' column\n",
        "df = df.dropna(subset=['comments'])\n",
        "\n",
        "# Remove stopwords, numerical values, and symbols from 'comments' column and convert to lowercase\n",
        "stop_words = stopwords.words('english')\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "    text = re.sub(r'\\d+', ' ', text)\n",
        "    text = \" \".join([word for word in text.split() if word not in stop_words])\n",
        "    return text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKPtQ9huRDOw",
        "outputId": "cb5906e2-2d76-4b30-fd3b-a358c2605c8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#text cleaning/preprocessing\n",
        "df['comments'] = df['comments'].apply(lambda x: clean_text(x))\n",
        "\n",
        "#storing the column values in comment\n",
        "comments = df['comments']"
      ],
      "metadata": {
        "id": "Hawv9d1ORJUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#implementing one-hot encoding\n",
        "comments_one = pd.get_dummies(comments)\n",
        "\n",
        "df['student_difficult'].shape\n",
        "comments_one.head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 890
        },
        "id": "oAV-S46aRlTx",
        "outputId": "3b5ae883-f8ed-42fb-a32f-9e388c41f4cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      \\\n",
              "0  0   \n",
              "1  0   \n",
              "2  0   \n",
              "3  0   \n",
              "4  0   \n",
              "5  0   \n",
              "6  0   \n",
              "7  0   \n",
              "8  0   \n",
              "9  0   \n",
              "\n",
              "   aaagh college dean husband sense part architecture part curriculum sometimes see coming run away bother pay lots money  \\\n",
              "0                                                  0                                                                        \n",
              "1                                                  0                                                                        \n",
              "2                                                  0                                                                        \n",
              "3                                                  0                                                                        \n",
              "4                                                  0                                                                        \n",
              "5                                                  0                                                                        \n",
              "6                                                  0                                                                        \n",
              "7                                                  0                                                                        \n",
              "8                                                  0                                                                        \n",
              "9                                                  0                                                                        \n",
              "\n",
              "   aaron kozbelt knows subject knows explain concepts organized presentations clear would liked courses professors brooklyn college watching teach brush lecture skills  \\\n",
              "0                                                  0                                                                                                                      \n",
              "1                                                  0                                                                                                                      \n",
              "2                                                  0                                                                                                                      \n",
              "3                                                  0                                                                                                                      \n",
              "4                                                  0                                                                                                                      \n",
              "5                                                  0                                                                                                                      \n",
              "6                                                  0                                                                                                                      \n",
              "7                                                  0                                                                                                                      \n",
              "8                                                  0                                                                                                                      \n",
              "9                                                  0                                                                                                                      \n",
              "\n",
              "   aarrggh fricking unbelievable decent guy bugger know teach makes physics sound like rocket science makes ants picnic end world like texan accent though  \\\n",
              "0                                                  0                                                                                                         \n",
              "1                                                  0                                                                                                         \n",
              "2                                                  0                                                                                                         \n",
              "3                                                  0                                                                                                         \n",
              "4                                                  0                                                                                                         \n",
              "5                                                  0                                                                                                         \n",
              "6                                                  0                                                                                                         \n",
              "7                                                  0                                                                                                         \n",
              "8                                                  0                                                                                                         \n",
              "9                                                  0                                                                                                         \n",
              "\n",
              "   abbas  abbas hilarious  \\\n",
              "0      0                0   \n",
              "1      0                0   \n",
              "2      0                0   \n",
              "3      0                0   \n",
              "4      0                0   \n",
              "5      0                0   \n",
              "6      0                0   \n",
              "7      0                0   \n",
              "8      0                0   \n",
              "9      0                0   \n",
              "\n",
              "   abert genuinely nice person shows enthusiasm invest sees invest learning material math people inevitably going practice constantly order fully grasp learning hw problems ever challenging hw tests eazy  \\\n",
              "0                                                  0                                                                                                                                                          \n",
              "1                                                  0                                                                                                                                                          \n",
              "2                                                  0                                                                                                                                                          \n",
              "3                                                  0                                                                                                                                                          \n",
              "4                                                  0                                                                                                                                                          \n",
              "5                                                  0                                                                                                                                                          \n",
              "6                                                  0                                                                                                                                                          \n",
              "7                                                  0                                                                                                                                                          \n",
              "8                                                  0                                                                                                                                                          \n",
              "9                                                  0                                                                                                                                                          \n",
              "\n",
              "   ability walk room bring silence terrifies time think awesome doubt could mean students really high standards expects work hard live underneath exterior actually care students lot  \\\n",
              "0                                                  0                                                                                                                                    \n",
              "1                                                  0                                                                                                                                    \n",
              "2                                                  0                                                                                                                                    \n",
              "3                                                  0                                                                                                                                    \n",
              "4                                                  0                                                                                                                                    \n",
              "5                                                  0                                                                                                                                    \n",
              "6                                                  0                                                                                                                                    \n",
              "7                                                  0                                                                                                                                    \n",
              "8                                                  0                                                                                                                                    \n",
              "9                                                  0                                                                                                                                    \n",
              "\n",
              "   able explain apply everything well also thought knew everything fair grading papers test  \\\n",
              "0                                                  0                                          \n",
              "1                                                  0                                          \n",
              "2                                                  0                                          \n",
              "3                                                  0                                          \n",
              "4                                                  0                                          \n",
              "5                                                  0                                          \n",
              "6                                                  0                                          \n",
              "7                                                  0                                          \n",
              "8                                                  0                                          \n",
              "9                                                  0                                          \n",
              "\n",
              "   able explain things well relating classmates tell really knows loves information teaches wants understand everything could also teach class  \\\n",
              "0                                                  0                                                                                             \n",
              "1                                                  0                                                                                             \n",
              "2                                                  0                                                                                             \n",
              "3                                                  0                                                                                             \n",
              "4                                                  0                                                                                             \n",
              "5                                                  0                                                                                             \n",
              "6                                                  0                                                                                             \n",
              "7                                                  0                                                                                             \n",
              "8                                                  0                                                                                             \n",
              "9                                                  0                                                                                             \n",
              "\n",
              "   ...  \\\n",
              "0  ...   \n",
              "1  ...   \n",
              "2  ...   \n",
              "3  ...   \n",
              "4  ...   \n",
              "5  ...   \n",
              "6  ...   \n",
              "7  ...   \n",
              "8  ...   \n",
              "9  ...   \n",
              "\n",
              "   z great teacher knowledgeable passionate subject lectures clear point often hilarious surprised much learned bec class seems casual found retaining material almost year since took class still identify rocks minerals fly  \\\n",
              "0                                                  0                                                                                                                                                                             \n",
              "1                                                  0                                                                                                                                                                             \n",
              "2                                                  0                                                                                                                                                                             \n",
              "3                                                  0                                                                                                                                                                             \n",
              "4                                                  0                                                                                                                                                                             \n",
              "5                                                  0                                                                                                                                                                             \n",
              "6                                                  0                                                                                                                                                                             \n",
              "7                                                  0                                                                                                                                                                             \n",
              "8                                                  0                                                                                                                                                                             \n",
              "9                                                  0                                                                                                                                                                             \n",
              "\n",
              "   zahajko worst instructor ever chose favorite students students grade classmates tests absolute violation privacy raced lectures called student like front whole class plain rude absolutely recommend  \\\n",
              "0                                                  0                                                                                                                                                       \n",
              "1                                                  0                                                                                                                                                       \n",
              "2                                                  0                                                                                                                                                       \n",
              "3                                                  0                                                                                                                                                       \n",
              "4                                                  0                                                                                                                                                       \n",
              "5                                                  0                                                                                                                                                       \n",
              "6                                                  0                                                                                                                                                       \n",
              "7                                                  0                                                                                                                                                       \n",
              "8                                                  0                                                                                                                                                       \n",
              "9                                                  0                                                                                                                                                       \n",
              "\n",
              "   zeal compassion better students anyone around unmatched honor professor coach approachable subject matter expert hence without doubt fully confident recommend genuine mentor leader mis business intelligence database  \\\n",
              "0                                                  0                                                                                                                                                                         \n",
              "1                                                  0                                                                                                                                                                         \n",
              "2                                                  0                                                                                                                                                                         \n",
              "3                                                  0                                                                                                                                                                         \n",
              "4                                                  0                                                                                                                                                                         \n",
              "5                                                  0                                                                                                                                                                         \n",
              "6                                                  0                                                                                                                                                                         \n",
              "7                                                  0                                                                                                                                                                         \n",
              "8                                                  0                                                                                                                                                                         \n",
              "9                                                  0                                                                                                                                                                         \n",
              "\n",
              "   zeman man take tests make percent grade questions test directly quizzes plus gives essays ahead time bad quiz wk major pain avg tho paper nearly impossible write grades easily lowest grade youll prob get b  \\\n",
              "0                                                  0                                                                                                                                                               \n",
              "1                                                  0                                                                                                                                                               \n",
              "2                                                  0                                                                                                                                                               \n",
              "3                                                  0                                                                                                                                                               \n",
              "4                                                  0                                                                                                                                                               \n",
              "5                                                  0                                                                                                                                                               \n",
              "6                                                  0                                                                                                                                                               \n",
              "7                                                  0                                                                                                                                                               \n",
              "8                                                  0                                                                                                                                                               \n",
              "9                                                  0                                                                                                                                                               \n",
              "\n",
              "   zeno redundant teacher avoid possible gives ton pointless work hard keep really teach anything related philosophy total waste time  \\\n",
              "0                                                  0                                                                                    \n",
              "1                                                  0                                                                                    \n",
              "2                                                  0                                                                                    \n",
              "3                                                  0                                                                                    \n",
              "4                                                  0                                                                                    \n",
              "5                                                  0                                                                                    \n",
              "6                                                  0                                                                                    \n",
              "7                                                  0                                                                                    \n",
              "8                                                  0                                                                                    \n",
              "9                                                  0                                                                                    \n",
              "\n",
              "   zoology best course taken far across different colleges difficult pass extremely interesting classroom loaded specimens craig amazing professor pleasant person around general dress weather prepared outdoor excursions time  \\\n",
              "0                                                  0                                                                                                                                                                               \n",
              "1                                                  0                                                                                                                                                                               \n",
              "2                                                  0                                                                                                                                                                               \n",
              "3                                                  0                                                                                                                                                                               \n",
              "4                                                  0                                                                                                                                                                               \n",
              "5                                                  0                                                                                                                                                                               \n",
              "6                                                  0                                                                                                                                                                               \n",
              "7                                                  0                                                                                                                                                                               \n",
              "8                                                  0                                                                                                                                                                               \n",
              "9                                                  0                                                                                                                                                                               \n",
              "\n",
              "   zorn intriguing intelligent guy man little crazy high standards essays basis entire grade dedication helping writing progress unmatched difficult grader really engaging discussion overall incredibly helpful character amazing  \\\n",
              "0                                                  0                                                                                                                                                                                  \n",
              "1                                                  0                                                                                                                                                                                  \n",
              "2                                                  0                                                                                                                                                                                  \n",
              "3                                                  0                                                                                                                                                                                  \n",
              "4                                                  0                                                                                                                                                                                  \n",
              "5                                                  0                                                                                                                                                                                  \n",
              "6                                                  0                                                                                                                                                                                  \n",
              "7                                                  0                                                                                                                                                                                  \n",
              "8                                                  0                                                                                                                                                                                  \n",
              "9                                                  0                                                                                                                                                                                  \n",
              "\n",
              "   zorn one favorites wacky right ways makes coming class worth reading manageable papers challenging grades pretty hard name make better writer zorn always help learned lot classes money well spent scu  \\\n",
              "0                                                  0                                                                                                                                                         \n",
              "1                                                  0                                                                                                                                                         \n",
              "2                                                  0                                                                                                                                                         \n",
              "3                                                  0                                                                                                                                                         \n",
              "4                                                  0                                                                                                                                                         \n",
              "5                                                  0                                                                                                                                                         \n",
              "6                                                  0                                                                                                                                                         \n",
              "7                                                  0                                                                                                                                                         \n",
              "8                                                  0                                                                                                                                                         \n",
              "9                                                  0                                                                                                                                                         \n",
              "\n",
              "   zzzzz huh oh credit hour class hour nap class absolutly pointless required majors minors like mr feeny may bad  \\\n",
              "0                                                  0                                                                \n",
              "1                                                  0                                                                \n",
              "2                                                  0                                                                \n",
              "3                                                  0                                                                \n",
              "4                                                  0                                                                \n",
              "5                                                  0                                                                \n",
              "6                                                  0                                                                \n",
              "7                                                  0                                                                \n",
              "8                                                  0                                                                \n",
              "9                                                  0                                                                \n",
              "\n",
              "   zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz  \n",
              "0                                                  0                         \n",
              "1                                                  0                         \n",
              "2                                                  0                         \n",
              "3                                                  0                         \n",
              "4                                                  0                         \n",
              "5                                                  0                         \n",
              "6                                                  0                         \n",
              "7                                                  0                         \n",
              "8                                                  0                         \n",
              "9                                                  0                         \n",
              "\n",
              "[10 rows x 18666 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-95479ea8-e6a5-41e6-99ce-5b6ca3671aae\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>aaagh college dean husband sense part architecture part curriculum sometimes see coming run away bother pay lots money</th>\n",
              "      <th>aaron kozbelt knows subject knows explain concepts organized presentations clear would liked courses professors brooklyn college watching teach brush lecture skills</th>\n",
              "      <th>aarrggh fricking unbelievable decent guy bugger know teach makes physics sound like rocket science makes ants picnic end world like texan accent though</th>\n",
              "      <th>abbas</th>\n",
              "      <th>abbas hilarious</th>\n",
              "      <th>abert genuinely nice person shows enthusiasm invest sees invest learning material math people inevitably going practice constantly order fully grasp learning hw problems ever challenging hw tests eazy</th>\n",
              "      <th>ability walk room bring silence terrifies time think awesome doubt could mean students really high standards expects work hard live underneath exterior actually care students lot</th>\n",
              "      <th>able explain apply everything well also thought knew everything fair grading papers test</th>\n",
              "      <th>able explain things well relating classmates tell really knows loves information teaches wants understand everything could also teach class</th>\n",
              "      <th>...</th>\n",
              "      <th>z great teacher knowledgeable passionate subject lectures clear point often hilarious surprised much learned bec class seems casual found retaining material almost year since took class still identify rocks minerals fly</th>\n",
              "      <th>zahajko worst instructor ever chose favorite students students grade classmates tests absolute violation privacy raced lectures called student like front whole class plain rude absolutely recommend</th>\n",
              "      <th>zeal compassion better students anyone around unmatched honor professor coach approachable subject matter expert hence without doubt fully confident recommend genuine mentor leader mis business intelligence database</th>\n",
              "      <th>zeman man take tests make percent grade questions test directly quizzes plus gives essays ahead time bad quiz wk major pain avg tho paper nearly impossible write grades easily lowest grade youll prob get b</th>\n",
              "      <th>zeno redundant teacher avoid possible gives ton pointless work hard keep really teach anything related philosophy total waste time</th>\n",
              "      <th>zoology best course taken far across different colleges difficult pass extremely interesting classroom loaded specimens craig amazing professor pleasant person around general dress weather prepared outdoor excursions time</th>\n",
              "      <th>zorn intriguing intelligent guy man little crazy high standards essays basis entire grade dedication helping writing progress unmatched difficult grader really engaging discussion overall incredibly helpful character amazing</th>\n",
              "      <th>zorn one favorites wacky right ways makes coming class worth reading manageable papers challenging grades pretty hard name make better writer zorn always help learned lot classes money well spent scu</th>\n",
              "      <th>zzzzz huh oh credit hour class hour nap class absolutly pointless required majors minors like mr feeny may bad</th>\n",
              "      <th>zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows × 18666 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-95479ea8-e6a5-41e6-99ce-5b6ca3671aae')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-95479ea8-e6a5-41e6-99ce-5b6ca3671aae button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-95479ea8-e6a5-41e6-99ce-5b6ca3671aae');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into train and test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = comments_one\n",
        "y = df['student_difficult']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "id": "2jRi6kt6TtiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the comments column of the train and test sets\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "# Pad the sequences of the train and test sets to have the same length\n",
        "max_length = max([len(seq) for seq in X_train_seq])\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=max_length)\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=max_length)"
      ],
      "metadata": {
        "id": "nE8xHMmxTyeR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training model\n",
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.Dense(128, activation = 'relu'))\n",
        "model.add(keras.layers.Dropout(0.5))\n",
        "model.add(keras.layers.Dense(64, activation = 'relu'))\n",
        "model.add(keras.layers.Dropout(0.2))\n",
        "model.add(keras.layers.Dense(32, activation='sigmoid'))\n",
        "\n",
        "# Output layer for 'student_star'\n",
        "model.add(keras.layers.Dense(6, activation='softmax', name='student_difficuly'))\n",
        "\n",
        "# Compile the model with a different optimizer\n",
        "optimizer = keras.optimizers.RMSprop(lr=0.001)\n",
        "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFhvHqkdT2KJ",
        "outputId": "daf2334a-7d28-442d-da75-e864d8d81514"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.RMSprop.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model on the train set for more epochs\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=128, validation_split=0.3)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print('Test Accuracy', accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFuzjJWgT7H2",
        "outputId": "d6f89a67-d46f-4ed8-a1cc-a550e6e29120"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "77/77 [==============================] - 3s 22ms/step - loss: 1.6885 - accuracy: 0.2342 - val_loss: 1.6122 - val_accuracy: 0.2758\n",
            "Epoch 2/10\n",
            "77/77 [==============================] - 1s 7ms/step - loss: 1.6052 - accuracy: 0.2681 - val_loss: 1.5944 - val_accuracy: 0.2758\n",
            "Epoch 3/10\n",
            "77/77 [==============================] - 0s 6ms/step - loss: 1.5935 - accuracy: 0.2681 - val_loss: 1.5907 - val_accuracy: 0.2758\n",
            "Epoch 4/10\n",
            "77/77 [==============================] - 0s 6ms/step - loss: 1.5879 - accuracy: 0.2702 - val_loss: 1.5882 - val_accuracy: 0.2758\n",
            "Epoch 5/10\n",
            "77/77 [==============================] - 1s 7ms/step - loss: 1.5832 - accuracy: 0.2711 - val_loss: 1.5897 - val_accuracy: 0.2758\n",
            "Epoch 6/10\n",
            "77/77 [==============================] - 0s 6ms/step - loss: 1.5728 - accuracy: 0.2810 - val_loss: 1.5898 - val_accuracy: 0.2753\n",
            "Epoch 7/10\n",
            "77/77 [==============================] - 1s 7ms/step - loss: 1.5444 - accuracy: 0.3426 - val_loss: 1.5915 - val_accuracy: 0.2753\n",
            "Epoch 8/10\n",
            "77/77 [==============================] - 0s 6ms/step - loss: 1.4593 - accuracy: 0.4241 - val_loss: 1.5904 - val_accuracy: 0.2655\n",
            "Epoch 9/10\n",
            "77/77 [==============================] - 0s 6ms/step - loss: 1.2911 - accuracy: 0.4702 - val_loss: 1.6057 - val_accuracy: 0.2422\n",
            "Epoch 10/10\n",
            "77/77 [==============================] - 1s 7ms/step - loss: 1.1034 - accuracy: 0.5558 - val_loss: 1.6546 - val_accuracy: 0.2405\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 1.6675 - accuracy: 0.2342\n",
            "Test Accuracy 0.23424474895000458\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **--- END OF SECTION 6 ---**"
      ],
      "metadata": {
        "id": "RoSHDJctVcbg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Approach 7**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## **Sentimental Analysis on RateMyProfessor Data using Glove Embedding and Bidirectional LSTM layers**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### The code is implementation for sentiment analysis of student reviews of professors on RateMyProfessor.com. It predicts a professor's teaching efficacy and difficulty using a Bidirectional LSTM neural network architecture with pre-trained GloVe word embeddings.\n",
        "\n",
        "### Here is a breakdown of the code:\n",
        "\n",
        "###1. Import necessary libraries\n",
        "\n",
        "###2. Preprocess the data by converting the text to lowercase, removing punctuation, tokenizing the text into words, and removing stop words.\n",
        "\n",
        "###3. Tokenize the comments column and pad the sequences to have the same length.\n",
        "\n",
        "###4. Split the data into train and test sets.\n",
        "\n",
        "###5. Load pre-trained GloVe embeddings and create an embedding matrix.\n",
        "\n",
        "###6. The model consists of an Embedding layer, a Bidirectional LSTM layer, and a Dense layer with linear activation function. The Embedding layer uses the pre-trained GloVe embeddings as weights and is set to non-trainable.\n",
        "\n",
        "###7. Compile the model with RMSprop optimizer, binary_crossentropy loss function, and metrics of mean squared error and accuracy.\n",
        "\n",
        "###8. Train the model with the training data for 5 epochs and batch size of 128, and validate with 20% of the training data.\n",
        "\n",
        "###9. Print the model summary and evaluate its performance on the test data."
      ],
      "metadata": {
        "id": "fagOtbFipkbR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from gensim.models import Word2Vec\n",
        "import string\n",
        "from google.colab import drive\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
        "import re\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "# Load the dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/RateMyProfessor_Sample_data.csv')\n",
        "nltk.download('stopwords')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hujl5_AGptZT",
        "outputId": "1f8ff048-f914-445f-8667-6265575a546f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows with missing values\n",
        "df=df.dropna(subset=['comments','student_star','student_difficult'])"
      ],
      "metadata": {
        "id": "-kHwjrhqqGs4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df['comments']"
      ],
      "metadata": {
        "id": "zGz8JuWeqKG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import pad_sequences\n",
        "\n",
        "# Tokenize the comments column\n",
        "tokenizer = Tokenizer()\n",
        "df['comments'] = df['comments'].fillna('')\n",
        "\n",
        "tokenizer.fit_on_texts(df['comments'])\n",
        "sequences = tokenizer.texts_to_sequences(df['comments'])\n",
        "\n",
        "# Pad the sequences to have the same length\n",
        "max_length = max([len(seq) for seq in sequences])\n",
        "X = pad_sequences(sequences, maxlen=max_length)"
      ],
      "metadata": {
        "id": "71zLcWJnqUyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "y = df[['student_star', 'student_difficult']].values\n",
        "y = np.argmax(y, axis=1)"
      ],
      "metadata": {
        "id": "Ti5Wjf5Vqdwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "glove_path='/content/drive/MyDrive/glove.42B.300d.txt'\n",
        "embedding_dim=300"
      ],
      "metadata": {
        "id": "Jiufg97Hqe5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Split the data into train and test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n"
      ],
      "metadata": {
        "id": "ud1xPKyrqqgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_glove_embeddings(embedding_path, word_index, embedding_dim):\n",
        "    num_words = len(word_index) + 1\n",
        "    if isinstance(embedding_dim, np.ndarray):\n",
        "        embedding_dim = embedding_dim.item()\n",
        "    embeddings_matrix = np.zeros((num_words, embedding_dim))\n",
        "\n",
        "    embeddings_index = {}\n",
        "    with open(embedding_path) as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            coefs = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings_index[word] = coefs\n",
        "\n",
        "    for word, i in word_index.items():\n",
        "        if i >= num_words:\n",
        "            continue\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embeddings_matrix[i] = embedding_vector\n",
        "\n",
        "    return embeddings_matrix\n"
      ],
      "metadata": {
        "id": "R2-YrKxeq05g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "embeddings_matrix = load_glove_embeddings(glove_path, tokenizer.word_index, embedding_dim=300)\n"
      ],
      "metadata": {
        "id": "9vQM8pteq4_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.Embedding(embeddings_matrix.shape[0],embeddings_matrix.shape[1],weights = [embeddings_matrix],trainable= False))\n",
        "\n",
        "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)))\n",
        "model.add(keras.layers.Dense(2, activation='linear'))\n",
        "\n",
        "optimizer = keras.optimizers.RMSprop(learning_rate=0.001)\n",
        "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['mse','accuracy'])\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K29DtU6gq9JA",
        "outputId": "e8052e43-f564-4ad9-a3a3-1575dcc54268"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, None, 300)         4939200   \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 64)               85248     \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dense (Dense)               (None, 2)                 130       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,024,578\n",
            "Trainable params: 85,378\n",
            "Non-trainable params: 4,939,200\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train, y_train, epochs=5, batch_size=128, validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6o6DFF2rD87",
        "outputId": "b393ece3-b755-43c8-e070-047a69c8bd26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "100/100 [==============================] - 12s 33ms/step - loss: 1.6670 - mse: 0.3218 - accuracy: 0.6136 - val_loss: 0.5527 - val_mse: 0.1791 - val_accuracy: 0.7230\n",
            "Epoch 2/5\n",
            "100/100 [==============================] - 2s 20ms/step - loss: 0.5496 - mse: 0.1723 - accuracy: 0.7472 - val_loss: 0.5101 - val_mse: 0.1584 - val_accuracy: 0.7677\n",
            "Epoch 3/5\n",
            "100/100 [==============================] - 2s 16ms/step - loss: 0.4948 - mse: 0.1529 - accuracy: 0.7893 - val_loss: 0.4760 - val_mse: 0.1486 - val_accuracy: 0.7974\n",
            "Epoch 4/5\n",
            "100/100 [==============================] - 2s 15ms/step - loss: 0.4767 - mse: 0.1452 - accuracy: 0.7997 - val_loss: 0.4715 - val_mse: 0.1435 - val_accuracy: 0.8009\n",
            "Epoch 5/5\n",
            "100/100 [==============================] - 2s 17ms/step - loss: 0.4604 - mse: 0.1397 - accuracy: 0.8097 - val_loss: 0.4980 - val_mse: 0.1527 - val_accuracy: 0.7918\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss,mse, accuracy = model.evaluate(X_test, y_test)\n",
        "print('Test Accuracy', accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGA7gp87rLH5",
        "outputId": "18c81da2-8b64-418c-a591-02c85dac4daf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "125/125 [==============================] - 2s 16ms/step - loss: 0.4830 - mse: 0.1513 - accuracy: 0.7974\n",
            "Test Accuracy 0.7974493503570557\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **--- END OF SECTION 7 ---**"
      ],
      "metadata": {
        "id": "C0wfx47swKrA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**APPROACH 8**\n",
        "---\n",
        "## **REGRESSION MODEL -2**\n",
        "---\n",
        "### In this model, we used '***Gensim***' package to load and access '***GloVE***' word embeddings, used '***spacy***' and '***nltk***' language models to preprocess the data in provided corpus '***RateMyProfessor_Sample data***', then built a '***regression model***' with a ***LSTM layer***, followed by a ***Dense layer***, followed by a ***Droput layer***, followed by ***two*** ***individual*** ***dense output layers*** that predicts the scores/ratings for '***Quality***' and '***Difficulty***' of the professor based on the comments."
      ],
      "metadata": {
        "id": "M_iVdMYtZZZy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WlsmXo1lrRAo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a8d7f2b-d466-4f1b-f852-9508cb340082"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SEI-XJ6rqHw",
        "outputId": "9d2587b4-b667-4498-9ac4-30bb12191ce5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.10.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.3.0)\n",
            "2023-04-29 05:55:11.465140: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-04-29 05:55:13.516369: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-04-29 05:55:16.987204: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-04-29 05:55:16.987763: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-04-29 05:55:16.988017: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-lg==3.5.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.5.0/en_core_web_lg-3.5.0-py3-none-any.whl (587.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.7/587.7 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-lg==3.5.0) (3.5.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (6.3.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.0.8)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.10.7)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.0.9)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.3.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.4.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.1.2)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (8.1.9)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.0.12)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (4.65.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (23.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.27.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.22.4)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.1.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (67.7.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.0.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.0.8)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.10.1)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.7.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n"
          ]
        }
      ],
      "source": [
        "# Installing required packages\n",
        "!pip install gensim\n",
        "!python -m spacy download en_core_web_lg\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we installed **gensim** package to load and access the **GloVe** embeddings,and downloaded **en_core_web_lg** language model from **spacy** to preprocess the corpus that we are provided with to use to train the model.\n"
      ],
      "metadata": {
        "id": "trlCsJdXpmtd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-DORv9nr8HY"
      },
      "outputs": [],
      "source": [
        "# Importing the required default libraries.\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "from keras.utils import pad_sequences\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "suhqjybfsUVL"
      },
      "outputs": [],
      "source": [
        "# converting glove to gensim word2vec format\n",
        "\n",
        "# glove_file = '/content/drive/MyDrive/glove.42B.300d.txt'\n",
        "# output_file = \"glove_word2vec.txt\"\n",
        "\n",
        "# glove2word2vec(glove_file, output_file)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "Don't need to run the above three lines of code again as we already converted glove to gensim word2vec format as \"glove_word2vec.txt\".\n",
        "if you don't have that file, you need to run this code again.\n",
        "better don't run the code as it may consume most of your colab resources and takes around 15 minutes of time.\n",
        "Instead, use the file \" glove_word2vec.txt\"\n",
        "\"\"\"\n",
        "\n",
        "model_glove = KeyedVectors.load_word2vec_format(\"glove_word2vec.txt\")\n",
        "\n",
        "word_vectors = model_glove\n",
        "#word_vectors.save('word_vectors.kv')\n",
        "#word_vectors = KeyedVectors.load('/content/word_vectors.kv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we converted ***GloVE*** embeddings into ***word2vec*** format of ***genism*** to easily access the word embeddings. Then, we loaded the converted ***word2vec*** format of ***GloVE*** as ***model_glove*** and created an instance of ***model_glove*** as ***word_vectors*** to save it as \"***word_vectors.kv***\", which is a ***Keyed vectors*** format of ***gensim***.\n",
        "\n",
        "***Note***: if you already have \"***word_vectors.kv***\" file in your notebook, you can skip all the lines of code until the last commented line and can use only the last commented line of code in the cell to load the file, this saves a lot of time and resources as it only takes a few seconds of time to load it."
      ],
      "metadata": {
        "id": "h1aBumdzuqD9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7sEi3FeyhRF",
        "outputId": "690782f7-817e-49f4-8d44-1148f2eea8ff"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 9.0617e-02,  3.2209e-01, -2.8606e-01, -4.3576e-01,  6.5842e-01,\n",
              "       -3.0986e-01, -3.2205e+00, -3.3805e-01, -1.1715e-01, -2.6288e-01,\n",
              "       -5.0936e-01, -1.7889e-03,  1.1095e-01,  2.0856e-01, -1.8062e-01,\n",
              "       -1.2951e-01,  3.0683e-02, -3.9619e-01, -2.4232e-02, -2.6083e-01,\n",
              "        8.3488e-02,  2.5158e-01,  1.8634e-01, -2.2445e-01,  2.0258e-01,\n",
              "       -1.7600e-01,  1.3958e-01,  1.5411e-01,  9.2125e-03,  2.2933e-01,\n",
              "        4.8343e-01,  5.4896e-02, -1.7834e-01,  2.9386e-01,  7.3732e-02,\n",
              "       -3.6977e-01,  3.3151e-02, -1.8546e-01, -1.4493e-02, -4.5637e-02,\n",
              "        3.0009e-01,  8.9469e-02, -2.2406e-01, -2.1427e-01,  1.4199e-01,\n",
              "        2.8529e-02,  1.5734e-01, -7.4364e-03,  1.9294e-01,  1.0349e-01,\n",
              "        4.7936e-02,  5.7523e-01, -3.7558e-01,  2.3186e-02, -3.2913e-01,\n",
              "        7.1907e-02,  2.9372e-02, -8.0111e-01, -1.3413e-01,  1.0208e-01,\n",
              "        1.1508e-01, -8.9648e-02,  1.8926e-01,  4.1204e-01, -1.0060e-01,\n",
              "       -4.0713e-01,  1.3602e-01,  1.6024e-01, -2.5463e-01, -6.6808e-01,\n",
              "        4.1232e-02, -1.5826e-01, -9.9248e-02, -1.0469e-01, -1.0647e-01,\n",
              "       -4.1015e-01, -3.3972e-01, -1.2510e-01, -3.5024e-01,  1.7109e-01,\n",
              "        2.9113e-01,  1.1340e-01, -2.1826e-02, -3.5087e-01, -4.0309e-01,\n",
              "        1.2672e-02, -1.7738e-01,  6.1339e-03, -1.3067e-01,  1.7302e-01,\n",
              "       -2.2322e-01,  3.0287e-01, -3.0024e-01,  4.7801e-01, -2.5972e-01,\n",
              "       -4.7774e-01, -2.6068e+00, -1.4816e-01, -1.1035e-01,  2.0250e-01,\n",
              "        1.5175e-01, -3.0543e-01,  3.2334e-01, -5.3779e-01,  3.3308e-01,\n",
              "        2.9739e-01,  1.1573e-01, -2.8508e-01, -1.3615e-01, -1.8367e-01,\n",
              "       -3.9057e-01, -4.2034e-01, -1.6214e-01, -4.0934e-01, -3.5248e-01,\n",
              "       -3.4447e-01,  3.0702e-01,  4.4001e-01,  2.8161e-01,  1.1679e-01,\n",
              "        7.2267e-02,  2.7912e-01, -9.4945e-02,  2.9758e-01,  3.4834e-01,\n",
              "        2.0057e-01, -5.1913e-02,  1.8811e-01,  4.3703e-02,  1.4880e-01,\n",
              "        9.1240e-02,  2.0745e-01,  2.0679e-01, -1.3041e-01, -5.1733e-01,\n",
              "       -2.7349e-02, -5.0306e-02, -5.5440e-01,  2.2809e-01,  3.1068e-01,\n",
              "       -5.3285e-02,  2.4501e-01,  4.7425e-01, -3.0602e-01, -4.7253e-01,\n",
              "        2.0317e-01, -2.3854e-02, -2.4755e-02,  5.6062e-01, -5.0340e-01,\n",
              "        3.2207e-01, -9.1937e-02,  8.6710e-03,  3.2744e-01, -6.2972e-03,\n",
              "        2.1944e-01, -1.7810e-01,  2.9138e-01, -4.1701e-01,  4.6112e-02,\n",
              "        6.3306e-01, -8.3864e-03,  3.5105e-01, -4.5087e-02, -2.3854e-01,\n",
              "        7.1880e-02, -3.2789e-01, -7.8777e-02, -8.9306e-02,  9.4049e-02,\n",
              "        2.8973e-01,  1.7340e-01,  2.8600e-01, -2.5491e-01, -4.3508e-02,\n",
              "        4.9839e-02, -3.0879e-02, -3.5098e-01, -1.7628e-02,  1.0937e-02,\n",
              "       -1.5428e-01,  2.8358e-02,  8.2269e-03, -1.3461e-01,  3.7129e-01,\n",
              "        4.1337e-01, -3.0814e-01,  1.8888e-01,  2.3987e-01,  3.4187e-01,\n",
              "       -1.0762e-01, -4.6884e-01, -1.6021e-01,  3.0738e-01, -6.1537e-01,\n",
              "        5.4625e-01, -6.6467e-02, -1.9527e-01,  1.4443e-01, -9.9351e-02,\n",
              "        4.7491e-02, -1.9135e-01,  3.0656e-01,  1.4507e-01, -4.2766e-01,\n",
              "       -6.0907e-03,  2.7497e-01,  3.3711e-01,  9.3519e-03, -2.1571e-01,\n",
              "       -1.3055e-01,  4.8344e-01, -3.2124e-01, -2.3795e-01,  1.7969e-01,\n",
              "        2.4110e-01, -1.1740e-01,  2.0077e-01, -3.4689e-01, -3.6073e-01,\n",
              "        1.7423e-01, -1.2627e-01,  1.8431e-01, -4.4600e-01, -2.1699e+00,\n",
              "       -3.2822e-01,  2.0003e-01, -3.7549e-01, -4.8700e-01,  4.8786e-01,\n",
              "        7.4258e-02, -5.9419e-01, -3.9532e-01, -3.9491e-01,  1.3971e-01,\n",
              "       -1.0246e-01, -1.7951e-01,  8.8448e-02,  8.9953e-02, -3.5068e-01,\n",
              "        1.6270e-01,  3.2671e-02, -3.6493e-01, -1.0070e-01,  7.8094e-02,\n",
              "        2.4380e-01,  7.6059e-02,  1.4710e-01, -2.7329e-01,  4.9332e-02,\n",
              "        3.9787e-01,  6.7721e-01, -1.3592e-01, -5.3221e-01, -1.6551e-02,\n",
              "        6.4887e-02,  3.2922e-01, -3.5826e-01, -3.1293e-02,  7.6553e-03,\n",
              "        3.3357e-01,  1.4766e-01,  1.8085e-01, -2.0623e-01,  6.8861e-01,\n",
              "       -2.2257e-01,  1.0946e-02,  1.3595e-01, -3.7181e-01,  1.2206e-01,\n",
              "       -1.3031e-02,  5.2083e-01, -2.9012e-01, -2.1221e-02, -2.5959e-01,\n",
              "        5.3814e-02, -2.1849e-01, -9.6463e-02, -4.5842e-01,  3.7008e-02,\n",
              "        1.7288e-01, -1.8977e-01,  1.7351e-01,  2.6872e-01,  6.0551e-01,\n",
              "        5.9281e-01, -2.7306e-02,  3.3251e-01,  3.7357e-01, -1.9623e-01,\n",
              "       -3.0062e-01, -3.3842e-01, -2.9291e-01,  2.0078e-01,  4.3498e-01,\n",
              "       -1.9842e-01, -2.0954e-01,  1.3294e-01, -2.7448e-01, -1.0664e-05],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# get the word embeddings of a word\n",
        "word_vectors['excellent']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we can access the word embeddings of a word using the above line of code"
      ],
      "metadata": {
        "id": "xr7jkSIFzh3u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rF8MXEjJylaa",
        "outputId": "42c3b241-3da7-4b5d-8188-2774ba60749a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.848614"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "word_vectors.similarity('great','good')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above code checks the similarity between to words."
      ],
      "metadata": {
        "id": "g7e7P09u1Tz3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWkGjsZr0xYw"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/RateMyProfessor_Sample data.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have loaded the given corpus i.e., \"***RateMyProfessor_Sample data.csv***\" into a pandas data frame."
      ],
      "metadata": {
        "id": "tOsb2Flh1gX5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "q2xD3PNG01Jt",
        "outputId": "a0124005-fa26-4d5f-bc53-080b868bd7c4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       student_star                                           comments  \\\n",
              "0               5.0  This class is hard, but its a two-in-one gen-e...   \n",
              "1               5.0  Definitely going to choose Prof. Looney\\'s cla...   \n",
              "2               4.0  I overall enjoyed this class because the assig...   \n",
              "3               5.0  Yes, it\\'s possible to get an A but you\\'ll de...   \n",
              "4               5.0  Professor Looney has great knowledge in Astron...   \n",
              "...             ...                                                ...   \n",
              "19988           1.5   Great sense of humor!!!! Love parasites now!!!!!   \n",
              "19989           2.5  he is a really nice guy and is really funny..h...   \n",
              "19990           5.0  His parasitology class is a lot of work but he...   \n",
              "19991           4.0  He is WAY too much work for a 1 credit class. ...   \n",
              "19992           5.0  Extremely easy lab teacher, quizzes are a litt...   \n",
              "\n",
              "       student_difficult  \n",
              "0                    3.0  \n",
              "1                    2.0  \n",
              "2                    3.0  \n",
              "3                    3.0  \n",
              "4                    1.0  \n",
              "...                  ...  \n",
              "19988                5.0  \n",
              "19989                4.0  \n",
              "19990                3.0  \n",
              "19991                5.0  \n",
              "19992                2.0  \n",
              "\n",
              "[19993 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-705d47c7-7686-440d-8d98-b4ddc0d0d0eb\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>student_star</th>\n",
              "      <th>comments</th>\n",
              "      <th>student_difficult</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.0</td>\n",
              "      <td>This class is hard, but its a two-in-one gen-e...</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5.0</td>\n",
              "      <td>Definitely going to choose Prof. Looney\\'s cla...</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.0</td>\n",
              "      <td>I overall enjoyed this class because the assig...</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5.0</td>\n",
              "      <td>Yes, it\\'s possible to get an A but you\\'ll de...</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>Professor Looney has great knowledge in Astron...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19988</th>\n",
              "      <td>1.5</td>\n",
              "      <td>Great sense of humor!!!! Love parasites now!!!!!</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19989</th>\n",
              "      <td>2.5</td>\n",
              "      <td>he is a really nice guy and is really funny..h...</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19990</th>\n",
              "      <td>5.0</td>\n",
              "      <td>His parasitology class is a lot of work but he...</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19991</th>\n",
              "      <td>4.0</td>\n",
              "      <td>He is WAY too much work for a 1 credit class. ...</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19992</th>\n",
              "      <td>5.0</td>\n",
              "      <td>Extremely easy lab teacher, quizzes are a litt...</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>19993 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-705d47c7-7686-440d-8d98-b4ddc0d0d0eb')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-705d47c7-7686-440d-8d98-b4ddc0d0d0eb button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-705d47c7-7686-440d-8d98-b4ddc0d0d0eb');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "df.dropna(subset = ['comments'], inplace = True)\n",
        "df.reset_index(drop = True, inplace = True)\n",
        "df = df [['student_star', 'comments','student_difficult']]\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code snippet drops the records with null comments and makes sure the indices are continuous."
      ],
      "metadata": {
        "id": "7aHWkpzj12n8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LkDvtZo905Lx",
        "outputId": "8394f6ae-6c6a-4850-9b95-39dec17e1a8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Import required language models\n",
        "\n",
        "import spacy\n",
        "import nltk\n",
        "\n",
        "# Download 'words' corpus from ntlk (Natural Language Toolkit) library\n",
        "nltk.download('words')\n",
        "\n",
        "# Load the previously downloaded \"en_core_web_lg\" language model as nlp\n",
        "nlp = spacy.load(\"en_core_web_lg\")\n",
        "\n",
        "#create a set of vocabulary in 'words' corpus of nltk as 'words'\n",
        "words = set(nltk.corpus.words.words())\n",
        "\n",
        "# Definig a function that preprocess and vectorize the text data\n",
        "def CleanText_and_Vectorize(text):\n",
        "\n",
        "    # Remove non-english words from the text\n",
        "    text = \" \".join(w for w in nltk.wordpunct_tokenize(text) if w.lower() in words or not w.isalpha())\n",
        "\n",
        "    # Remove stop words and lemmatize the text\n",
        "    doc = nlp(text)\n",
        "\n",
        "    word_embeddings = []\n",
        "    for token in doc:\n",
        "        # check if the token is in the word_vectors(Keyed Vectors format of GloVE). Continue without performing any action if it is not in the word_vectors\n",
        "        if token.lemma_ not in word_vectors.key_to_index:\n",
        "          continue\n",
        "        # check if it is a stop word or a punctuation. Continue without performing any action if it is a stop word or a punctuation\n",
        "        elif token.is_stop or token.is_punct : # it's optional to use token.is_punct here\n",
        "          continue\n",
        "\n",
        "        # convert words into embbeddings\n",
        "        vector = word_vectors[token.lemma_]\n",
        "        word_embeddings.append(vector)\n",
        "\n",
        "    return  word_embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code downloads the \"***words***\" corpus from the ***NLTK library*** and uses it to preprocess and vectorize  text data using the ***spaCy language model*** and pre-trained **GloVe word embeddings**."
      ],
      "metadata": {
        "id": "zyypKz7j8LAk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LepQP6se1bgC",
        "outputId": "e4c4972c-06d1-4379-93ff-865665bdd3b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([ 1.8315e-01,  1.6785e-01,  4.2958e-01, -2.9332e-01,  3.1203e-01,\n",
            "       -4.4731e-02, -3.5613e+00, -1.1850e-01,  2.0356e-02, -8.1756e-01,\n",
            "        1.7654e-02, -1.4323e-01,  6.0164e-01,  9.8865e-02,  1.6221e-01,\n",
            "        6.2312e-02,  1.1385e-01, -1.9449e-01, -1.1285e-01, -8.1337e-02,\n",
            "       -5.0393e-02, -2.4186e-02,  1.9904e-01, -7.6335e-02,  2.1435e-01,\n",
            "       -3.0881e-01, -2.2568e-01,  6.0657e-01,  2.4933e-01,  2.8433e-01,\n",
            "       -2.9857e-02, -9.5095e-02,  1.2549e-01,  9.8321e-02, -2.1169e-01,\n",
            "       -2.8529e-01, -1.2910e-01,  2.4108e-01,  2.5237e-01,  1.6759e-01,\n",
            "        4.9352e-04,  1.2010e-01, -7.3734e-02, -5.4717e-02,  1.6821e-01,\n",
            "       -1.4597e-01, -1.4255e-01, -1.0056e-01,  2.5957e-01,  2.7934e-02,\n",
            "        1.9946e-01, -8.7880e-03,  2.8088e-01, -1.5341e-01, -1.5649e-01,\n",
            "       -2.8636e-01,  8.6480e-03, -4.9797e-01,  4.1838e-01,  1.4312e-01,\n",
            "        1.1074e-01,  9.4336e-02,  1.6607e-01,  2.1501e-01, -1.3163e-01,\n",
            "        1.3850e-02,  8.6084e-02, -2.1840e-01,  1.2086e-02, -4.1902e-01,\n",
            "        6.5361e-02,  7.3115e-02,  1.0629e-01, -2.4928e-01,  1.3665e-01,\n",
            "       -4.5397e-02,  1.4329e-01,  1.9360e-01,  7.8869e-02,  2.3941e-01,\n",
            "       -4.3743e-02, -5.8161e-01, -4.5206e-02,  2.4334e-01, -1.9619e-01,\n",
            "       -1.9919e-01, -1.3074e-01,  1.4862e-01,  1.8633e-01, -1.7493e-01,\n",
            "        5.1520e-02,  1.0212e-02,  8.9539e-03,  3.4671e-01,  4.5161e-02,\n",
            "       -3.9897e-01, -1.2440e+00,  1.6477e-01, -2.0563e-02, -1.3045e-01,\n",
            "        4.8356e-01, -2.0084e-01, -2.8987e-02, -2.1759e-01,  1.5102e-01,\n",
            "        1.5006e-01, -2.5402e-01, -2.2710e-01,  1.5882e-01, -8.4928e-02,\n",
            "       -5.3460e-02, -4.1409e-01, -2.8945e-01,  1.4171e-01, -3.5157e-01,\n",
            "        3.1867e-01, -1.5859e-01, -1.8566e-01,  2.3677e-01, -1.6000e-01,\n",
            "        1.9048e-01,  3.0555e-02,  1.9992e-02,  1.1200e-01,  4.0273e-02,\n",
            "       -5.2106e-02,  2.0504e-01, -1.2829e-01,  1.5228e-01, -1.0646e-01,\n",
            "       -4.9122e-01, -8.0010e-02,  1.7955e-02,  5.9830e-02,  9.7281e-02,\n",
            "       -2.5198e-01,  1.2691e-02, -2.5178e-01,  2.1193e-01,  1.9675e-01,\n",
            "       -1.1520e-01,  2.2369e-02,  1.7457e-01, -1.8690e-01, -3.0697e-01,\n",
            "        6.2504e-02, -2.2753e-01,  1.3621e-01, -8.4883e-02, -4.3334e-02,\n",
            "        2.6670e-01,  9.3971e-02, -3.5808e-01, -4.8310e-02,  2.0994e-01,\n",
            "        2.2135e-01,  2.8927e-02, -5.7231e-02,  2.6705e-02,  1.1909e-02,\n",
            "        3.1577e-01, -2.0687e-01, -3.7710e-02,  1.4254e-01, -9.7776e-02,\n",
            "        2.1517e-01,  3.4920e-01, -2.1281e-01,  6.2627e-02,  1.4196e-01,\n",
            "       -3.3737e-01,  3.2658e-01, -3.5696e-03, -1.2488e-01, -7.0843e-02,\n",
            "       -3.9852e-02,  1.3537e-02, -6.7399e-02, -3.3968e-01, -5.0161e-01,\n",
            "        2.9652e-01,  4.9899e-01, -1.8060e-01, -1.6486e-02, -8.1030e-02,\n",
            "        5.4411e-02, -1.1058e-01, -2.0926e-01, -6.8767e-02, -4.0616e-02,\n",
            "        1.4339e-01, -5.9297e-02, -1.7794e-01, -2.7963e-01,  3.9449e-01,\n",
            "        4.4089e-01,  5.5237e-02, -6.0245e-02,  3.2727e-01,  1.1895e-01,\n",
            "        3.8724e-01, -1.7666e-01, -6.2602e-02, -2.2207e-01, -5.0942e-01,\n",
            "        2.0089e-01,  1.0468e-02, -1.1692e-01,  4.2678e-01,  3.3441e-02,\n",
            "        3.9813e-01, -3.5437e-02,  2.1772e-01, -2.1281e-01,  1.7936e-01,\n",
            "        1.1380e-01,  2.6729e-02, -1.3281e-01, -2.9483e-01, -2.0508e-01,\n",
            "        1.4550e-01, -1.4575e-01, -6.1170e-02, -5.7160e-01, -2.0086e+00,\n",
            "       -4.4346e-01,  3.8805e-01, -2.4213e-01,  2.3531e-01, -5.6752e-01,\n",
            "       -3.4106e-01, -2.8301e-01, -4.2548e-01, -3.8678e-01, -7.6138e-02,\n",
            "       -1.1911e-01, -5.1717e-02,  1.6149e-01, -2.4391e-01,  2.7253e-01,\n",
            "       -2.4649e-03, -1.4426e-01, -5.9956e-01,  3.8906e-01, -2.2834e-01,\n",
            "        1.2744e-01, -3.1377e-01,  4.9522e-01,  1.4115e-01, -1.0874e-01,\n",
            "       -2.1502e-01,  3.1793e-01,  6.2484e-02, -3.6127e-01,  3.2584e-02,\n",
            "       -1.8682e-01,  4.1104e-01,  1.6462e-01, -1.5073e-01,  4.4928e-02,\n",
            "       -1.1153e-01,  2.1358e-01,  3.3519e-01, -1.0318e-01,  7.2934e-02,\n",
            "        8.5325e-02, -1.1882e-01,  1.2358e-01, -4.7334e-03, -1.5931e-02,\n",
            "       -1.2628e-01, -6.8358e-02,  7.7764e-02, -9.5935e-02, -2.2580e-01,\n",
            "       -1.1035e-01,  1.5505e-01, -9.0120e-02, -3.7273e-01,  1.1115e-01,\n",
            "        7.0010e-01, -2.6962e-01,  2.0471e-01,  2.0571e-01,  8.9942e-03,\n",
            "        5.7585e-01,  1.4850e-01,  3.3576e-01, -4.6781e-01, -3.2660e-01,\n",
            "       -9.2478e-02, -8.5151e-02, -3.2997e-01,  1.9846e-01,  3.9161e-01,\n",
            "       -3.8886e-02,  1.6083e-01, -1.6173e-01, -8.7620e-02, -4.5577e-01],\n",
            "      dtype=float32), array([ 9.4418e-02,  2.6803e-01, -1.8872e-01, -3.4682e-01,  1.7336e-01,\n",
            "        5.3370e-01, -3.8678e+00, -2.1750e-01,  3.3643e-01, -6.3875e-01,\n",
            "        2.8250e-01, -2.2189e-01,  3.3060e-01, -4.3387e-01, -2.7718e-01,\n",
            "        1.7048e-01, -1.3535e-01,  4.0161e-01,  1.7076e-01, -3.5158e-02,\n",
            "        1.2924e-01,  2.3708e-01,  7.5131e-02, -1.1332e-01,  1.1799e-01,\n",
            "        8.4099e-02,  6.8827e-02, -1.1418e-01,  2.8357e-01,  1.9997e-01,\n",
            "       -3.4542e-01,  2.7971e-02,  3.8076e-01, -1.0336e-01, -3.6837e-01,\n",
            "        1.2589e-01, -2.5971e-01, -7.7609e-02, -2.1524e-02,  2.4337e-01,\n",
            "        1.0898e-01,  2.6051e-01,  1.1254e-01, -2.9503e-01, -1.5013e-01,\n",
            "       -1.2229e-01,  1.7116e-01,  5.7593e-02,  1.2916e-01, -3.0448e-01,\n",
            "       -1.3809e-01, -2.6064e-01,  2.7423e-01, -9.4414e-02, -1.7649e-01,\n",
            "        2.9260e-01,  2.4155e-01, -3.7817e-01,  1.2518e-02, -1.0953e-01,\n",
            "        1.1117e-01, -6.0748e-01,  1.4360e-01, -2.1671e-01,  1.7096e-01,\n",
            "        9.3611e-02, -1.7715e-01, -1.9421e-01,  5.1221e-02, -2.3039e-01,\n",
            "       -2.3034e-01,  2.3402e-01,  1.6565e-01, -2.2186e-01, -1.0233e-01,\n",
            "       -3.4272e-02,  2.0840e-01, -1.3385e-01, -1.9470e-01, -2.8823e-01,\n",
            "       -1.4610e-01,  2.4716e-02, -7.6389e-02,  3.6808e-01,  2.9266e-01,\n",
            "        1.2493e-01,  5.1800e-01, -1.8280e-01, -2.1462e-01, -2.1068e-01,\n",
            "       -3.0506e-01, -2.7327e-01, -3.2984e-01,  3.7675e-01,  1.2728e-01,\n",
            "       -5.2076e-01, -1.6536e+00, -5.8644e-03,  2.3341e-01, -9.8024e-02,\n",
            "       -3.8863e-01,  2.2057e-01,  5.1245e-01,  1.0673e-01, -4.0742e-01,\n",
            "       -4.4126e-01, -2.8272e-02, -3.3467e-01,  9.3732e-02, -2.6459e-01,\n",
            "        2.3972e-01, -4.6910e-03, -5.1896e-02, -1.5794e-01, -2.1726e-02,\n",
            "        8.0105e-02, -1.1512e-01, -1.4630e-01, -5.7591e-02,  2.4287e-01,\n",
            "        1.8640e-01, -2.4916e-01, -3.5551e-02, -5.5182e-03, -3.0756e-01,\n",
            "       -1.3387e-01,  1.6082e-01,  3.0195e-02,  6.8633e-01, -2.9844e-01,\n",
            "        2.1015e-02,  4.5918e-01,  2.0404e-02, -5.4270e-02,  2.6948e-01,\n",
            "       -3.0895e-01,  3.1672e-01, -1.2130e-01, -5.3150e-02,  9.1316e-01,\n",
            "       -4.2386e-02,  3.2920e-01, -1.7039e-01,  1.2711e-01,  2.6708e-01,\n",
            "       -1.4788e-01,  4.9467e-02,  1.9836e-01,  8.5242e-02,  3.1954e-01,\n",
            "       -8.7412e-02, -2.7581e-01,  2.1797e-01, -2.5983e-01,  4.0781e-01,\n",
            "        1.1174e-01, -4.7836e-01,  4.0652e-02,  3.6448e-01,  1.0573e-01,\n",
            "        4.9726e-02, -3.4456e-01, -4.0149e-01,  3.4655e-01, -1.8311e-01,\n",
            "        1.8504e-01,  6.1529e-02,  1.4284e-01,  1.6496e-01,  2.6701e-02,\n",
            "       -3.0705e-01,  1.5180e-01, -1.6390e-02,  2.1554e-01,  3.0284e-01,\n",
            "       -2.9016e-01, -1.5141e-01, -1.5587e-01,  8.4599e-02,  3.3001e-02,\n",
            "        2.4069e-01,  3.5252e-01,  7.0173e-02,  1.8034e-01, -3.7824e-02,\n",
            "       -1.5563e-01, -1.5525e-01, -6.7491e-02,  3.8310e-01,  1.5184e-01,\n",
            "       -4.2235e-04, -8.4997e-02, -4.0564e-02,  2.3772e-01, -1.2050e-01,\n",
            "       -1.5529e-01,  1.5730e-01,  5.2571e-01, -2.1653e-01, -1.7935e-01,\n",
            "        3.5412e-01, -4.4004e-01, -5.7136e-01, -1.0608e-01, -6.3105e-02,\n",
            "       -1.7406e-01,  1.4419e-02, -6.4050e-01,  4.2912e-01, -2.7190e-01,\n",
            "        4.4754e-01, -4.6831e-01, -2.3365e-01, -2.7647e-02,  3.5820e-01,\n",
            "       -9.4965e-02, -1.0119e-01,  1.9289e-01, -1.5690e-03, -3.5406e-01,\n",
            "        5.6095e-02, -3.7550e-01, -6.1131e-01, -9.4798e-02, -3.2913e+00,\n",
            "        2.6161e-01, -1.4395e-01,  1.2929e-01, -7.1316e-02,  2.1817e-02,\n",
            "        1.7378e-02,  4.2405e-01,  6.3701e-02,  2.5891e-01, -5.1071e-01,\n",
            "        3.9142e-01, -8.4620e-02, -5.5619e-02, -4.0948e-01, -3.0162e-01,\n",
            "       -6.1097e-01,  1.0462e-01, -6.8954e-01,  3.5474e-01, -2.3485e-01,\n",
            "        1.0324e-02,  9.2725e-02, -3.1455e-02,  2.8184e-01,  1.8024e-01,\n",
            "        2.0908e-01, -3.6636e-01,  2.7290e-01,  3.9809e-02,  7.0743e-02,\n",
            "       -2.3840e-01,  2.1604e-01, -1.4528e-01, -1.5731e-01,  1.1612e-01,\n",
            "       -4.5281e-02,  1.3893e-01,  3.1892e-02, -1.5857e-01, -1.3022e-02,\n",
            "        1.6125e-01, -5.0203e-01,  6.5499e-02,  6.9506e-02,  1.9330e-01,\n",
            "       -2.0954e-01, -1.5868e-01, -3.8791e-02, -4.3139e-02,  8.9203e-02,\n",
            "       -2.9773e-02,  2.8683e-01,  1.9455e-01, -5.8685e-02, -2.1241e-02,\n",
            "        4.1166e-01, -1.8149e-01, -3.6402e-01,  4.4552e-01, -1.4136e-02,\n",
            "        1.2705e-01, -2.4363e-01,  1.0021e-01, -3.9856e-01, -6.1053e-02,\n",
            "       -6.9433e-02,  3.2278e-01, -7.2565e-02,  1.1502e-01, -2.8844e-01,\n",
            "        4.0352e-01, -1.3672e-01,  2.4823e-01, -9.2303e-02, -2.0414e-01],\n",
            "      dtype=float32), array([ 1.4951e-01,  7.8636e-02, -1.1666e-01, -4.1344e-01,  1.9657e-01,\n",
            "        7.1695e-01, -3.4390e+00,  2.1966e-01, -1.7485e-01,  2.1877e-02,\n",
            "        1.6370e-02, -4.6068e-01,  4.5733e-01,  2.9272e-01,  1.4108e-01,\n",
            "        2.7651e-01,  1.8338e-01,  1.9567e-01, -3.7251e-01, -6.0289e-01,\n",
            "        1.6556e-01,  3.0980e-01, -2.4475e-01, -3.0287e-03,  6.6409e-01,\n",
            "       -5.7230e-02, -5.5280e-01,  9.4462e-02, -5.3001e-01, -1.1576e-01,\n",
            "        8.4104e-02, -4.7590e-02,  5.7240e-03,  5.4144e-01, -1.2820e-02,\n",
            "        9.2108e-02, -3.1514e-01, -2.9909e-02, -1.5590e-01,  6.8471e-01,\n",
            "        1.1357e-01,  9.9130e-02, -3.4289e-02, -4.2263e-01,  5.9044e-01,\n",
            "       -1.6190e-01, -4.1808e-01,  7.7923e-03,  2.6324e-01, -4.3663e-01,\n",
            "        1.4796e-01,  1.1105e-01, -1.0574e-01,  1.4958e-02,  2.3633e-01,\n",
            "        6.2554e-02, -5.2639e-02,  1.6149e-01, -7.2366e-01,  4.8319e-02,\n",
            "       -3.2742e-01, -3.6063e-01,  2.5347e-01, -2.8177e-01, -2.2060e-01,\n",
            "        1.8415e-01,  7.1104e-04, -1.2059e-01, -6.3017e-02, -5.0052e-02,\n",
            "       -3.0859e-01,  1.2274e-01,  2.4219e-01, -2.4839e-01,  1.4120e-02,\n",
            "        2.9671e-01,  2.5110e-01,  4.8793e-01,  7.0032e-02,  1.8985e-02,\n",
            "        5.4759e-01,  8.0363e-01,  8.6750e-02,  4.3004e-01,  2.1106e-01,\n",
            "        6.8173e-02,  1.4642e-01,  1.2628e-01,  3.7349e-02,  1.7525e-01,\n",
            "       -1.6471e-01,  3.7647e-02, -1.0213e-01,  1.7060e-01, -3.1424e-01,\n",
            "       -6.6324e-01, -2.3556e+00, -2.1239e-01,  2.3099e-01, -1.3520e-01,\n",
            "        9.2495e-02,  2.9072e-02,  4.3054e-01, -4.9770e-01, -4.9529e-02,\n",
            "        1.6489e-02, -5.7227e-01, -2.8795e-01,  2.5776e-02, -1.9053e-01,\n",
            "       -6.9279e-02,  2.9127e-01, -2.0384e-01, -1.3163e-01, -2.2451e-01,\n",
            "        4.2105e-02,  1.4770e-01,  6.7662e-02,  4.5159e-01,  2.1548e-01,\n",
            "        3.9285e-01, -2.0058e-01,  2.7266e-01, -4.5369e-01, -2.8765e-01,\n",
            "        2.2452e-01, -4.4250e-02, -4.4127e-02, -9.1058e-02,  7.0855e-01,\n",
            "       -3.4910e-01,  2.9227e-01, -3.3565e-01,  3.4266e-02,  3.8184e-01,\n",
            "       -2.2540e-01,  5.4519e-02, -2.9981e-01, -3.6325e-01,  5.1390e-01,\n",
            "        1.6616e-01,  6.4142e-01, -9.6178e-02, -3.2936e-01,  2.1223e-02,\n",
            "        1.7658e-01, -8.0073e-02,  6.3715e-02, -3.1506e-01,  6.0472e-02,\n",
            "       -1.8135e-01,  1.8052e-01,  2.6291e-01, -2.8269e-01,  9.4559e-03,\n",
            "        2.3572e-01,  9.8229e-03,  2.4188e-01,  3.2197e-01,  3.9762e-01,\n",
            "       -7.1023e-02, -2.0536e-01, -1.7981e-01,  2.6617e-01, -2.6272e-01,\n",
            "       -7.7818e-02,  2.1819e-01, -2.2364e-02,  1.3057e-01,  7.3226e-01,\n",
            "       -1.6863e-01, -1.5251e-01, -1.3650e-01, -4.3737e-01, -8.6238e-03,\n",
            "        2.5895e-01,  1.1818e-01, -3.3369e-01,  2.3962e-01, -2.8989e-01,\n",
            "       -8.6333e-02,  2.1463e-01, -3.0875e-01, -4.8992e-01, -2.9682e-01,\n",
            "        2.6859e-01,  3.8812e-01,  2.0201e-01,  1.9049e-01,  6.3564e-02,\n",
            "       -1.4002e-01,  2.2589e-01, -7.3805e-01, -6.0278e-02,  4.5440e-01,\n",
            "        1.8752e-01,  3.8548e-01,  5.7793e-01,  4.2120e-01,  2.9500e-02,\n",
            "        6.5819e-01,  3.9201e-02, -1.0344e-01,  1.6525e-01,  1.1681e-01,\n",
            "       -6.5434e-02,  1.2176e-01, -2.9763e-01,  3.0553e-01, -2.1572e-01,\n",
            "        3.1048e-01, -4.3096e-01,  1.2096e-02,  3.7436e-01,  7.2160e-02,\n",
            "       -1.2890e-01, -2.0244e-01,  1.0079e-01,  3.4203e-01, -2.5209e-01,\n",
            "       -8.8739e-02, -3.4439e-01, -5.1461e-01,  1.9358e-01, -2.0287e+00,\n",
            "        1.9071e-01,  2.4856e-01, -1.4970e-01, -8.0329e-02,  2.7924e-01,\n",
            "        4.1210e-01,  4.5754e-01,  8.2509e-02, -5.4972e-02, -2.7955e-01,\n",
            "       -7.9821e-02, -4.7245e-01,  1.1137e-01,  1.6579e-02,  1.0333e-02,\n",
            "       -2.0121e-01, -3.7269e-01, -1.3988e-01, -1.9149e-01, -1.5352e-01,\n",
            "        1.5785e-01, -4.1220e-02,  3.9860e-01,  8.5861e-02,  3.9155e-02,\n",
            "       -7.2666e-02, -1.6797e-01, -8.6014e-02, -1.5165e-01,  6.1667e-02,\n",
            "       -3.2053e-01, -1.7776e-01,  7.0953e-01, -4.9203e-01, -2.1730e-01,\n",
            "       -8.0348e-02,  2.3586e-01, -1.4263e-01, -1.2580e-01, -3.6108e-02,\n",
            "       -3.8015e-02, -1.9431e-01, -1.0119e-03,  4.4754e-01, -1.4382e-02,\n",
            "       -1.7438e-01, -1.2884e-01,  8.1585e-02,  1.6104e-02,  2.5876e-01,\n",
            "        4.0951e-02,  3.0811e-01, -2.7438e-01, -1.3032e-03, -7.6947e-02,\n",
            "       -9.8141e-02,  1.9406e-01, -3.1817e-01, -2.7227e-02, -2.0120e-01,\n",
            "        5.6374e-01,  1.5219e-02,  6.3748e-02, -2.4249e-01,  1.3194e-02,\n",
            "       -2.4985e-01,  8.7793e-03, -3.1470e-01, -3.4599e-01,  3.9304e-02,\n",
            "        6.9105e-02, -3.5026e-01,  9.4254e-02,  5.3798e-01, -2.7314e-01],\n",
            "      dtype=float32), array([ 7.0622e-02,  2.9253e-01,  1.2940e-01,  6.2026e-01,  4.7091e-01,\n",
            "        3.0846e-01, -2.5635e+00,  6.4880e-01,  3.4072e-01, -6.7809e-01,\n",
            "        7.8232e-01,  7.3290e-02, -9.5126e-02, -4.1274e-02,  1.1277e-01,\n",
            "       -2.3610e-01,  3.4036e-01, -5.7892e-01,  4.6717e-01, -2.2651e-01,\n",
            "        2.8185e-01,  4.6541e-02, -3.0462e-01,  3.4265e-01,  4.6340e-01,\n",
            "       -2.4975e-01, -2.6142e-02,  3.9416e-01,  3.8635e-01,  8.1646e-02,\n",
            "       -2.2292e-02, -5.2232e-01,  1.6878e-01, -4.1726e-03,  3.6231e-01,\n",
            "        2.0203e-02,  2.8763e-01, -9.8177e-02, -2.5434e-03, -1.4600e-02,\n",
            "       -3.4208e-01,  8.5456e-01, -4.9932e-01,  3.5827e-01, -7.0541e-01,\n",
            "       -3.6221e-01, -7.0204e-02, -1.2696e-01,  3.8608e-01,  7.2476e-02,\n",
            "       -2.4907e-01, -1.0138e-01, -4.3844e-01,  6.7502e-02, -5.4541e-01,\n",
            "       -2.2068e-01, -2.4521e-01,  4.0172e-01,  2.9536e-01, -1.7653e-02,\n",
            "        5.5466e-01, -2.6298e-01,  1.7232e-01,  2.4077e-01, -2.8821e-01,\n",
            "        3.3673e-03, -5.8969e-01,  2.3368e-01, -3.0071e-01, -3.6956e-03,\n",
            "       -8.2169e-02, -1.6541e-01, -1.1505e-01,  2.4175e-01,  1.6320e-01,\n",
            "       -2.6484e-01,  2.3677e-01, -1.7238e-01,  4.1886e-02,  1.1633e-01,\n",
            "       -3.8570e-01, -7.1493e-01,  1.6211e-01,  1.2038e-01, -5.0502e-03,\n",
            "       -1.0850e-01,  2.1872e-02, -2.5375e-01, -2.4663e-02,  2.1571e-01,\n",
            "       -1.6919e-01,  1.8504e-01,  3.7589e-01,  1.2703e-01,  3.3959e-01,\n",
            "       -6.4707e-02, -2.3517e+00,  4.6068e-01,  3.1788e-01,  9.5382e-01,\n",
            "       -2.8514e-01,  3.8444e-02, -2.2314e-01, -3.9675e-01, -3.2256e-01,\n",
            "       -3.5182e-01, -2.6726e-01,  4.4401e-01, -2.7065e-01, -2.9046e-01,\n",
            "        5.0031e-01,  2.8439e-02, -4.5128e-01, -4.2802e-02,  4.0010e-03,\n",
            "        1.4410e-04,  8.8137e-01,  2.0642e-01,  5.5321e-01, -3.3898e-01,\n",
            "        1.7174e-01,  1.4372e-01, -2.5023e-01, -4.2179e-01, -1.7949e-01,\n",
            "       -2.0283e-01, -4.1455e-02, -3.6878e-01,  1.2857e-01,  5.8718e-02,\n",
            "        1.6510e-01, -1.9807e-01, -4.3494e-02,  1.3804e-01, -2.5133e-01,\n",
            "       -8.5314e-02,  8.8644e-02, -5.9466e-02, -4.2843e-01,  4.2083e-01,\n",
            "        7.4420e-02, -6.5599e-02, -4.6380e-01, -1.3520e-01, -4.6522e-01,\n",
            "       -2.2187e-01,  7.1727e-02,  3.0467e-01,  1.7732e-01,  2.2664e-01,\n",
            "        4.3560e-01, -3.7583e-01, -9.8499e-02,  2.0465e-01, -4.8405e-01,\n",
            "       -1.3988e-01, -2.8577e-01, -7.4274e-02, -1.8569e-01,  1.6272e-01,\n",
            "       -1.5514e-01,  4.7517e-02,  3.6007e-01, -8.0608e-01,  2.0616e-01,\n",
            "       -2.5533e-01, -1.0755e-01,  5.9368e-02,  5.7939e-01,  3.4987e-01,\n",
            "        9.0802e-03, -1.8299e-01,  1.9263e-01,  4.2617e-01, -3.6850e-01,\n",
            "        6.3721e-01, -1.6059e-01, -1.2610e-03,  5.0107e-02, -1.0670e-01,\n",
            "       -6.3054e-01, -8.6435e-02, -6.8465e-02, -4.1642e-01, -3.1389e-01,\n",
            "       -3.5753e-01,  4.0724e-01,  8.2427e-02,  3.1769e-01, -2.7401e-01,\n",
            "       -7.5316e-03, -2.6895e-01,  8.9344e-02,  4.5775e-01,  1.8991e-01,\n",
            "       -3.4894e-01, -1.0637e-01,  2.1446e-01,  2.7107e-01, -5.0847e-01,\n",
            "       -3.7631e-01,  1.5528e-02,  1.0936e-01,  2.1165e-01, -5.7132e-02,\n",
            "       -2.0405e-02,  2.4498e-01,  1.1274e-01,  2.6029e-02,  2.5321e-01,\n",
            "        5.6944e-01, -1.4494e-01, -7.2747e-01,  8.9723e-02, -1.8329e-02,\n",
            "       -3.7087e-01, -2.1208e-01, -1.0799e-01,  9.8670e-02,  3.6965e-01,\n",
            "       -2.9780e-01,  3.9059e-01,  4.0987e-01,  4.3080e-01, -3.4397e+00,\n",
            "       -1.0993e-01, -1.0114e-01, -3.5419e-01,  5.6113e-02, -4.3987e-02,\n",
            "        3.2936e-01, -3.3964e-01, -5.8038e-01, -1.2903e-01, -5.3987e-01,\n",
            "       -1.4469e-01, -2.0820e-03,  2.8003e-01, -2.4483e-01, -4.2316e-01,\n",
            "        3.9949e-02,  2.2865e-01,  2.8694e-01,  5.2373e-01,  3.5883e-01,\n",
            "       -8.3230e-02,  1.1826e-01, -3.9916e-01, -4.2462e-01, -3.0936e-02,\n",
            "       -9.1265e-02, -1.8759e-01, -7.4079e-02,  2.9533e-01, -5.6891e-01,\n",
            "        1.8820e-02, -1.9119e-01,  1.3441e-01, -6.5208e-02,  1.5667e-01,\n",
            "        9.1243e-02,  3.4300e-01,  2.4732e-01, -3.8062e-01,  1.4858e-01,\n",
            "        5.2567e-01,  3.4949e-01, -4.6291e-02,  1.3851e-01, -1.3545e-01,\n",
            "        5.8345e-01, -1.2537e-01, -1.0513e-01,  4.9464e-01, -7.0559e-02,\n",
            "        6.2041e-01, -2.7864e-01, -1.7222e-01,  2.5764e-01, -1.4447e-01,\n",
            "       -4.3128e-01,  1.6052e-01,  3.7363e-01, -4.8629e-01, -5.5217e-02,\n",
            "        1.8992e-01,  4.8053e-01,  7.9485e-02,  7.2392e-02, -1.8461e-01,\n",
            "        1.2397e-01, -2.4111e-01, -5.8242e-01,  1.3303e-02, -1.2167e-01,\n",
            "        7.6911e-02,  3.4110e-01, -2.2279e-01,  5.6662e-01, -4.1205e-01],\n",
            "      dtype=float32), array([ 0.80308  , -0.016776 ,  0.025788 , -0.18749  ,  0.39202  ,\n",
            "       -0.035497 , -2.7588   ,  0.14405  , -0.33699  ,  0.1603   ,\n",
            "        0.094968 , -0.027912 ,  0.064087 , -0.21293  ,  0.12107  ,\n",
            "       -0.79438  , -0.34563  , -0.066337 ,  0.40751  , -0.54473  ,\n",
            "        0.18373  ,  0.28789  , -0.16561  ,  0.34534  , -0.35837  ,\n",
            "       -0.6952   ,  0.30423  , -0.47689  , -0.40859  ,  0.19899  ,\n",
            "       -0.58226  , -0.21235  , -0.44229  , -0.03624  ,  0.11027  ,\n",
            "        0.095109 , -0.24733  ,  0.072429 ,  0.33366  ,  0.49123  ,\n",
            "       -0.79042  , -0.20579  , -0.34934  , -0.42521  ,  0.073978 ,\n",
            "        0.070243 ,  0.27731  , -0.22146  ,  0.070997 , -0.37468  ,\n",
            "       -0.21665  ,  0.37639  ,  0.12705  ,  0.25819  , -0.23819  ,\n",
            "        0.20287  , -0.60274  ,  0.3881   , -0.15537  , -0.096572 ,\n",
            "       -0.23609  , -0.069739 , -0.14771  ,  0.011003 , -0.017943 ,\n",
            "        0.16917  ,  0.33864  , -0.29066  ,  0.68262  , -0.46173  ,\n",
            "       -0.30623  ,  0.22574  ,  0.21039  ,  0.12731  , -0.16469  ,\n",
            "       -0.003741 ,  0.24455  , -0.14912  ,  0.13648  ,  0.18498  ,\n",
            "        0.11519  , -0.54905  , -0.19051  , -0.40312  , -0.060848 ,\n",
            "       -0.36704  ,  0.48956  , -0.48843  , -0.074416 , -0.14942  ,\n",
            "       -0.34544  , -0.51917  , -0.37653  ,  0.60851  , -0.39752  ,\n",
            "        0.53602  , -2.6302   , -0.19774  ,  0.0519   ,  0.23441  ,\n",
            "        0.20086  , -0.31456  , -0.28436  , -0.3325   ,  0.85009  ,\n",
            "       -0.35766  ,  0.15211  ,  0.24975  , -0.3594   , -0.017336 ,\n",
            "       -0.39166  ,  0.12707  , -0.019749 ,  0.248    ,  0.34041  ,\n",
            "        0.25193  , -0.25829  ,  0.039227 ,  0.056587 , -0.2759   ,\n",
            "       -0.49618  , -0.18124  ,  0.22669  , -0.7766   ,  0.17588  ,\n",
            "       -0.14647  ,  0.034616 , -0.069103 , -0.41464  ,  0.28987  ,\n",
            "        0.039955 , -0.24426  ,  0.77097  , -0.012336 , -0.22667  ,\n",
            "       -0.029481 ,  0.585    , -0.33059  ,  0.61639  ,  0.49246  ,\n",
            "       -0.27768  , -0.36937  ,  0.23287  ,  0.59802  , -0.40638  ,\n",
            "       -0.32426  ,  0.14236  ,  0.11978  , -0.17607  , -0.10391  ,\n",
            "       -0.42016  , -0.70583  ,  0.69563  ,  0.015839 ,  0.092355 ,\n",
            "        0.21946  ,  0.43544  , -0.60013  , -0.47939  ,  0.23134  ,\n",
            "        0.0030769,  0.36531  ,  0.13168  , -0.25126  ,  0.30519  ,\n",
            "       -0.17284  ,  0.019647 , -0.22808  , -0.23333  ,  0.37047  ,\n",
            "        0.46287  , -0.21304  , -0.41283  , -0.66185  ,  0.1554   ,\n",
            "       -0.2061   , -0.12471  ,  0.049752 , -0.055903 , -0.18113  ,\n",
            "        0.32597  ,  0.81903  ,  0.059237 , -0.38794  , -0.36903  ,\n",
            "       -0.24322  , -0.45387  ,  0.33121  , -0.56186  ,  0.29811  ,\n",
            "        0.32977  , -0.015022 , -0.10829  , -0.042009 , -0.30949  ,\n",
            "       -0.47719  ,  0.19074  ,  0.59402  ,  0.077896 , -0.17676  ,\n",
            "        0.43221  ,  0.25658  , -0.079821 , -0.17338  , -0.048101 ,\n",
            "       -0.37701  , -0.049928 ,  0.11445  ,  0.33444  ,  0.37309  ,\n",
            "        0.62483  ,  0.31638  , -0.45518  ,  0.01502  , -0.24264  ,\n",
            "        0.14381  , -0.41897  , -0.0096535,  0.18322  , -0.38458  ,\n",
            "        0.027271 , -0.078379 , -0.68953  ,  0.075884 , -2.8057   ,\n",
            "       -0.62481  , -0.59873  , -0.21327  , -0.51074  ,  0.21778  ,\n",
            "       -0.34754  ,  0.056986 , -0.23448  , -0.27812  , -0.51925  ,\n",
            "       -0.51656  ,  0.2154   ,  0.085515 ,  0.39651  ,  0.17648  ,\n",
            "        0.1341   ,  0.042378 ,  0.42732  ,  0.18363  ,  0.21337  ,\n",
            "        0.38357  ,  0.043625 , -0.53441  , -0.091829 , -0.48416  ,\n",
            "       -0.075409 ,  0.024323 ,  0.37517  ,  0.44455  ,  0.077994 ,\n",
            "        0.42273  ,  0.38273  ,  0.26384  ,  0.31202  , -0.41986  ,\n",
            "        0.63329  , -0.20539  , -0.0055835,  0.39349  , -0.56129  ,\n",
            "       -0.27337  ,  0.38117  , -0.28813  ,  0.29831  ,  0.073124 ,\n",
            "       -0.11996  , -0.098919 , -0.45023  ,  0.092908 , -0.11479  ,\n",
            "       -0.17715  , -0.62175  ,  0.37615  , -0.31627  ,  0.28654  ,\n",
            "        0.078164 , -0.20697  ,  0.17502  ,  0.6093   , -0.51688  ,\n",
            "        0.23647  , -0.24028  ,  0.51303  , -0.1996   ,  0.12602  ,\n",
            "        0.75176  ,  0.27919  , -0.11109  , -0.16661  , -0.12302  ,\n",
            "        0.59343  , -0.1643   ,  0.042583 , -0.39625  , -0.040408 ],\n",
            "      dtype=float32), array([ 8.1899e-02, -2.0806e-01, -2.1512e-01, -1.7479e-01,  1.1350e-01,\n",
            "       -9.2490e-02, -3.2101e+00, -1.3796e-01, -9.1090e-02, -3.9092e-01,\n",
            "       -2.5211e-01, -2.4655e-01, -2.9622e-01,  6.9815e-01,  3.7453e-01,\n",
            "        9.8240e-02,  4.4034e-02,  1.0446e-01, -6.3903e-02, -9.3480e-02,\n",
            "       -1.0355e-01,  1.5568e-01,  5.0118e-02, -3.1551e-01, -5.8661e-02,\n",
            "        1.1364e-01,  2.1058e-01,  4.0892e-01,  3.3129e-01,  9.2863e-02,\n",
            "       -6.0779e-02,  3.0965e-02,  1.4581e-01,  1.4852e-01, -3.3230e-01,\n",
            "       -3.8815e-01,  3.5192e-01, -4.6338e-02,  2.1831e-01, -9.7374e-03,\n",
            "       -1.5577e-01, -1.7622e-01, -8.7644e-02, -9.1538e-02, -8.7107e-02,\n",
            "        9.0659e-02, -2.7615e-01, -1.7087e-02,  1.7657e-01, -3.0766e-01,\n",
            "       -2.9640e-01, -2.2112e-01,  2.2499e-01, -2.2901e-01, -7.1854e-03,\n",
            "       -1.2260e-01,  1.7856e-01, -3.5441e-01,  2.0682e-01,  1.6865e-01,\n",
            "        8.5107e-02,  1.0946e-01,  4.1448e-01,  4.8945e-01, -2.1534e-01,\n",
            "       -4.0750e-01, -4.5412e-01, -2.8986e-01,  1.5074e-01, -6.5955e-01,\n",
            "       -1.8809e-01, -1.7483e-01, -3.6416e-01, -1.6762e-01,  4.5382e-01,\n",
            "       -7.7418e-01, -1.6073e-01, -9.4649e-03,  2.0115e-01,  1.5752e-01,\n",
            "        5.5283e-03, -1.2609e+00,  9.6335e-02, -2.0653e-01, -1.4179e-01,\n",
            "       -2.2689e-01,  1.3587e-01, -1.1206e-01,  2.8284e-01, -1.2643e-01,\n",
            "       -3.5806e-02,  1.3033e-01,  1.5570e-02,  3.0194e-01, -2.1498e-01,\n",
            "       -1.7386e-01, -2.2892e+00, -1.7610e-01, -1.2790e-01, -1.2442e-01,\n",
            "        1.4819e-01,  1.9555e-02, -2.0087e-01,  3.4712e-02,  7.9838e-02,\n",
            "        2.7525e-01,  2.2992e-01, -6.2195e-01,  8.4305e-02, -7.8692e-02,\n",
            "       -1.6951e-01, -4.1558e-01, -2.6736e-01, -5.3352e-02, -4.7278e-01,\n",
            "        5.3541e-01,  6.4072e-01, -4.9730e-02,  5.1408e-02, -2.8789e-01,\n",
            "        4.8711e-01, -1.3363e-01,  5.3156e-01, -8.5880e-02,  3.7058e-01,\n",
            "       -4.0748e-01, -2.5247e-01,  1.7955e-02, -3.1787e-01,  3.2729e-01,\n",
            "       -3.6921e-01,  3.6353e-02,  4.3310e-02, -2.7397e-02, -3.8876e-01,\n",
            "       -3.6026e-01,  2.2255e-01, -2.5100e-01,  2.6605e-01,  9.0713e-01,\n",
            "        8.8593e-02,  1.8960e-01,  2.8796e-01, -3.9464e-02, -2.8351e-01,\n",
            "       -1.8359e-01, -2.7948e-02,  1.2980e-02,  4.0080e-01,  7.9409e-02,\n",
            "        1.2435e-01, -1.5899e-01, -1.7841e-01,  1.7146e-01,  5.7972e-02,\n",
            "        4.4871e-01,  3.0246e-02,  9.8721e-02, -1.9961e-01,  2.8861e-01,\n",
            "        1.1973e-01, -9.8482e-04,  4.7076e-02,  1.1869e-02, -3.2628e-02,\n",
            "        4.2360e-01, -5.4438e-01, -2.8379e-01,  6.5350e-02,  1.9730e-01,\n",
            "       -1.4814e-01,  3.8073e-01,  3.0144e-02, -3.1943e-01,  3.5871e-02,\n",
            "        1.3323e-01,  3.3703e-02, -2.1669e-01, -4.4835e-01, -6.8231e-01,\n",
            "        4.7769e-01,  2.4510e-01, -2.1489e-01, -1.0122e-01, -2.3954e-01,\n",
            "        3.5209e-01,  1.2189e-01,  7.0094e-03,  2.9520e-01, -1.6126e-01,\n",
            "        3.1204e-02, -8.0108e-02, -8.7409e-02,  1.9611e-02, -1.1860e-01,\n",
            "        5.4191e-01, -4.4480e-01, -3.3926e-01,  5.0356e-02,  2.8052e-01,\n",
            "        1.6539e-01,  4.2863e-02, -2.1935e-01,  3.6973e-01, -5.3238e-01,\n",
            "        3.4064e-01, -2.3852e-01, -6.9075e-02,  8.2358e-01,  2.4711e-01,\n",
            "        6.1094e-02, -1.7287e-01,  7.6747e-03, -1.2217e-01, -4.7482e-01,\n",
            "        3.8848e-02,  3.0537e-02,  2.5422e-01, -4.7333e-01,  1.8020e-01,\n",
            "       -1.3019e-01, -4.0056e-01,  6.4591e-02, -5.3335e-02, -1.7874e+00,\n",
            "       -3.8686e-01,  3.2775e-01, -1.6935e-01,  1.7930e-01, -2.1928e-01,\n",
            "       -1.5697e-01, -2.0420e-01,  8.2211e-02,  3.5055e-01,  3.2940e-01,\n",
            "       -1.8319e-01,  1.3974e-02,  1.4055e-01, -5.3206e-01,  6.6116e-02,\n",
            "        4.6162e-01,  9.5295e-02, -1.6997e-01, -1.2733e-01, -2.5983e-01,\n",
            "        2.3071e-01, -2.5165e-01,  3.2169e-01,  2.3378e-01,  3.3606e-01,\n",
            "        2.3354e-01,  3.7849e-01,  8.9217e-02,  1.2024e-02, -2.0785e-02,\n",
            "       -1.9766e-01, -1.4773e-02, -2.9877e-01,  2.7486e-01, -1.2581e-01,\n",
            "       -3.3772e-01,  1.0271e-01,  2.3313e-01, -7.2297e-02,  7.9153e-02,\n",
            "        1.3980e-01, -3.8080e-01,  6.1530e-01, -6.7532e-02,  1.6099e-01,\n",
            "       -4.3343e-02,  3.2747e-01,  1.8707e-01, -2.6951e-01, -3.9081e-01,\n",
            "       -1.7177e-01,  1.3292e-01, -5.3208e-02, -8.4944e-01,  4.7666e-01,\n",
            "        4.3214e-01,  1.6056e-01, -1.1914e-01,  3.3960e-02,  4.8377e-01,\n",
            "        2.9020e-01, -2.7836e-01, -1.7103e-01, -1.3864e-01, -3.0772e-01,\n",
            "       -1.1554e-01,  3.2001e-01,  9.6894e-02,  7.6523e-02,  4.3925e-01,\n",
            "        1.2051e-01,  1.6808e-01,  1.9462e-01, -1.1255e-01, -1.4470e-01],\n",
            "      dtype=float32), array([ 0.80308  , -0.016776 ,  0.025788 , -0.18749  ,  0.39202  ,\n",
            "       -0.035497 , -2.7588   ,  0.14405  , -0.33699  ,  0.1603   ,\n",
            "        0.094968 , -0.027912 ,  0.064087 , -0.21293  ,  0.12107  ,\n",
            "       -0.79438  , -0.34563  , -0.066337 ,  0.40751  , -0.54473  ,\n",
            "        0.18373  ,  0.28789  , -0.16561  ,  0.34534  , -0.35837  ,\n",
            "       -0.6952   ,  0.30423  , -0.47689  , -0.40859  ,  0.19899  ,\n",
            "       -0.58226  , -0.21235  , -0.44229  , -0.03624  ,  0.11027  ,\n",
            "        0.095109 , -0.24733  ,  0.072429 ,  0.33366  ,  0.49123  ,\n",
            "       -0.79042  , -0.20579  , -0.34934  , -0.42521  ,  0.073978 ,\n",
            "        0.070243 ,  0.27731  , -0.22146  ,  0.070997 , -0.37468  ,\n",
            "       -0.21665  ,  0.37639  ,  0.12705  ,  0.25819  , -0.23819  ,\n",
            "        0.20287  , -0.60274  ,  0.3881   , -0.15537  , -0.096572 ,\n",
            "       -0.23609  , -0.069739 , -0.14771  ,  0.011003 , -0.017943 ,\n",
            "        0.16917  ,  0.33864  , -0.29066  ,  0.68262  , -0.46173  ,\n",
            "       -0.30623  ,  0.22574  ,  0.21039  ,  0.12731  , -0.16469  ,\n",
            "       -0.003741 ,  0.24455  , -0.14912  ,  0.13648  ,  0.18498  ,\n",
            "        0.11519  , -0.54905  , -0.19051  , -0.40312  , -0.060848 ,\n",
            "       -0.36704  ,  0.48956  , -0.48843  , -0.074416 , -0.14942  ,\n",
            "       -0.34544  , -0.51917  , -0.37653  ,  0.60851  , -0.39752  ,\n",
            "        0.53602  , -2.6302   , -0.19774  ,  0.0519   ,  0.23441  ,\n",
            "        0.20086  , -0.31456  , -0.28436  , -0.3325   ,  0.85009  ,\n",
            "       -0.35766  ,  0.15211  ,  0.24975  , -0.3594   , -0.017336 ,\n",
            "       -0.39166  ,  0.12707  , -0.019749 ,  0.248    ,  0.34041  ,\n",
            "        0.25193  , -0.25829  ,  0.039227 ,  0.056587 , -0.2759   ,\n",
            "       -0.49618  , -0.18124  ,  0.22669  , -0.7766   ,  0.17588  ,\n",
            "       -0.14647  ,  0.034616 , -0.069103 , -0.41464  ,  0.28987  ,\n",
            "        0.039955 , -0.24426  ,  0.77097  , -0.012336 , -0.22667  ,\n",
            "       -0.029481 ,  0.585    , -0.33059  ,  0.61639  ,  0.49246  ,\n",
            "       -0.27768  , -0.36937  ,  0.23287  ,  0.59802  , -0.40638  ,\n",
            "       -0.32426  ,  0.14236  ,  0.11978  , -0.17607  , -0.10391  ,\n",
            "       -0.42016  , -0.70583  ,  0.69563  ,  0.015839 ,  0.092355 ,\n",
            "        0.21946  ,  0.43544  , -0.60013  , -0.47939  ,  0.23134  ,\n",
            "        0.0030769,  0.36531  ,  0.13168  , -0.25126  ,  0.30519  ,\n",
            "       -0.17284  ,  0.019647 , -0.22808  , -0.23333  ,  0.37047  ,\n",
            "        0.46287  , -0.21304  , -0.41283  , -0.66185  ,  0.1554   ,\n",
            "       -0.2061   , -0.12471  ,  0.049752 , -0.055903 , -0.18113  ,\n",
            "        0.32597  ,  0.81903  ,  0.059237 , -0.38794  , -0.36903  ,\n",
            "       -0.24322  , -0.45387  ,  0.33121  , -0.56186  ,  0.29811  ,\n",
            "        0.32977  , -0.015022 , -0.10829  , -0.042009 , -0.30949  ,\n",
            "       -0.47719  ,  0.19074  ,  0.59402  ,  0.077896 , -0.17676  ,\n",
            "        0.43221  ,  0.25658  , -0.079821 , -0.17338  , -0.048101 ,\n",
            "       -0.37701  , -0.049928 ,  0.11445  ,  0.33444  ,  0.37309  ,\n",
            "        0.62483  ,  0.31638  , -0.45518  ,  0.01502  , -0.24264  ,\n",
            "        0.14381  , -0.41897  , -0.0096535,  0.18322  , -0.38458  ,\n",
            "        0.027271 , -0.078379 , -0.68953  ,  0.075884 , -2.8057   ,\n",
            "       -0.62481  , -0.59873  , -0.21327  , -0.51074  ,  0.21778  ,\n",
            "       -0.34754  ,  0.056986 , -0.23448  , -0.27812  , -0.51925  ,\n",
            "       -0.51656  ,  0.2154   ,  0.085515 ,  0.39651  ,  0.17648  ,\n",
            "        0.1341   ,  0.042378 ,  0.42732  ,  0.18363  ,  0.21337  ,\n",
            "        0.38357  ,  0.043625 , -0.53441  , -0.091829 , -0.48416  ,\n",
            "       -0.075409 ,  0.024323 ,  0.37517  ,  0.44455  ,  0.077994 ,\n",
            "        0.42273  ,  0.38273  ,  0.26384  ,  0.31202  , -0.41986  ,\n",
            "        0.63329  , -0.20539  , -0.0055835,  0.39349  , -0.56129  ,\n",
            "       -0.27337  ,  0.38117  , -0.28813  ,  0.29831  ,  0.073124 ,\n",
            "       -0.11996  , -0.098919 , -0.45023  ,  0.092908 , -0.11479  ,\n",
            "       -0.17715  , -0.62175  ,  0.37615  , -0.31627  ,  0.28654  ,\n",
            "        0.078164 , -0.20697  ,  0.17502  ,  0.6093   , -0.51688  ,\n",
            "        0.23647  , -0.24028  ,  0.51303  , -0.1996   ,  0.12602  ,\n",
            "        0.75176  ,  0.27919  , -0.11109  , -0.16661  , -0.12302  ,\n",
            "        0.59343  , -0.1643   ,  0.042583 , -0.39625  , -0.040408 ],\n",
            "      dtype=float32), array([-4.9216e-02, -8.8784e-04, -9.1246e-02, -3.5250e-01,  2.6935e-01,\n",
            "       -3.5665e-02, -3.7134e+00, -1.8508e-02, -2.7385e-01, -6.7364e-01,\n",
            "        1.6668e-01, -3.2477e-01, -4.3200e-01,  3.8129e-01, -2.9966e-02,\n",
            "       -2.6970e-01, -6.0516e-01, -2.2232e-01, -2.4000e-01, -3.5481e-01,\n",
            "       -4.6826e-01, -6.2231e-02, -1.1825e-01,  7.6029e-02,  3.6483e-01,\n",
            "        6.8783e-02,  3.8632e-01,  3.6808e-01, -2.3100e-01, -3.4522e-01,\n",
            "       -1.9198e-01, -3.6593e-01,  5.8254e-01, -2.9469e-03,  1.5689e-01,\n",
            "       -4.9505e-01, -2.1635e-01, -7.6145e-03,  7.6668e-02,  6.0156e-01,\n",
            "       -1.3372e-01, -5.2457e-01, -2.8969e-02, -4.1081e-02, -1.2196e-01,\n",
            "       -6.9109e-02,  7.6330e-02, -4.7858e-01, -1.4029e-01, -5.4750e-01,\n",
            "        1.8600e-01,  2.8001e-01,  1.0357e-01,  2.2056e-01, -1.5263e-03,\n",
            "        1.5200e-01, -7.2806e-01, -2.3065e-01, -3.8048e-02, -4.5823e-01,\n",
            "        3.2328e-01,  1.0919e-02,  4.9321e-01, -2.5433e-01,  1.3628e-01,\n",
            "       -3.6452e-01,  3.0959e-01,  3.9574e-01,  4.6293e-01, -3.9424e-01,\n",
            "        6.4094e-01,  2.5150e-01, -3.2011e-01, -2.3110e-01, -4.4776e-01,\n",
            "        1.4644e-01, -1.5063e-01,  5.2548e-01,  1.0441e-01, -2.5669e-01,\n",
            "        2.3842e-01,  4.8515e-01,  1.8512e-01,  4.9702e-01,  6.8673e-02,\n",
            "       -5.4341e-02,  5.7610e-02,  3.3480e-01, -5.1341e-02,  3.8573e-01,\n",
            "        3.4263e-01,  1.6459e-01, -6.2059e-02,  3.4001e-01,  2.1709e-01,\n",
            "       -7.1235e-01, -2.4494e+00,  2.6471e-01,  2.7099e-01, -1.4114e-01,\n",
            "        3.9113e-02, -2.8197e-01,  7.7948e-02, -9.5951e-02, -2.5476e-01,\n",
            "       -3.2652e-01, -3.2372e-02,  2.8774e-01, -1.2980e-01, -2.5016e-01,\n",
            "       -2.3785e-01,  1.5247e-01,  3.1348e-02, -6.6142e-02, -6.1225e-01,\n",
            "       -5.5801e-01,  3.2244e-01, -1.1872e-01,  3.6287e-01,  2.7557e-01,\n",
            "        5.6727e-01, -1.0213e-01, -1.0770e-01,  4.5861e-01, -2.0195e-01,\n",
            "       -3.8461e-01, -5.5121e-02,  1.3380e-01, -1.5904e-02,  4.4850e-02,\n",
            "        7.5332e-02,  3.8675e-01,  3.3143e-02, -1.5846e-01, -1.9805e-01,\n",
            "        1.4945e-01,  2.9326e-01,  1.6406e-01,  7.7583e-02,  5.4755e-01,\n",
            "        1.4985e-01,  2.1102e-02,  2.2906e-01, -3.4878e-01, -3.0965e-01,\n",
            "       -5.4305e-02, -1.5220e-01, -3.1465e-01,  5.1563e-01, -1.1459e-01,\n",
            "        6.4911e-02, -1.8691e-01,  7.8363e-02, -5.9629e-02,  4.3337e-01,\n",
            "       -1.1941e-01, -5.7149e-03,  4.1161e-01,  2.3359e-01,  9.6502e-02,\n",
            "        4.0341e-01,  9.8409e-02, -3.4144e-01,  1.0939e-01, -6.0028e-02,\n",
            "       -1.5215e-02, -5.9736e-01,  6.6100e-02,  7.6784e-01,  4.1037e-01,\n",
            "       -2.2467e-01,  1.4044e-01, -7.3609e-01, -2.7111e-01,  4.6525e-02,\n",
            "        1.9473e-01, -8.3990e-02, -1.8881e-01, -2.1230e-01, -2.9872e-01,\n",
            "        2.2446e-01,  7.2022e-02, -3.2329e-01,  8.6410e-02,  2.1232e-01,\n",
            "        4.5086e-01,  6.4901e-02, -3.8011e-01,  4.6596e-02,  2.7409e-01,\n",
            "        1.6071e-02,  3.0301e-01, -3.1343e-01, -2.0421e-01, -2.7601e-01,\n",
            "        2.5270e-01,  3.4984e-01, -5.3346e-01, -6.4571e-02,  9.1139e-02,\n",
            "        5.5827e-01, -3.8231e-01, -1.9145e-03,  2.2047e-01, -2.0823e-01,\n",
            "        3.4250e-01,  6.3099e-01, -7.0864e-01,  1.5156e-01, -1.9309e-01,\n",
            "       -3.2460e-01, -6.4057e-02,  2.7138e-01,  3.3328e-01,  3.7961e-01,\n",
            "       -1.3542e-01,  2.1928e-01,  1.5653e-01, -2.4328e-01, -8.8930e-01,\n",
            "       -7.7562e-02,  6.1465e-02, -7.4553e-02, -8.6391e-01, -2.2895e+00,\n",
            "        6.8781e-02,  2.2677e-01, -2.0791e-02,  3.1629e-02,  1.5637e-01,\n",
            "        2.9822e-01,  3.3771e-01, -4.6516e-01, -2.8640e-01,  8.3932e-02,\n",
            "       -1.5228e-02,  3.2183e-01, -1.4718e-01,  2.1640e-01, -4.2558e-02,\n",
            "       -2.6958e-01, -1.2898e-01, -5.5204e-01,  1.6047e-01,  1.5378e-02,\n",
            "        4.9342e-01, -1.4739e-02, -1.8918e-01, -2.5358e-02, -9.1946e-02,\n",
            "        2.5513e-01,  6.8425e-03, -3.0662e-01, -1.0641e-01,  3.9308e-01,\n",
            "       -7.9461e-02,  3.3331e-01, -1.9455e-01,  2.4329e-01, -2.0720e-01,\n",
            "        1.2856e-01,  8.7965e-02,  2.0282e-01, -1.3032e-01,  2.6776e-01,\n",
            "        7.1697e-02,  2.1021e-01,  4.0831e-01, -5.2731e-02,  9.8999e-02,\n",
            "        7.5814e-02,  4.7941e-01, -2.0745e-02, -7.0884e-02,  1.8420e-01,\n",
            "        1.2343e-02,  2.2434e-01,  2.5597e-01,  1.9781e-02,  2.7144e-01,\n",
            "        5.2582e-02, -1.5523e-01,  2.0710e-01,  5.0602e-01,  5.2056e-01,\n",
            "        4.8987e-01, -3.0767e-01, -1.5141e-01,  5.6418e-02, -5.8706e-01,\n",
            "        1.1062e-01, -1.3578e-01, -2.6596e-01, -1.6059e-02, -5.6247e-02,\n",
            "        1.0734e-01,  1.5693e-02, -3.1520e-01, -1.0711e-01, -3.5229e-01],\n",
            "      dtype=float32), array([-1.0380e-01,  6.8736e-02,  3.9488e-01, -2.1609e-02,  6.7492e-02,\n",
            "        1.4869e-01, -3.6835e+00, -2.2127e-01,  1.0802e-01, -6.2257e-01,\n",
            "       -1.6363e-01, -1.6668e-01,  2.6105e-01, -1.4259e-01, -1.7816e-02,\n",
            "        3.6717e-02,  1.5730e-01, -8.6320e-02,  2.8491e-01,  1.4517e-01,\n",
            "        3.9408e-02, -3.1427e-01, -2.1542e-01,  6.6559e-02, -5.1795e-03,\n",
            "        2.4162e-01,  2.7933e-01,  4.2486e-01,  2.2282e-01,  2.0424e-01,\n",
            "       -1.3423e-01, -4.0510e-02, -8.6286e-02, -2.0665e-01,  1.2718e-02,\n",
            "       -2.6422e-01, -2.0662e-01, -1.9674e-01,  2.2912e-01, -3.6877e-01,\n",
            "        3.0535e-01,  3.3352e-01, -7.3770e-02, -3.1760e-01, -4.7490e-02,\n",
            "        1.5421e-01, -1.7840e-01, -3.5514e-01,  5.9374e-02,  9.8761e-02,\n",
            "       -1.3204e-01, -3.6885e-02,  6.7090e-02, -3.3792e-01,  3.0171e-01,\n",
            "       -1.7249e-01, -1.0823e-01, -3.8584e-01,  1.2666e-01,  3.0507e-01,\n",
            "        7.8853e-02, -2.1501e-01,  5.2517e-02, -3.3451e-02, -1.4889e-02,\n",
            "        2.8580e-01,  2.3806e-01, -2.3268e-01,  2.3075e-01,  3.4963e-02,\n",
            "       -2.6379e-01, -2.4976e-02,  8.2255e-02, -3.2844e-02, -1.1517e-01,\n",
            "        2.2087e-01,  2.0240e-01, -1.8012e-01, -2.0900e-02,  1.4774e-02,\n",
            "        7.3545e-02,  3.4139e-01, -2.0424e-01,  1.0732e-01, -2.4167e-01,\n",
            "       -1.7269e-01,  4.7061e-02,  6.2533e-02, -5.2623e-02, -1.4158e-01,\n",
            "        1.1072e-01,  3.0240e-02,  8.0802e-02,  4.7182e-01, -2.4759e-01,\n",
            "       -3.5882e-01, -2.4383e+00, -6.2312e-01,  3.6663e-01, -1.4379e-01,\n",
            "       -3.5499e-01,  8.8026e-02,  5.7156e-01,  2.5114e-02,  2.4783e-01,\n",
            "        3.2020e-01,  5.2352e-02, -2.7993e-01,  8.6542e-02, -1.3940e-01,\n",
            "        1.8641e-01,  1.9005e-01, -5.8151e-02,  2.0342e-01,  4.8107e-02,\n",
            "       -2.3135e-01, -4.0487e-01,  3.0739e-02, -2.1450e-02,  8.2027e-02,\n",
            "       -6.9901e-02,  7.6970e-02,  2.1599e-01, -9.2764e-02, -1.6665e-01,\n",
            "       -1.2935e-02,  6.8171e-02,  1.2302e-01, -7.1136e-02,  9.6363e-02,\n",
            "       -2.8242e-01, -1.5719e-01, -1.8881e-01, -1.0088e-01,  4.1372e-01,\n",
            "       -4.0282e-01,  1.7636e-01, -1.4772e-01,  2.6685e-01,  4.5526e-01,\n",
            "        1.2773e-01,  9.8185e-02,  1.6700e-01,  1.2635e-01,  1.3925e-01,\n",
            "        4.2014e-02,  1.0894e-01,  1.2892e-01,  2.3152e-01,  2.7402e-01,\n",
            "        1.0479e-02,  1.4819e-01, -2.0729e-02, -1.1686e-01,  2.2843e-01,\n",
            "        4.0607e-02, -2.2796e-02, -3.3888e-02,  1.5298e-01,  4.8431e-02,\n",
            "       -1.5234e-01, -2.9219e-01, -1.8204e-01, -4.4569e-02, -1.6724e-01,\n",
            "        3.9691e-02, -2.4442e-01,  3.0699e-01,  4.3966e-02, -1.8575e-01,\n",
            "       -4.7632e-01,  4.2221e-02,  1.1714e-02, -2.0891e-01,  2.6618e-01,\n",
            "        5.5185e-02, -1.1019e-01,  2.6909e-01, -1.6258e-01,  2.6856e-01,\n",
            "        4.1979e-01,  2.2139e-01,  1.3673e-01, -3.2463e-01,  2.5719e-01,\n",
            "       -1.4026e-01, -3.3687e-01, -1.2950e-01,  2.5900e-01, -7.3285e-02,\n",
            "       -1.2193e-01, -2.5095e-01,  2.1677e-02,  1.1043e-01,  1.3338e-01,\n",
            "       -4.2089e-01, -1.5892e-01,  4.2478e-01,  8.7679e-02,  1.9025e-01,\n",
            "        1.1883e-01,  4.8040e-03, -3.4603e-01, -1.6628e-01, -1.1899e-01,\n",
            "        1.0610e-01, -2.4359e-01, -7.5183e-02, -5.5698e-02, -2.5372e-01,\n",
            "       -1.8252e-02,  1.3398e-01,  1.1840e-03, -8.3477e-02, -1.5496e-01,\n",
            "       -2.2963e-01,  8.3340e-02, -7.1502e-02,  1.4503e-01, -7.7231e-02,\n",
            "        1.9288e-01, -1.5633e-01,  8.6805e-02,  2.3449e-01, -1.9652e+00,\n",
            "        3.9048e-02, -1.6694e-02,  2.9253e-01,  5.6783e-02, -9.4453e-02,\n",
            "        1.6321e-01,  7.4245e-02, -3.7879e-01,  4.4343e-01, -1.2598e-01,\n",
            "        2.3142e-01, -1.2318e-01, -3.3121e-02, -1.6057e-01,  2.9126e-01,\n",
            "       -1.3774e-02, -4.2529e-02, -4.2996e-01,  2.9623e-01, -1.8696e-01,\n",
            "       -5.0252e-02,  1.9190e-03,  8.4215e-02,  2.6747e-02, -1.7941e-01,\n",
            "       -2.1066e-01, -2.2158e-01, -3.7885e-02,  8.7229e-03, -1.2470e-01,\n",
            "        1.4124e-01,  8.1329e-02,  1.5530e-01, -1.0776e-01,  1.8882e-01,\n",
            "        2.3084e-01,  8.5835e-02, -8.0361e-02,  7.3583e-02, -1.7627e-02,\n",
            "        4.3977e-01, -6.2890e-02,  1.5305e-01,  1.0705e-01,  4.8906e-02,\n",
            "       -1.2526e-01, -2.9326e-01, -3.6662e-01, -2.8983e-01,  1.6257e-01,\n",
            "       -9.5158e-02, -8.5256e-03, -5.8266e-01, -4.9950e-01, -7.1073e-02,\n",
            "        1.6365e-01,  4.5000e-02, -1.0756e-01,  2.4650e-01,  7.3145e-02,\n",
            "        2.0871e-01,  7.4518e-02, -9.4963e-02, -2.2539e-01,  2.2172e-02,\n",
            "       -4.0915e-01, -1.6804e-02, -2.0030e-01,  9.5438e-02,  9.7041e-02,\n",
            "       -2.0626e-01, -1.8085e-01, -6.9549e-02, -1.1010e-02,  1.1024e-01],\n",
            "      dtype=float32), array([-1.4511e-01,  2.7673e-03,  2.3434e-01,  2.7536e-01, -1.5853e-01,\n",
            "        2.6285e-01, -1.8685e+00, -5.4698e-01,  1.5990e-01, -5.0738e-01,\n",
            "        8.4077e-01,  3.9153e-01,  6.1944e-01, -5.8580e-02,  1.1379e-01,\n",
            "       -7.6829e-01,  2.6840e-01, -3.5662e-01,  3.1747e-01,  1.9726e-02,\n",
            "        5.6081e-01,  7.4090e-02, -1.0363e-01,  2.5531e-01, -2.0682e-01,\n",
            "        3.7181e-01, -2.1735e-01, -5.6321e-02, -1.6707e-01,  1.7482e-01,\n",
            "        2.7086e-01, -6.4336e-01, -1.3513e-01,  5.8365e-02, -2.0869e-03,\n",
            "       -2.4012e-01,  2.8000e-01,  1.4660e-03, -1.0441e-01,  3.8414e-01,\n",
            "       -6.8402e-02,  2.2398e-01, -3.7723e-02,  3.3241e-01,  1.6504e-01,\n",
            "        1.0352e-01, -3.5184e-02,  5.2533e-01,  9.1847e-03,  4.6554e-01,\n",
            "        1.6700e-01, -4.2188e-01, -2.6247e-01, -4.1878e-01,  2.0578e-01,\n",
            "       -5.7470e-02,  1.6731e-01, -1.3383e-01, -3.3206e-01,  5.2725e-01,\n",
            "       -4.4911e-01,  3.2238e-01,  5.8645e-01, -7.7083e-01, -2.1178e-01,\n",
            "        6.0814e-02,  1.9847e-01, -8.0475e-02,  2.6855e-01, -4.6212e-01,\n",
            "        4.6485e-02,  1.1271e-01,  3.7282e-01, -1.4557e-01,  1.3734e-01,\n",
            "        4.3638e-01,  4.3646e-01, -3.5174e-02, -6.0883e-01, -1.4489e-01,\n",
            "       -4.3862e-01, -5.2689e-01, -4.0550e-01,  1.8149e-01, -1.5427e-01,\n",
            "       -7.2318e-02,  2.0824e-02, -6.4083e-01,  1.7030e-01,  2.6377e-01,\n",
            "        2.9101e-01,  5.7934e-01, -3.1055e-01,  1.6166e-01, -2.4741e-01,\n",
            "       -1.8811e-01, -1.0855e+00,  1.4708e-01,  6.4991e-02,  3.4342e-01,\n",
            "       -2.2646e-01, -2.5017e-01, -8.7090e-01, -2.4011e-01,  6.5133e-01,\n",
            "        1.7395e-01, -2.8814e-02,  2.8618e-01,  2.6003e-01, -1.5928e-01,\n",
            "        3.8617e-01, -6.5853e-01,  3.5196e-02, -1.8878e-01, -2.4049e-01,\n",
            "       -1.4538e-01,  1.5650e-01, -1.3330e-01,  3.6973e-01,  5.9497e-01,\n",
            "       -9.3537e-02, -2.8367e-01, -1.8781e-01, -3.4647e-02, -3.9437e-01,\n",
            "       -4.5925e-01, -1.6106e-01, -3.1639e-01,  4.6573e-01,  3.9295e-01,\n",
            "       -9.4397e-02, -1.9980e-01, -3.7846e-01,  3.5352e-01,  2.8196e-01,\n",
            "        1.3607e-01,  5.0416e-01,  1.3112e-01, -5.8464e-01,  7.0209e-01,\n",
            "       -2.7641e-01,  1.8228e-01,  3.4995e-01,  2.6689e-01, -2.0727e-01,\n",
            "       -6.2028e-01, -1.9304e-01, -1.2292e-01,  1.7727e-01,  1.4283e-01,\n",
            "        1.1720e-01,  1.7673e-01, -1.1920e-01,  2.4465e-01, -1.7410e-01,\n",
            "        6.3558e-01, -5.6152e-02, -3.3509e-01,  2.2964e-01,  3.3065e-01,\n",
            "        3.6718e-01, -2.6476e-01, -3.8643e-01, -3.2491e-01, -2.0061e-01,\n",
            "        8.0925e-03, -2.6666e-02, -7.3267e-02, -1.7369e-01,  4.0293e-01,\n",
            "        1.9100e-01, -2.6880e-01, -8.8661e-02, -1.4032e-01, -2.0534e-01,\n",
            "       -1.0575e-01, -1.2775e-01, -3.6447e-01,  4.4232e-01, -3.1740e-02,\n",
            "        3.9387e-01, -1.7759e-01,  3.5001e-01,  1.9468e-01,  2.1841e-01,\n",
            "       -1.9565e-02,  2.6436e-01, -4.0500e-02, -1.5777e-01, -3.2486e-01,\n",
            "       -3.9934e-01, -9.5695e-02,  2.6247e-01,  2.8712e-01, -3.5550e-01,\n",
            "       -6.9411e-01, -1.7524e-01, -1.3817e-01, -1.1531e-01, -9.2466e-02,\n",
            "        9.9401e-02, -5.7037e-01, -6.6234e-01, -3.9776e-02, -1.4515e-01,\n",
            "        4.2905e-02, -1.4395e-01,  1.5177e-01,  8.0015e-02, -6.7061e-02,\n",
            "        4.3183e-01, -1.2031e-01, -2.4649e-03,  6.1189e-02, -6.4813e-02,\n",
            "       -1.2514e-02, -1.0221e-01,  5.3943e-01, -1.1648e-01,  1.8633e-01,\n",
            "        7.9049e-01,  1.6360e-01,  1.3586e-01,  2.5753e-01, -2.8265e+00,\n",
            "        3.7164e-01, -6.5338e-02,  4.0969e-01,  3.7714e-01, -1.6049e-01,\n",
            "       -3.0867e-01,  3.7806e-01, -2.0131e-01,  1.2399e-01, -1.8783e-01,\n",
            "       -1.4787e-01,  2.8645e-01,  2.0822e-01,  3.5865e-02,  1.7129e-01,\n",
            "        2.1348e-01,  9.0371e-02, -5.1521e-02, -5.3712e-02, -9.5414e-02,\n",
            "       -1.2910e-01, -5.1104e-01,  1.1001e-02,  1.7955e-02,  7.1046e-01,\n",
            "        4.4096e-01,  1.0919e-01, -4.2861e-01,  3.6723e-01,  7.7160e-02,\n",
            "       -2.9743e-01,  1.9742e-01,  1.2850e-01,  5.0926e-01, -6.0001e-02,\n",
            "       -2.2562e-01,  1.9254e-01, -2.5240e-01,  1.4240e-01,  1.4344e-01,\n",
            "        1.2107e-01,  8.3029e-02,  4.4365e-01, -7.4981e-02, -6.7863e-02,\n",
            "        1.9964e-01,  2.3000e-02, -3.8300e-01,  1.5254e-01,  3.8500e-01,\n",
            "       -4.1078e-01,  7.3168e-01, -6.1930e-02,  2.3051e-01, -1.4631e-01,\n",
            "        5.3005e-01, -1.0556e-02,  1.4464e-01,  1.9615e-01,  1.4054e-01,\n",
            "       -8.9524e-02, -4.1271e-01, -1.8328e-01,  1.7044e-01, -2.3877e-01,\n",
            "       -4.5782e-01,  6.5239e-02, -2.5633e-02,  2.9908e-01,  4.0938e-01,\n",
            "        2.4078e-02,  1.0816e-01, -4.1394e-04,  7.5313e-01, -6.7311e-02],\n",
            "      dtype=float32), array([ 0.17347  ,  0.72212  ,  0.095553 ,  0.56536  ,  0.27536  ,\n",
            "        0.37784  , -2.6084   ,  0.11331  ,  0.74325  , -0.54417  ,\n",
            "        0.65544  ,  0.46552  ,  0.1239   ,  0.21685  ,  0.13992  ,\n",
            "        0.18037  , -0.59241  , -0.041912 ,  0.41894  , -0.68263  ,\n",
            "        0.34509  , -0.13962  ,  0.30971  ,  0.59596  , -0.1575   ,\n",
            "       -0.14879  , -0.48207  ,  0.046974 ,  0.37695  ,  0.30543  ,\n",
            "        0.12084  , -0.74593  ,  0.35346  , -0.27169  ,  0.54396  ,\n",
            "        0.27251  , -0.068291 , -0.22623  , -0.12268  , -0.24841  ,\n",
            "       -0.36431  ,  0.43594  , -0.089563 ,  0.10237  , -0.3186   ,\n",
            "        0.17334  , -0.45503  ,  0.28062  ,  0.72768  ,  0.20629  ,\n",
            "        0.17116  ,  0.0070106, -0.47289  , -0.25586  , -0.42402  ,\n",
            "       -0.071489 , -0.15642  ,  0.66474  ,  0.56163  , -0.28103  ,\n",
            "       -0.46575  ,  0.14648  , -0.029697 , -0.41037  ,  0.44008  ,\n",
            "       -0.1008   , -0.51038  , -0.15829  , -0.18338  , -0.39734  ,\n",
            "       -0.16388  ,  0.12622  ,  0.17165  , -0.13904  ,  0.34121  ,\n",
            "        0.042992 ,  0.40167  , -0.18994  , -0.55019  ,  0.27962  ,\n",
            "       -0.30977  ,  0.16821  ,  0.23206  , -0.22854  , -0.39088  ,\n",
            "       -0.39669  , -0.12067  , -0.55504  ,  0.096655 , -0.074444 ,\n",
            "        0.10056  , -0.081393 ,  0.35291  , -0.055993 ,  0.21348  ,\n",
            "        0.62015  , -1.73     ,  0.34423  ,  0.76524  ,  0.74556  ,\n",
            "       -0.46264  ,  0.0059947, -0.12587  ,  0.067965 ,  0.23341  ,\n",
            "       -0.20222  , -0.14207  ,  0.66028  ,  0.33395  , -0.43224  ,\n",
            "        0.80295  , -0.05405  , -0.41037  ,  0.2984   , -0.77224  ,\n",
            "        0.1181   ,  0.41461  ,  0.668    ,  0.033754 , -0.074989 ,\n",
            "        0.25372  ,  0.062323 , -0.017168 , -0.32585  , -0.036984 ,\n",
            "       -0.43729  , -0.2925   , -0.17518  ,  0.52134  , -0.35746  ,\n",
            "       -0.48277  ,  0.11676  , -0.039332 ,  0.25213  , -0.070178 ,\n",
            "       -0.20427  , -0.37748  ,  0.066655 , -0.33363  ,  0.35979  ,\n",
            "        0.1388   ,  0.44686  , -0.46653  ,  0.37679  , -0.01269  ,\n",
            "       -0.36931  , -0.095106 , -0.086172 , -0.048859 ,  0.040756 ,\n",
            "        0.55715  ,  0.12293  , -0.46191  ,  0.088948 , -0.0073891,\n",
            "       -0.10655  ,  0.074134 , -0.071839 ,  0.12981  ,  0.2623   ,\n",
            "       -0.14353  ,  0.26564  , -0.62303  , -0.72668  , -0.30549  ,\n",
            "       -0.88357  , -0.044792 , -0.086399 ,  0.10887  ,  0.47367  ,\n",
            "        0.093433 , -0.29447  , -0.11097  ,  0.14087  ,  0.22794  ,\n",
            "        0.26978  , -0.48439  , -0.17676  ,  0.20423  ,  0.042414 ,\n",
            "       -0.3242   ,  0.37864  , -0.13583  , -0.1893   ,  0.0211   ,\n",
            "       -0.21043  , -0.13206  , -0.071688 ,  0.27116  , -0.070021 ,\n",
            "        0.50155  ,  0.17115  , -0.034003 ,  0.279    , -0.45935  ,\n",
            "       -0.88041  , -0.026012 ,  0.04424  ,  0.37575  , -0.38958  ,\n",
            "       -0.029565 , -0.37136  , -0.39374  ,  0.072876 ,  0.12719  ,\n",
            "       -0.57471  ,  0.17475  ,  0.3307   ,  0.4695   ,  0.01479  ,\n",
            "        0.26663  , -0.1381   ,  0.35937  , -0.11994  , -0.20089  ,\n",
            "       -0.37143  , -0.27295  ,  0.046783 , -0.13042  , -0.4562   ,\n",
            "        0.75056  ,  0.098328 ,  0.44367  ,  0.82851  , -3.1551   ,\n",
            "       -0.16663  ,  0.33826  , -0.47216  ,  0.7974   , -0.18577  ,\n",
            "       -0.006572 , -0.027219 , -0.014801 ,  0.22343  , -0.44426  ,\n",
            "        0.11095  ,  0.49427  , -0.28162  , -0.18337  ,  0.51876  ,\n",
            "       -0.25616  ,  0.2641   , -0.029544 , -0.11418  , -0.045042 ,\n",
            "        0.29661  , -0.13871  ,  0.29945  ,  0.083044 ,  0.24088  ,\n",
            "        0.30631  , -0.052773 ,  0.22028  , -0.30392  , -0.43174  ,\n",
            "       -0.24416  , -0.048439 , -0.060884 , -0.021813 ,  0.080054 ,\n",
            "       -0.015563 ,  0.3059   ,  0.49353  , -0.51579  , -0.068909 ,\n",
            "        0.41021  ,  0.1794   , -0.17728  , -0.084603 , -0.15405  ,\n",
            "        0.52139  , -0.061963 ,  0.005559 ,  0.39022  ,  0.33027  ,\n",
            "       -0.12864  , -0.082968 ,  0.5718   ,  0.62805  ,  0.093276 ,\n",
            "       -0.15785  ,  0.030211 ,  0.15113  ,  0.63265  ,  0.38126  ,\n",
            "        0.089151 , -0.075917 ,  0.34895  ,  0.075394 , -0.31124  ,\n",
            "        0.21334  , -0.24063  , -0.078855 ,  0.21311  ,  0.067377 ,\n",
            "       -0.0075328,  0.20553  , -0.3187   ,  0.73854  ,  0.22615  ],\n",
            "      dtype=float32), array([-1.2700e-01,  3.0177e-01,  3.4226e-01, -2.6468e-01, -4.7371e-02,\n",
            "        1.6988e-01, -4.4144e+00,  4.3783e-01,  3.3566e-01, -2.4480e-01,\n",
            "        2.6837e-01,  1.4199e-01,  4.0601e-01, -3.5035e-01,  5.5878e-02,\n",
            "       -9.2213e-02,  4.8647e-02, -2.1177e-01,  1.2384e-02, -1.1755e-01,\n",
            "        4.8425e-02, -1.4063e-01,  2.0563e-01,  1.6473e-01,  1.0472e-01,\n",
            "       -3.8179e-01, -1.7540e-01,  1.3985e-02, -1.0245e-01,  2.5526e-02,\n",
            "       -4.8474e-01, -3.0900e-01,  5.0037e-01,  1.4320e-01,  3.9092e-01,\n",
            "        1.2342e-02, -1.3832e-01,  5.0708e-03,  9.8466e-02, -2.2247e-01,\n",
            "       -1.1754e-02,  4.4419e-01, -7.2688e-02, -6.5321e-01,  1.0697e-01,\n",
            "        6.6457e-02,  9.7483e-03,  1.3204e-01, -3.4844e-02, -5.3266e-02,\n",
            "       -1.7348e-01, -6.8761e-02, -1.5258e-01, -2.2968e-01,  3.4703e-01,\n",
            "       -6.8412e-02,  1.2319e-01, -1.2114e-01, -1.4506e-01,  1.6131e-01,\n",
            "        3.7976e-02, -1.7532e-01, -2.4602e-01,  8.0949e-03,  4.1524e-01,\n",
            "        2.3299e-01,  5.1998e-02,  2.8031e-03,  3.5813e-03, -2.0855e-01,\n",
            "       -9.8870e-02,  5.2083e-02,  3.3444e-02, -1.1963e-01, -2.9797e-01,\n",
            "        7.2394e-02,  4.2882e-01,  2.3052e-01,  1.6354e-01, -1.4370e-01,\n",
            "        6.9838e-02,  6.2951e-02, -4.0852e-01,  7.7168e-02, -9.5317e-02,\n",
            "       -9.6269e-02,  2.4338e-01,  3.9439e-02, -1.1930e-02,  7.0856e-02,\n",
            "        2.0315e-01, -5.4227e-01, -3.0088e-01,  2.2012e-01,  1.0204e-01,\n",
            "       -8.7827e-01, -2.5467e+00, -1.3059e-01,  2.1439e-01,  1.3250e-01,\n",
            "       -2.7299e-01, -5.6703e-02,  3.4196e-02,  1.3453e-02,  1.8776e-01,\n",
            "        3.4716e-02,  4.1717e-02, -4.0456e-02, -2.1981e-01, -7.0382e-02,\n",
            "        1.7131e-01, -2.0318e-01,  1.2087e-01,  1.5904e-01, -2.4554e-01,\n",
            "       -9.8235e-02, -2.4130e-01, -1.4055e-02,  5.2053e-03,  1.9077e-01,\n",
            "        1.5404e-01,  2.0754e-01,  1.4213e-01, -4.3726e-02,  1.0304e-01,\n",
            "       -2.3264e-01, -9.0714e-02, -1.2269e-01, -3.5555e-02, -2.6445e-01,\n",
            "       -2.8969e-01,  3.7851e-01, -4.1938e-02, -6.9835e-03,  1.1180e-01,\n",
            "       -3.6043e-01, -7.5913e-02, -1.2619e-01,  5.3433e-02,  5.9954e-01,\n",
            "        1.9558e-01,  3.2103e-01,  1.8136e-01, -2.3762e-01, -1.0549e-01,\n",
            "       -1.5645e-01, -2.7518e-01, -1.2304e-01,  1.1196e-01,  1.9336e-01,\n",
            "       -3.5942e-02, -2.9606e-01,  5.1979e-01, -2.9756e-01,  2.3084e-02,\n",
            "       -3.3086e-02,  3.9534e-02, -1.3224e-01,  3.6706e-01,  1.0987e-01,\n",
            "       -1.3877e-01, -1.8236e-02, -2.9123e-01, -1.7135e-01,  1.7614e-01,\n",
            "        1.1959e-01,  1.1596e-01,  4.6442e-03, -2.6559e-02,  1.9500e-01,\n",
            "       -2.3883e-01,  3.8745e-02, -1.0711e-01, -1.4648e-01,  3.9696e-02,\n",
            "       -1.4365e-01, -2.9095e-02,  9.9169e-02,  1.0573e-01, -1.5243e-01,\n",
            "        1.7535e-01,  1.1345e-01, -1.1334e-01, -6.4171e-02, -1.0007e-01,\n",
            "       -1.3541e-01,  2.7705e-01,  2.7753e-01,  1.3319e-01,  9.8197e-03,\n",
            "       -1.0992e-01, -4.2945e-01,  2.4436e-01, -9.6194e-02,  4.0666e-02,\n",
            "       -9.6933e-02,  1.4274e-01,  1.9105e-01, -1.6814e-02, -9.4383e-02,\n",
            "        4.8403e-01, -8.4497e-02, -4.6428e-01,  2.1462e-01, -5.1064e-02,\n",
            "        1.3422e-01,  1.8706e-01, -1.8531e-01,  2.3570e-01, -5.4037e-02,\n",
            "       -2.3294e-01, -3.1561e-01,  3.5166e-01, -4.2467e-01, -2.8877e-01,\n",
            "       -2.0482e-01,  9.8418e-02, -3.1717e-01,  1.1570e-01, -4.2938e-01,\n",
            "        1.9749e-02, -1.5630e-01, -4.4933e-01, -6.5736e-02, -2.2033e+00,\n",
            "        6.8076e-01,  1.6524e-01,  1.4523e-01,  2.0988e-01, -5.4221e-02,\n",
            "       -2.6381e-01,  4.6167e-02, -1.8353e-02, -8.6028e-02, -7.7295e-02,\n",
            "        2.5966e-01, -2.7805e-01, -1.3176e-01,  3.4568e-02,  3.2705e-02,\n",
            "       -2.6196e-02, -1.5631e-01, -5.6283e-01, -2.5608e-01, -7.2810e-02,\n",
            "       -2.7644e-01,  1.6727e-02,  2.8475e-01, -1.1031e-01, -7.8391e-02,\n",
            "       -3.4895e-02, -1.0470e-01, -4.1329e-02, -4.4123e-02,  1.7455e-01,\n",
            "       -1.3371e-01,  3.0550e-01,  6.3527e-02, -5.6659e-02,  1.6943e-01,\n",
            "       -7.3332e-02, -3.6405e-01,  6.0335e-02, -3.4675e-01,  8.8905e-03,\n",
            "       -1.9473e-01,  3.6273e-02,  2.4839e-01, -3.6873e-03,  2.0491e-02,\n",
            "       -1.5816e-01, -2.2727e-01, -5.8895e-02, -1.1133e-01,  1.2658e-01,\n",
            "       -2.7108e-01,  5.2010e-01,  6.3909e-02,  3.0876e-01,  7.1974e-02,\n",
            "        2.5641e-01, -1.0525e-01, -6.3905e-02,  2.7271e-01, -1.3210e-01,\n",
            "        2.7420e-01,  4.4627e-02,  4.7989e-01,  1.6915e-01, -4.0537e-01,\n",
            "       -4.4061e-01, -2.6426e-01, -3.3296e-01, -8.8765e-02, -1.6545e-02,\n",
            "       -4.6084e-01, -5.4303e-02, -2.4947e-02,  2.5758e-01,  1.4487e-02],\n",
            "      dtype=float32), array([-3.5381e-01, -8.8830e-02,  2.8371e-01, -8.2593e-02,  5.0503e-02,\n",
            "       -2.6584e-02, -3.2806e+00, -1.6482e-02, -1.5377e-01, -7.3285e-01,\n",
            "        1.1802e-01, -8.2432e-03,  3.4731e-01, -1.3305e-01, -9.4212e-02,\n",
            "       -2.2979e-02, -1.9351e-01,  9.8277e-02,  2.0244e-02,  4.6315e-02,\n",
            "        5.1662e-01, -1.2814e-01,  4.4096e-01,  4.8966e-02, -3.0687e-01,\n",
            "        4.3222e-02, -1.6831e-01,  1.0894e-02, -9.3771e-02, -1.2869e-01,\n",
            "       -2.2235e-01, -1.7181e-01,  3.3371e-02,  3.5540e-01, -2.3589e-02,\n",
            "        5.2940e-02, -3.8635e-01,  3.0513e-01, -2.6722e-01,  6.5108e-02,\n",
            "       -1.9588e-01,  1.4437e-01, -7.5669e-02,  9.8447e-02, -5.7364e-02,\n",
            "        1.9927e-01,  1.6435e-01,  7.5672e-02,  2.6489e-01, -2.9818e-02,\n",
            "       -1.6021e-01, -3.1271e-01,  2.2412e-01, -1.0176e-01,  3.2144e-02,\n",
            "       -2.7950e-01,  2.4260e-01,  1.5138e-01,  2.4356e-01,  1.3051e-01,\n",
            "        3.7660e-01, -2.3499e-02,  3.5254e-03, -3.9445e-03,  2.7800e-01,\n",
            "        1.4040e-01,  1.8458e-01, -1.9416e-01, -1.9128e-01, -1.0825e-01,\n",
            "       -2.6313e-01, -4.7898e-02,  2.3296e-01,  4.9132e-02, -1.7888e-01,\n",
            "        6.8258e-02,  2.1048e-01,  9.0140e-02,  3.2054e-01, -7.0877e-02,\n",
            "        1.1774e-01, -2.7378e-01, -2.9880e-01,  1.7398e-03,  1.0419e-01,\n",
            "       -1.2171e-02,  2.1464e-01,  8.6812e-02,  4.0988e-02,  3.2775e-01,\n",
            "        1.2581e-01,  2.5306e-01,  2.2068e-01, -2.0338e-01, -1.4772e-01,\n",
            "       -3.4748e-01, -1.2321e+00,  1.4038e-01,  5.4342e-01, -4.7731e-01,\n",
            "        1.5466e-03, -2.9012e-01, -2.3668e-01, -2.7674e-01,  2.0965e-01,\n",
            "        1.2115e-01,  1.3060e-01, -7.4088e-01, -9.9738e-02, -2.0418e-01,\n",
            "        3.7772e-01, -3.3499e-01,  1.7638e-03,  1.1448e-01, -1.2890e-01,\n",
            "        6.4825e-01, -3.6592e-01, -4.5224e-01,  2.8464e-01,  8.3726e-02,\n",
            "       -5.4846e-02,  2.3871e-01,  3.8124e-02,  5.0775e-02, -1.2438e-01,\n",
            "       -6.6526e-01, -3.0305e-02, -9.4721e-02,  4.7167e-02,  2.5766e-01,\n",
            "       -6.2675e-01,  2.6496e-01, -1.2670e-01,  5.1965e-02,  3.5726e-01,\n",
            "       -3.5636e-01,  2.8688e-01, -1.1731e-02, -5.8082e-01,  5.7855e-01,\n",
            "        4.0022e-01,  3.0751e-01,  2.4664e-01,  2.6809e-01, -5.6866e-01,\n",
            "       -3.4155e-01,  1.3046e-02, -4.5153e-02,  9.1635e-02,  1.7852e-01,\n",
            "        1.7526e-01,  2.2010e-02,  3.1673e-01, -3.3166e-01,  3.5132e-01,\n",
            "       -1.5935e-01, -7.6997e-02,  8.1530e-02, -1.1950e-01,  1.5095e-01,\n",
            "        6.7216e-03,  3.1646e-02, -2.8429e-01, -6.3885e-05,  2.6739e-01,\n",
            "       -2.3389e-02, -2.5811e-01,  2.9215e-01, -9.8760e-02, -1.9435e-02,\n",
            "       -2.6590e-01,  1.6001e-01, -1.3090e-01, -2.5980e-01, -1.9408e-01,\n",
            "        1.9110e-01, -5.8213e-02,  1.8939e-01, -1.4884e-01, -2.5834e-01,\n",
            "        2.1246e-01,  2.1307e-01, -1.4915e-01, -1.5781e-01, -1.7513e-01,\n",
            "        1.1132e-02,  3.6164e-02,  1.0748e-02, -1.0260e-01,  7.6385e-02,\n",
            "        2.7697e-01,  1.1661e-01,  1.1401e-01, -4.4741e-02,  1.8345e-01,\n",
            "       -1.2760e-01,  1.1030e-01, -1.1185e-02,  5.1054e-01,  2.4319e-01,\n",
            "        5.8781e-02, -1.0413e-01, -2.4263e-01, -1.0185e-01, -4.7067e-01,\n",
            "       -6.3661e-03, -1.5871e-01, -2.1366e-01,  8.7986e-01, -2.0957e-01,\n",
            "       -1.2122e-01, -1.6119e-01, -1.3273e-01,  3.7434e-02,  6.3256e-02,\n",
            "        1.6428e-01, -2.0640e-01,  4.4090e-01,  1.8998e-01,  1.7605e-01,\n",
            "        1.0910e-01,  3.8985e-02, -1.0209e-01, -5.1496e-02, -2.6625e+00,\n",
            "        2.3743e-01,  4.0433e-01,  2.5681e-01,  6.5472e-02, -1.4838e-01,\n",
            "       -1.1268e-01, -6.9206e-02,  5.7601e-02,  2.5692e-01, -2.5300e-01,\n",
            "       -2.2872e-01, -1.4601e-01,  9.2534e-02,  1.2424e-01,  1.5555e-01,\n",
            "       -3.4003e-01,  1.7264e-02, -6.7655e-01,  2.5653e-01,  2.6402e-01,\n",
            "        9.1459e-02, -2.2176e-01,  3.6491e-01,  2.8709e-01,  1.4913e-01,\n",
            "        1.3906e-01,  3.5987e-01, -1.7770e-01, -2.1141e-01,  2.0889e-01,\n",
            "       -4.1264e-01,  4.0428e-01,  5.0273e-01, -4.0098e-02, -3.9538e-01,\n",
            "       -4.4577e-01,  2.6848e-02,  2.4513e-01, -1.8224e-01,  2.9706e-01,\n",
            "       -1.2307e-01,  3.2117e-01,  6.9552e-02,  3.4860e-01,  4.7685e-02,\n",
            "        1.3976e-01,  2.0935e-02, -7.2212e-02, -3.7044e-01,  4.8667e-03,\n",
            "       -1.2069e-01,  1.7815e-01, -1.4476e-01,  5.4661e-02, -1.3756e-01,\n",
            "        4.2052e-01,  1.9829e-01, -5.4238e-02,  2.0827e-01, -2.6685e-02,\n",
            "        1.7233e-02, -1.7325e-01,  3.4999e-02, -1.4851e-01,  1.3920e-01,\n",
            "        1.2473e-01, -3.7767e-02, -1.7657e-02, -2.6770e-01,  8.1862e-02,\n",
            "        2.5351e-02,  9.6778e-02,  1.7381e-01,  1.9977e-01,  6.0289e-02],\n",
            "      dtype=float32), array([ 2.2694e-02,  2.4897e-01, -9.5021e-02, -3.6910e-01,  2.6657e-01,\n",
            "        4.1412e-01, -3.4947e+00,  5.1654e-02, -2.3412e-01, -7.7341e-01,\n",
            "       -4.5300e-02, -1.9296e-01, -8.5690e-02, -5.1874e-02,  1.0586e-01,\n",
            "        3.1765e-01, -5.8077e-01, -1.1138e-02, -3.4938e-02, -6.5235e-02,\n",
            "        1.0970e-01, -3.8012e-01,  5.6088e-01, -1.2869e-01, -2.0679e-01,\n",
            "       -3.7799e-01,  9.6730e-02,  3.8808e-01,  1.4324e-01, -1.0353e-01,\n",
            "       -6.6100e-01, -5.8098e-02, -1.5082e-01,  2.3545e-01, -8.0767e-02,\n",
            "       -3.4974e-01,  1.8467e-01, -3.2577e-01,  1.1189e-01, -2.9690e-01,\n",
            "       -1.1195e-02,  3.2295e-02,  5.7096e-03, -1.3593e-01,  9.5731e-03,\n",
            "        6.8052e-01, -2.1068e-01, -4.8930e-02,  1.6024e-01, -1.7477e-01,\n",
            "       -1.8158e-01, -2.3035e-01, -1.9890e-01, -5.0520e-01,  8.5665e-02,\n",
            "       -1.9918e-01,  9.0549e-03, -4.6137e-01,  1.9691e-01,  2.8229e-01,\n",
            "        1.8480e-01, -1.6741e-01, -2.6818e-01,  2.2816e-01, -2.0775e-01,\n",
            "       -4.4649e-01,  2.7694e-01,  6.7895e-01, -4.5793e-02, -1.8089e-01,\n",
            "        9.3328e-03, -5.3135e-01, -2.7461e-01, -3.5258e-02,  1.1774e-01,\n",
            "       -4.4298e-01,  4.3597e-01,  2.1816e-01,  9.0363e-02, -7.0156e-02,\n",
            "       -7.0747e-02, -6.2537e-01,  1.5579e-01, -2.2559e-02, -3.2217e-01,\n",
            "       -1.4742e-01,  5.0730e-01, -3.7688e-02,  3.0628e-01, -1.4461e-02,\n",
            "        6.2131e-02, -1.5825e-01,  1.5364e-02, -7.4417e-02, -1.4787e-01,\n",
            "       -4.0786e-01, -2.0529e+00, -4.4138e-02,  1.4094e-01, -7.6259e-02,\n",
            "       -2.8055e-01,  5.0686e-03, -1.3341e-01,  8.2195e-03,  3.4061e-01,\n",
            "        6.5468e-02, -6.5924e-02, -5.4573e-01, -3.7687e-01, -1.3391e-02,\n",
            "       -1.3272e-01, -6.3375e-02,  8.2473e-02, -1.8134e-01,  1.7999e-02,\n",
            "        2.9581e-02,  3.3396e-02,  1.5525e-01,  2.6841e-01, -1.2527e-01,\n",
            "        3.2386e-01,  9.6064e-03, -5.7866e-02, -6.8499e-02, -1.3885e-01,\n",
            "       -8.8896e-02, -1.3534e-01, -3.5906e-01,  7.9651e-01,  2.3781e-01,\n",
            "        3.7800e-01, -4.1983e-02,  3.4190e-02,  1.1818e-01, -1.5707e-01,\n",
            "       -3.0381e-01, -9.2308e-02,  1.4459e-01,  1.9177e-01,  5.6057e-01,\n",
            "       -1.7149e-01, -2.9332e-01, -2.2296e-01, -4.9628e-01, -1.8446e-01,\n",
            "       -4.9629e-01, -2.3698e-01, -2.3507e-02, -1.6492e-01, -2.1177e-01,\n",
            "        2.6996e-01,  9.6660e-03, -3.0221e-01,  1.4926e-01,  4.7580e-01,\n",
            "       -1.8042e-01,  1.7160e-01, -2.1143e-01, -6.7134e-01,  2.2100e-01,\n",
            "       -4.8915e-01, -3.5391e-01,  4.5101e-02, -6.0116e-02,  1.3493e-01,\n",
            "        1.8858e-01,  3.1378e-01, -5.2392e-01,  2.0180e-01, -5.2515e-01,\n",
            "       -6.7844e-02, -2.0039e-01, -3.8274e-01, -1.3041e-01,  3.3682e-02,\n",
            "       -3.0067e-02, -7.6332e-03, -9.1640e-03, -3.9040e-01, -1.9008e-01,\n",
            "        5.8987e-02,  5.9357e-01, -2.6038e-01,  4.8443e-04, -8.6927e-02,\n",
            "        7.3700e-02, -3.3731e-01, -6.0308e-02,  4.0258e-01, -2.1916e-01,\n",
            "       -1.8414e-02, -1.5710e-01, -2.1539e-01, -9.9908e-02, -3.3318e-01,\n",
            "        2.3880e-01, -6.7434e-02, -4.7498e-02, -2.2145e-01, -2.3592e-01,\n",
            "        1.6579e-01,  2.3687e-01, -3.3336e-01,  1.6060e-01, -3.3294e-01,\n",
            "        2.6756e-01,  2.6762e-01, -5.9468e-01, -2.6436e-01,  4.8464e-01,\n",
            "        2.8035e-01, -4.0649e-01,  2.5073e-01, -8.5135e-01, -2.2194e-01,\n",
            "        3.8271e-02,  1.2219e-02,  1.5534e-01,  6.6803e-02,  5.3540e-01,\n",
            "        4.6049e-02,  1.3953e-01, -4.2762e-01, -2.6050e-01, -2.7046e+00,\n",
            "       -1.6203e-01,  4.2490e-01,  2.4144e-01, -3.2836e-01, -1.9591e-01,\n",
            "       -2.3677e-01,  4.4536e-02, -3.5793e-01, -4.3380e-01, -1.9715e-01,\n",
            "        3.1722e-01,  4.5511e-01, -3.2718e-01, -1.7193e-02, -3.1377e-01,\n",
            "        1.7999e-01,  2.4004e-02, -1.4924e-01, -1.7125e-01, -1.0110e-01,\n",
            "        7.2512e-02, -1.6251e-02,  6.8044e-02,  3.7749e-02,  5.1249e-01,\n",
            "        8.9680e-02, -7.8855e-02,  1.0415e-01, -6.6420e-02,  4.3469e-01,\n",
            "       -8.0054e-02, -8.2720e-03, -4.1351e-01, -1.4963e-01,  3.1244e-01,\n",
            "       -3.2654e-01,  2.7329e-01, -7.7078e-01, -5.0356e-01,  3.8692e-02,\n",
            "       -3.1167e-01,  1.5501e-01, -2.2352e-02,  2.7781e-01,  2.3732e-01,\n",
            "        2.9546e-01,  6.3223e-01, -4.3793e-01, -5.2959e-01,  1.3575e-01,\n",
            "       -2.4510e-01,  1.0610e-01,  9.6401e-02,  2.4398e-01,  8.0186e-02,\n",
            "        9.2972e-01, -6.6424e-02, -6.2171e-02,  1.2988e-01,  1.4679e-01,\n",
            "        2.6813e-01, -2.0166e-01,  2.0611e-01, -3.1860e-01,  2.1611e-01,\n",
            "        2.7711e-02, -1.0940e-01,  7.4457e-02,  5.8247e-03,  1.6894e-01,\n",
            "       -2.9028e-01,  3.3345e-01,  1.1111e-01,  3.5210e-01, -2.1549e-01],\n",
            "      dtype=float32), array([ 2.2694e-02,  2.4897e-01, -9.5021e-02, -3.6910e-01,  2.6657e-01,\n",
            "        4.1412e-01, -3.4947e+00,  5.1654e-02, -2.3412e-01, -7.7341e-01,\n",
            "       -4.5300e-02, -1.9296e-01, -8.5690e-02, -5.1874e-02,  1.0586e-01,\n",
            "        3.1765e-01, -5.8077e-01, -1.1138e-02, -3.4938e-02, -6.5235e-02,\n",
            "        1.0970e-01, -3.8012e-01,  5.6088e-01, -1.2869e-01, -2.0679e-01,\n",
            "       -3.7799e-01,  9.6730e-02,  3.8808e-01,  1.4324e-01, -1.0353e-01,\n",
            "       -6.6100e-01, -5.8098e-02, -1.5082e-01,  2.3545e-01, -8.0767e-02,\n",
            "       -3.4974e-01,  1.8467e-01, -3.2577e-01,  1.1189e-01, -2.9690e-01,\n",
            "       -1.1195e-02,  3.2295e-02,  5.7096e-03, -1.3593e-01,  9.5731e-03,\n",
            "        6.8052e-01, -2.1068e-01, -4.8930e-02,  1.6024e-01, -1.7477e-01,\n",
            "       -1.8158e-01, -2.3035e-01, -1.9890e-01, -5.0520e-01,  8.5665e-02,\n",
            "       -1.9918e-01,  9.0549e-03, -4.6137e-01,  1.9691e-01,  2.8229e-01,\n",
            "        1.8480e-01, -1.6741e-01, -2.6818e-01,  2.2816e-01, -2.0775e-01,\n",
            "       -4.4649e-01,  2.7694e-01,  6.7895e-01, -4.5793e-02, -1.8089e-01,\n",
            "        9.3328e-03, -5.3135e-01, -2.7461e-01, -3.5258e-02,  1.1774e-01,\n",
            "       -4.4298e-01,  4.3597e-01,  2.1816e-01,  9.0363e-02, -7.0156e-02,\n",
            "       -7.0747e-02, -6.2537e-01,  1.5579e-01, -2.2559e-02, -3.2217e-01,\n",
            "       -1.4742e-01,  5.0730e-01, -3.7688e-02,  3.0628e-01, -1.4461e-02,\n",
            "        6.2131e-02, -1.5825e-01,  1.5364e-02, -7.4417e-02, -1.4787e-01,\n",
            "       -4.0786e-01, -2.0529e+00, -4.4138e-02,  1.4094e-01, -7.6259e-02,\n",
            "       -2.8055e-01,  5.0686e-03, -1.3341e-01,  8.2195e-03,  3.4061e-01,\n",
            "        6.5468e-02, -6.5924e-02, -5.4573e-01, -3.7687e-01, -1.3391e-02,\n",
            "       -1.3272e-01, -6.3375e-02,  8.2473e-02, -1.8134e-01,  1.7999e-02,\n",
            "        2.9581e-02,  3.3396e-02,  1.5525e-01,  2.6841e-01, -1.2527e-01,\n",
            "        3.2386e-01,  9.6064e-03, -5.7866e-02, -6.8499e-02, -1.3885e-01,\n",
            "       -8.8896e-02, -1.3534e-01, -3.5906e-01,  7.9651e-01,  2.3781e-01,\n",
            "        3.7800e-01, -4.1983e-02,  3.4190e-02,  1.1818e-01, -1.5707e-01,\n",
            "       -3.0381e-01, -9.2308e-02,  1.4459e-01,  1.9177e-01,  5.6057e-01,\n",
            "       -1.7149e-01, -2.9332e-01, -2.2296e-01, -4.9628e-01, -1.8446e-01,\n",
            "       -4.9629e-01, -2.3698e-01, -2.3507e-02, -1.6492e-01, -2.1177e-01,\n",
            "        2.6996e-01,  9.6660e-03, -3.0221e-01,  1.4926e-01,  4.7580e-01,\n",
            "       -1.8042e-01,  1.7160e-01, -2.1143e-01, -6.7134e-01,  2.2100e-01,\n",
            "       -4.8915e-01, -3.5391e-01,  4.5101e-02, -6.0116e-02,  1.3493e-01,\n",
            "        1.8858e-01,  3.1378e-01, -5.2392e-01,  2.0180e-01, -5.2515e-01,\n",
            "       -6.7844e-02, -2.0039e-01, -3.8274e-01, -1.3041e-01,  3.3682e-02,\n",
            "       -3.0067e-02, -7.6332e-03, -9.1640e-03, -3.9040e-01, -1.9008e-01,\n",
            "        5.8987e-02,  5.9357e-01, -2.6038e-01,  4.8443e-04, -8.6927e-02,\n",
            "        7.3700e-02, -3.3731e-01, -6.0308e-02,  4.0258e-01, -2.1916e-01,\n",
            "       -1.8414e-02, -1.5710e-01, -2.1539e-01, -9.9908e-02, -3.3318e-01,\n",
            "        2.3880e-01, -6.7434e-02, -4.7498e-02, -2.2145e-01, -2.3592e-01,\n",
            "        1.6579e-01,  2.3687e-01, -3.3336e-01,  1.6060e-01, -3.3294e-01,\n",
            "        2.6756e-01,  2.6762e-01, -5.9468e-01, -2.6436e-01,  4.8464e-01,\n",
            "        2.8035e-01, -4.0649e-01,  2.5073e-01, -8.5135e-01, -2.2194e-01,\n",
            "        3.8271e-02,  1.2219e-02,  1.5534e-01,  6.6803e-02,  5.3540e-01,\n",
            "        4.6049e-02,  1.3953e-01, -4.2762e-01, -2.6050e-01, -2.7046e+00,\n",
            "       -1.6203e-01,  4.2490e-01,  2.4144e-01, -3.2836e-01, -1.9591e-01,\n",
            "       -2.3677e-01,  4.4536e-02, -3.5793e-01, -4.3380e-01, -1.9715e-01,\n",
            "        3.1722e-01,  4.5511e-01, -3.2718e-01, -1.7193e-02, -3.1377e-01,\n",
            "        1.7999e-01,  2.4004e-02, -1.4924e-01, -1.7125e-01, -1.0110e-01,\n",
            "        7.2512e-02, -1.6251e-02,  6.8044e-02,  3.7749e-02,  5.1249e-01,\n",
            "        8.9680e-02, -7.8855e-02,  1.0415e-01, -6.6420e-02,  4.3469e-01,\n",
            "       -8.0054e-02, -8.2720e-03, -4.1351e-01, -1.4963e-01,  3.1244e-01,\n",
            "       -3.2654e-01,  2.7329e-01, -7.7078e-01, -5.0356e-01,  3.8692e-02,\n",
            "       -3.1167e-01,  1.5501e-01, -2.2352e-02,  2.7781e-01,  2.3732e-01,\n",
            "        2.9546e-01,  6.3223e-01, -4.3793e-01, -5.2959e-01,  1.3575e-01,\n",
            "       -2.4510e-01,  1.0610e-01,  9.6401e-02,  2.4398e-01,  8.0186e-02,\n",
            "        9.2972e-01, -6.6424e-02, -6.2171e-02,  1.2988e-01,  1.4679e-01,\n",
            "        2.6813e-01, -2.0166e-01,  2.0611e-01, -3.1860e-01,  2.1611e-01,\n",
            "        2.7711e-02, -1.0940e-01,  7.4457e-02,  5.8247e-03,  1.6894e-01,\n",
            "       -2.9028e-01,  3.3345e-01,  1.1111e-01,  3.5210e-01, -2.1549e-01],\n",
            "      dtype=float32), array([-2.1749e-01,  2.4543e-01, -2.4232e-01,  2.0514e-01,  5.2063e-01,\n",
            "        2.8674e-01, -2.5347e+00,  5.5500e-01, -3.3821e-01, -3.1260e-01,\n",
            "        3.3941e-02,  5.1323e-02, -1.6392e-01,  6.1026e-02, -5.0825e-01,\n",
            "        7.7383e-02, -5.2517e-02,  7.3011e-01,  4.7901e-02, -4.3211e-02,\n",
            "        5.8813e-01, -2.1255e-01,  4.1731e-02,  2.8478e-01,  4.7071e-01,\n",
            "        1.7965e-02, -3.5946e-01,  1.4305e-01, -2.6788e-01,  3.7886e-01,\n",
            "       -3.8900e-01,  5.8034e-01,  1.4683e-01, -2.5061e-01, -3.1886e-02,\n",
            "       -4.9444e-01,  1.7811e-01, -3.0072e-01, -2.7652e-01,  3.1219e-01,\n",
            "       -3.7878e-01,  2.9187e-01, -8.6919e-01, -1.8795e-01,  3.1501e-01,\n",
            "       -4.2365e-01, -3.0031e-01,  2.6678e-02, -3.2397e-01,  6.5453e-01,\n",
            "       -4.0139e-02,  9.2529e-02,  3.0106e-01, -3.9732e-01, -3.8780e-01,\n",
            "        5.1012e-02, -2.7779e-01,  1.6215e-01,  3.8341e-01,  1.3305e-01,\n",
            "        1.8122e-01, -1.0753e-01,  4.8374e-01,  1.4100e-01,  2.0636e-01,\n",
            "        2.5830e-01, -3.3179e-01, -3.2697e-01,  4.1599e-01,  1.3261e-01,\n",
            "        3.2164e-01, -3.2235e-01, -9.6987e-02, -1.7659e-01,  6.8433e-01,\n",
            "       -3.2514e-01, -4.4132e-02, -2.5832e-01,  6.5073e-01, -3.2608e-02,\n",
            "       -8.3348e-02,  6.7078e-01,  3.8866e-01,  4.0679e-02,  3.2792e-01,\n",
            "       -4.2540e-02,  5.2095e-01, -2.5594e-01, -4.2892e-02, -3.8561e-01,\n",
            "       -4.1762e-01,  5.1196e-02,  5.1000e-01, -5.4671e-01, -5.9051e-01,\n",
            "       -4.7142e-01, -1.5229e+00,  5.0341e-01,  5.7436e-02,  2.0472e-01,\n",
            "       -2.9485e-01, -4.4382e-01,  9.7591e-01, -1.4054e-01, -3.1962e-01,\n",
            "        1.9591e-01, -1.4781e-01,  3.1691e-01, -9.4808e-01, -3.1031e-01,\n",
            "       -4.5967e-01, -5.7002e-01, -1.5729e-01, -7.3317e-01,  7.2323e-02,\n",
            "       -7.0112e-01, -5.7420e-01, -2.6689e-01,  2.2304e-01, -1.2011e-01,\n",
            "        1.7702e-01,  1.6259e-02,  3.7153e-02,  5.3105e-01,  1.5289e-01,\n",
            "        5.6317e-01, -1.3285e-01, -8.2971e-02, -1.3998e-02,  3.6054e-01,\n",
            "       -2.4532e-01,  3.4862e-01,  6.6295e-01,  4.0594e-01, -4.8323e-01,\n",
            "       -9.4129e-02, -3.3965e-01, -6.5401e-01, -1.8720e-01,  7.2072e-01,\n",
            "        3.4991e-01, -9.4906e-03,  4.0399e-01, -6.6426e-01, -3.7648e-01,\n",
            "        3.4539e-01,  2.0848e-01, -9.4328e-02, -4.0723e-01, -3.9155e-01,\n",
            "       -1.9062e-01, -7.4004e-03,  1.1189e-01,  5.9171e-01,  6.3709e-01,\n",
            "        4.6432e-01,  8.3094e-02, -2.7098e-01,  3.9048e-01, -3.4628e-01,\n",
            "        2.0212e-01,  4.1407e-01,  8.4358e-01,  6.3363e-01,  4.7894e-01,\n",
            "       -6.4504e-01,  2.9235e-02, -1.8184e-01,  1.8336e-01, -1.8070e-01,\n",
            "        3.3111e-01, -1.1579e-03,  6.4035e-01, -6.3727e-01, -1.5192e-01,\n",
            "       -4.9002e-01,  6.7329e-01, -3.9873e-01, -5.8535e-01, -2.9986e-01,\n",
            "        3.7676e-01,  1.4549e-01,  1.8175e-01, -2.1465e-01,  1.0340e-02,\n",
            "       -7.0425e-01, -3.9346e-01, -5.1135e-01, -1.6566e-01, -1.2985e-01,\n",
            "       -3.3617e-01,  4.7691e-01, -6.3908e-02, -3.1605e-01,  8.1726e-02,\n",
            "        3.0191e-01, -3.6564e-01,  1.1810e-01, -1.0279e-01, -1.4876e-01,\n",
            "       -4.4074e-01,  8.5981e-02,  8.3934e-02, -2.8059e-01, -5.7192e-01,\n",
            "        4.4330e-01,  1.7729e-01,  1.5605e-01, -2.0139e-01,  4.7783e-02,\n",
            "       -2.3955e-01, -5.8217e-01,  1.7327e-02, -1.5339e-01, -6.3892e-02,\n",
            "        2.4358e-02,  1.5690e-01,  1.6944e-01, -3.8866e-01,  1.7526e-01,\n",
            "        1.3139e-01,  7.8376e-01, -2.1283e-02, -2.8592e-03, -2.4562e+00,\n",
            "        5.6729e-02,  1.5757e-01, -5.9311e-01, -5.1679e-03,  2.6356e-01,\n",
            "        4.7552e-01, -1.7829e-01, -3.5868e-01, -2.9230e-01,  3.0070e-01,\n",
            "       -9.7295e-02, -3.8692e-01, -7.3922e-02, -8.2433e-01, -5.3691e-01,\n",
            "        3.3366e-03, -7.3084e-01,  7.6621e-02,  6.9413e-01, -1.7890e-01,\n",
            "        3.4846e-01, -1.4647e-01,  6.5744e-01,  4.5058e-01,  4.8002e-01,\n",
            "        1.9839e-01,  3.7916e-01, -9.7311e-02, -4.6915e-01,  1.2906e-02,\n",
            "        4.0277e-01,  2.9005e-01, -1.3655e-01, -3.0007e-01, -2.8419e-01,\n",
            "        2.6574e-01,  5.5798e-02,  1.2871e-03,  1.0482e-01,  1.6386e-01,\n",
            "       -2.2719e-01, -7.5520e-02,  8.1075e-01, -2.5052e-01,  3.7104e-01,\n",
            "       -4.3872e-02,  3.1186e-01,  2.3877e-01, -2.6666e-01, -2.4868e-01,\n",
            "       -1.4020e-01, -2.0802e-01,  7.5834e-01,  3.5258e-01, -3.1391e-01,\n",
            "       -3.9379e-01,  3.3628e-01, -1.1883e-02,  6.0201e-01, -6.7904e-01,\n",
            "       -2.1852e-01,  3.7465e-01, -6.8857e-01,  1.8765e-01,  1.6381e-01,\n",
            "        1.5644e-02,  2.2835e-03,  2.7778e-01, -2.4920e-01, -5.1406e-02,\n",
            "       -2.0325e-01,  5.4940e-02, -3.3201e-01,  3.5822e-01,  1.6779e-01],\n",
            "      dtype=float32), array([-7.9926e-03, -5.5351e-01,  2.5180e-01, -2.6775e-01,  5.2682e-01,\n",
            "        7.8011e-01, -3.2841e+00,  3.9134e-01, -4.3216e-01, -4.9800e-03,\n",
            "       -9.7502e-02,  2.5742e-02,  5.3134e-01,  3.8607e-01,  1.1095e-02,\n",
            "       -2.6313e-03, -6.2583e-01,  3.7581e-01, -1.1871e-02,  6.1248e-02,\n",
            "        3.9393e-02,  4.1884e-01, -3.6712e-01, -2.8767e-01,  4.3724e-01,\n",
            "       -5.3605e-01,  1.2345e-01, -6.1628e-01, -1.2631e-02,  1.2375e-01,\n",
            "       -1.0868e-01,  9.4232e-02,  2.0318e-01, -3.6850e-01,  2.7691e-01,\n",
            "       -2.5305e-01,  1.6200e-01, -3.2348e-01,  3.0152e-01,  3.3029e-01,\n",
            "       -5.5988e-01, -5.3579e-01, -3.2242e-01, -4.0564e-01, -2.2286e-01,\n",
            "       -1.9120e-01,  4.5336e-02, -6.8645e-02, -1.2736e-01, -5.5131e-01,\n",
            "       -1.0752e-01,  3.7109e-01, -4.2796e-01, -4.3169e-01,  2.1273e-01,\n",
            "        2.7399e-01,  3.6443e-01,  2.7316e-01, -3.6326e-01, -5.1721e-01,\n",
            "       -3.0991e-01,  6.5097e-02,  3.7322e-01,  3.7718e-01, -2.0251e-01,\n",
            "        1.9921e-01,  2.0002e-01, -1.1705e-01,  2.7642e-01, -2.1288e-01,\n",
            "       -4.3874e-01,  8.9985e-02, -1.2721e-01, -3.4331e-01,  2.6683e-02,\n",
            "       -2.7844e-01,  9.7163e-02,  3.2342e-03, -1.3145e-01,  3.0249e-01,\n",
            "        3.9341e-01,  9.7989e-03,  4.0534e-01,  8.9311e-02,  8.0410e-02,\n",
            "       -9.3015e-03,  3.6787e-02, -3.5419e-01,  1.5470e-01,  1.3887e-01,\n",
            "       -7.2565e-02,  1.6451e-01, -5.7462e-02,  1.8330e-01,  1.9607e-01,\n",
            "       -6.1845e-01, -2.4751e+00, -4.8125e-02, -6.1398e-01,  1.2818e-02,\n",
            "        3.0068e-01, -2.5459e-01,  4.8210e-01, -9.6294e-02,  4.5591e-02,\n",
            "        6.4748e-01, -2.1675e-02, -1.3162e-01,  7.7756e-02, -2.3301e-01,\n",
            "       -1.1886e-02,  1.3478e-02, -2.7886e-01, -2.9127e-01, -5.5737e-01,\n",
            "        1.3096e-01, -8.2932e-02,  3.8906e-01, -3.2719e-01, -1.2949e-03,\n",
            "        1.0865e-01,  1.2926e-01, -4.4963e-01, -2.4885e-01,  3.7913e-01,\n",
            "        5.7438e-01, -7.8006e-02, -3.4256e-01,  3.1390e-01,  3.8343e-01,\n",
            "       -1.2884e-01, -5.9250e-02,  1.5599e-01,  3.0918e-02,  4.2916e-02,\n",
            "        6.8767e-02, -5.0873e-01, -3.0208e-01,  6.5575e-01,  3.6114e-01,\n",
            "        2.4731e-01,  3.3684e-01, -2.8469e-01, -1.9312e-01,  1.8695e-01,\n",
            "        3.7503e-01, -8.9372e-02, -3.3262e-01,  1.9903e-01,  2.5514e-01,\n",
            "       -6.2217e-01, -5.3728e-01, -2.6064e-01,  1.5485e-01,  5.9014e-01,\n",
            "       -3.9670e-01, -1.7034e-01,  6.5501e-02, -5.8078e-02, -3.8009e-02,\n",
            "       -3.0155e-01, -1.9550e-01,  3.3969e-01,  2.2201e-01, -1.2113e-01,\n",
            "       -4.8972e-01,  3.0480e-01, -2.0492e-01, -9.3720e-02,  4.7798e-01,\n",
            "       -1.0729e-01, -1.5250e-01, -5.6053e-02, -8.3590e-02, -1.2159e-01,\n",
            "        5.4780e-01,  3.1624e-01, -9.4054e-03,  4.7032e-01, -1.6736e-01,\n",
            "       -3.1995e-01,  1.8265e-01, -1.2148e-02,  1.6604e-01, -3.5355e-02,\n",
            "       -1.9633e-01, -2.1442e-01,  3.1613e-01,  4.1087e-01, -8.7041e-02,\n",
            "       -5.2293e-01,  7.5817e-02, -2.3894e-01, -3.2395e-02,  6.4267e-01,\n",
            "        2.1762e-01,  7.8675e-02,  1.2452e-02, -1.0566e-01, -2.1925e-01,\n",
            "        7.4882e-01, -1.2247e-01,  2.7263e-01,  4.1377e-01,  6.3094e-01,\n",
            "        1.9733e-02,  1.1587e-01, -1.0669e-02, -3.7927e-01, -4.3893e-01,\n",
            "       -3.6705e-01, -8.8085e-03, -4.5613e-01,  1.3245e-01,  5.1154e-02,\n",
            "       -6.1534e-02, -8.5659e-02, -2.7121e-01,  7.9104e-03,  7.6351e-03,\n",
            "       -4.4850e-01, -1.7339e-01, -1.7088e-01,  2.9315e-03, -3.0827e+00,\n",
            "        5.1903e-01, -1.1224e-01, -1.4076e-01, -3.6153e-01,  4.5040e-01,\n",
            "        3.5855e-01,  7.2232e-02, -2.4850e-02,  1.2117e-01,  3.6595e-02,\n",
            "        2.3978e-02, -3.0444e-01, -3.9547e-01, -1.9124e-01, -1.2183e-01,\n",
            "        3.2135e-01, -1.8978e-01,  2.1472e-01, -2.5739e-01, -2.2170e-01,\n",
            "       -3.7528e-01, -3.4948e-01, -1.8052e-01, -4.0326e-02,  4.5781e-02,\n",
            "        3.0279e-02,  3.1536e-01, -3.5099e-03, -2.0707e-01, -3.2708e-01,\n",
            "       -9.9817e-02,  2.4305e-01, -2.1873e-01,  2.1107e-01, -4.4238e-01,\n",
            "        2.2811e-02,  7.1617e-02, -2.2452e-01, -5.1383e-02,  2.1359e-01,\n",
            "       -1.4065e-01, -3.4828e-01, -4.3306e-02,  4.9766e-03, -2.2764e-01,\n",
            "       -1.3563e-02,  7.0866e-02,  9.8086e-02,  3.7456e-01, -4.7928e-01,\n",
            "        2.7906e-01,  3.7424e-01, -4.1395e-03,  5.9956e-01,  4.8190e-03,\n",
            "       -4.4240e-01, -5.1407e-01, -2.0717e-02,  3.5043e-01, -3.3511e-01,\n",
            "        1.0807e-01, -1.6955e-02, -1.3094e-01,  2.8129e-01,  1.0671e-01,\n",
            "       -3.6386e-01,  6.7377e-02, -2.8381e-02, -2.8158e-01, -4.1747e-01,\n",
            "       -6.0115e-01, -4.3728e-01, -1.3097e-02,  3.6018e-03, -2.6590e-01],\n",
            "      dtype=float32), array([ 0.15369  , -1.252    , -0.061935 ,  0.092786 , -0.14039  ,\n",
            "       -0.39466  , -0.36428  , -0.40078  , -0.32517  ,  0.98788  ,\n",
            "        0.11349  , -0.12214  , -1.1416   , -0.24116  , -0.048714 ,\n",
            "        0.28006  , -0.19638  , -0.49473  , -0.57949  , -0.22053  ,\n",
            "       -0.20223  , -0.022866 , -0.12602  , -0.37986  , -0.18401  ,\n",
            "        0.27956  , -0.45232  ,  0.53367  , -0.11285  , -0.17206  ,\n",
            "        0.14975  , -0.11231  , -0.27795  , -0.65433  , -0.29249  ,\n",
            "       -0.55379  ,  0.46489  , -0.0085809,  0.22199  , -0.56101  ,\n",
            "        0.31594  , -0.2768   , -0.14398  ,  0.22919  , -0.81539  ,\n",
            "       -0.56556  , -0.027108 ,  0.44677  ,  0.068677 , -0.17461  ,\n",
            "       -0.079729 ,  0.41997  ,  0.73137  , -0.24823  ,  0.36923  ,\n",
            "       -0.17348  ,  0.95051  , -0.51063  , -0.32833  ,  0.096826 ,\n",
            "        0.40623  , -0.36449  , -0.037021 ,  0.029754 ,  0.29153  ,\n",
            "       -0.027024 ,  0.19517  ,  0.50622  ,  0.23362  , -0.10005  ,\n",
            "       -0.01727  , -0.49618  , -0.076671 , -0.1557   , -0.43232  ,\n",
            "       -0.059413 ,  0.34159  , -0.098985 ,  0.076105 , -0.014301 ,\n",
            "       -0.30555  , -0.73386  ,  0.57164  ,  0.15982  , -0.31129  ,\n",
            "       -0.54523  , -0.048936 , -0.16213  , -0.22244  , -0.20425  ,\n",
            "       -0.035734 , -0.028922 , -0.17802  ,  0.44027  , -0.22607  ,\n",
            "       -0.60345  , -0.81584  ,  0.74092  , -0.30162  , -0.047132 ,\n",
            "       -0.089729 ,  0.87992  ,  0.68079  , -0.27685  ,  0.096305 ,\n",
            "       -0.41649  ,  0.25988  , -0.35042  , -0.066851 ,  0.61552  ,\n",
            "       -0.30037  ,  0.048989 ,  0.13408  , -0.14842  , -0.12001  ,\n",
            "        0.20648  ,  0.37306  , -0.22833  , -0.21462  , -0.35409  ,\n",
            "       -0.21004  ,  0.13141  ,  0.16796  , -0.30828  ,  0.41264  ,\n",
            "        0.081055 ,  0.44497  , -0.56248  , -0.056457 ,  0.22486  ,\n",
            "        0.65719  ,  0.17119  , -0.26113  ,  0.58282  ,  0.50564  ,\n",
            "       -0.36124  , -0.26362  ,  0.40957  ,  0.41655  , -0.27584  ,\n",
            "       -0.42288  ,  0.47787  , -0.16813  ,  0.055164 , -0.11437  ,\n",
            "       -0.26074  ,  0.33211  , -0.081297 ,  0.64227  ,  0.73619  ,\n",
            "        0.46203  , -0.56342  ,  0.66303  , -0.069254 , -0.31267  ,\n",
            "       -0.63525  ,  0.024352 ,  0.35421  ,  0.80939  ,  0.033397 ,\n",
            "        0.017622 ,  0.54435  ,  0.24515  ,  0.082529 , -0.35166  ,\n",
            "        0.0099891, -0.19086  ,  0.37312  ,  0.85756  , -0.066389 ,\n",
            "       -0.19534  ,  0.35968  , -0.20093  , -0.018071 ,  0.27769  ,\n",
            "       -0.093138 ,  0.30268  , -0.27971  ,  0.4233   ,  0.57488  ,\n",
            "        0.052964 , -0.15846  ,  0.60251  , -0.25039  ,  0.25582  ,\n",
            "       -0.17826  ,  0.74892  ,  0.19681  , -0.11955  ,  0.062155 ,\n",
            "        0.40019  , -0.50811  ,  0.39912  , -0.35896  , -0.4281   ,\n",
            "       -0.13368  , -0.15014  , -0.16258  , -0.13292  ,  0.45311  ,\n",
            "       -0.51464  , -0.6384   ,  0.72755  ,  0.16326  , -0.28369  ,\n",
            "        0.33985  , -0.14603  ,  0.83533  ,  0.15476  , -0.11849  ,\n",
            "        0.15099  ,  0.53985  , -0.05652  , -0.40838  ,  0.55724  ,\n",
            "        0.7098   , -0.43036  , -0.025888 , -1.0487   , -0.28949  ,\n",
            "        0.50929  ,  0.23862  ,  0.3041   , -0.14947  , -1.8452   ,\n",
            "        0.13302  ,  0.34824  ,  0.29863  , -0.23815  ,  0.87509  ,\n",
            "        0.55286  , -0.11963  ,  0.01827  , -0.10686  , -0.71336  ,\n",
            "        0.21458  ,  0.36892  , -0.52647  , -0.59597  ,  0.77178  ,\n",
            "       -0.38676  , -0.49515  ,  0.46797  , -0.39005  , -0.53673  ,\n",
            "       -0.014198 ,  0.18004  , -0.79493  ,  0.28217  ,  0.39056  ,\n",
            "        0.88515  , -0.13037  , -0.065269 ,  0.16838  , -0.29978  ,\n",
            "       -0.060295 ,  0.19807  , -0.38538  ,  0.19339  , -0.35772  ,\n",
            "        0.70971  , -0.25969  ,  0.40635  ,  0.65121  , -0.17576  ,\n",
            "       -0.11405  , -0.99658  ,  0.19852  ,  0.034637 , -0.065138 ,\n",
            "        0.094399 , -0.41505  ,  0.10963  ,  0.44741  , -0.24247  ,\n",
            "        0.18524  , -0.40052  , -0.59553  , -0.076593 ,  0.84353  ,\n",
            "        0.087194 , -0.03908  , -0.30729  , -0.48837  , -0.28474  ,\n",
            "       -0.26033  , -0.24738  , -0.55169  , -0.22014  ,  0.38215  ,\n",
            "       -0.29562  ,  0.42191  , -0.57612  , -0.12329  , -0.59072  ,\n",
            "        0.35733  ,  0.67999  , -0.17597  ,  0.34512  ,  0.33647  ],\n",
            "      dtype=float32), array([ 0.20304  , -0.18631  ,  0.00407  ,  0.32736  ,  0.20132  ,\n",
            "        0.13166  , -2.3465   ,  0.70814  ,  0.027983 ,  0.4855   ,\n",
            "       -0.14653  , -0.038625 , -0.17222  , -0.36289  ,  0.18249  ,\n",
            "       -0.090779 ,  0.023871 ,  0.31818  ,  0.13185  ,  0.10582  ,\n",
            "        0.50934  , -0.37038  ,  0.22091  , -0.15445  , -0.056447 ,\n",
            "       -0.2298   ,  0.13836  ,  0.39534  , -0.36911  ,  0.056492 ,\n",
            "        0.44537  ,  0.16045  , -0.17846  , -0.41532  ,  0.095721 ,\n",
            "       -0.22805  ,  0.039214 , -0.45847  , -0.11306  , -0.092831 ,\n",
            "       -0.23362  , -0.21505  ,  0.52386  , -0.03573  ,  0.17427  ,\n",
            "       -0.22172  ,  0.28233  ,  0.23735  , -0.64605  , -0.13553  ,\n",
            "        0.090202 ,  0.25722  ,  0.074412 ,  0.63425  , -0.21035  ,\n",
            "       -0.42974  , -0.75314  ,  0.26438  , -0.0098307,  0.0932   ,\n",
            "       -0.27493  ,  0.25679  ,  0.28418  , -0.19306  ,  0.20561  ,\n",
            "        0.30987  , -0.18451  , -0.42455  ,  0.073287 , -0.4727   ,\n",
            "       -0.17102  ,  0.36718  , -0.085459 , -0.022028 , -0.4992   ,\n",
            "       -0.064444 ,  0.077481 ,  0.37288  ,  0.5317   , -0.031048 ,\n",
            "       -0.079653 , -0.4462   ,  0.11175  , -0.35885  ,  0.30992  ,\n",
            "        0.25342  ,  0.32255  , -0.12332  , -0.05994  , -0.035515 ,\n",
            "        0.32545  ,  0.079643 ,  0.38412  ,  0.78785  , -1.0571   ,\n",
            "        0.82408  , -1.902    ,  0.15403  ,  0.010096 ,  0.78337  ,\n",
            "       -0.27854  ,  0.22852  , -0.24448  , -0.1698   , -0.13924  ,\n",
            "       -0.55455  , -0.12622  ,  0.53987  ,  0.13616  ,  0.52667  ,\n",
            "       -0.27183  , -0.45177  ,  0.22384  , -0.28377  ,  0.028054 ,\n",
            "        0.16652  , -1.0404   ,  0.23223  ,  0.0061396, -0.045941 ,\n",
            "        0.025211 ,  0.12552  ,  0.85724  ,  0.13468  ,  0.2366   ,\n",
            "        0.42123  ,  0.18938  ,  0.646    , -0.68191  ,  0.24724  ,\n",
            "        0.63364  , -0.60162  , -0.37675  , -0.21341  ,  0.055077 ,\n",
            "       -0.046723 , -0.051917 ,  0.62319  ,  0.30858  ,  0.55972  ,\n",
            "        0.3602   ,  0.56014  , -0.0821   ,  1.0799   , -0.52457  ,\n",
            "        0.0099559, -0.20709  , -0.2334   ,  0.21305  ,  0.17501  ,\n",
            "       -0.2755   , -0.30857  , -0.3218   ,  0.1559   ,  0.052795 ,\n",
            "        0.086519 , -0.059779 , -0.2302   ,  0.25845  ,  0.0763   ,\n",
            "        0.037902 , -0.26395  ,  0.42275  ,  0.06805  ,  0.3133   ,\n",
            "        0.0404   , -0.54441  , -0.032373 , -0.13362  , -0.87926  ,\n",
            "       -0.015322 , -0.11136  ,  0.48232  , -0.31465  , -0.36195  ,\n",
            "        0.20171  , -0.20458  ,  0.72972  ,  0.010045 ,  0.0633   ,\n",
            "        0.49793  , -0.049445 ,  0.53911  , -0.1756   , -0.33898  ,\n",
            "       -0.45285  , -0.30911  ,  0.67172  , -0.14019  , -0.0084288,\n",
            "       -0.084543 , -0.2247   , -0.33826  ,  0.014024 , -0.39758  ,\n",
            "       -0.14462  , -0.37884  , -0.057329 , -0.31044  , -0.10675  ,\n",
            "        0.77149  ,  0.43485  ,  0.16108  , -0.32647  ,  0.036869 ,\n",
            "       -0.039442 ,  0.072498 , -0.51928  ,  0.085982 , -0.17809  ,\n",
            "        0.64768  , -0.70119  , -0.56878  , -0.53389  , -0.29016  ,\n",
            "        0.41191  , -0.21424  ,  0.55267  , -0.27403  , -0.31798  ,\n",
            "       -0.65128  , -0.17127  , -0.35338  , -0.27642  , -2.6144   ,\n",
            "       -0.237    ,  0.1835   , -0.2598   , -0.34767  ,  0.42867  ,\n",
            "       -0.56276  , -0.014253 , -0.1412   ,  0.098409 ,  0.2991   ,\n",
            "       -0.21406  , -0.41458  ,  0.42974  , -0.20985  ,  0.28292  ,\n",
            "        0.12763  ,  0.1944   ,  0.028441 ,  0.41641  , -0.37006  ,\n",
            "       -0.62729  , -0.12759  ,  0.37932  , -0.1032   ,  0.15305  ,\n",
            "       -0.012559 , -0.038818 ,  0.11548  , -0.34828  ,  0.24553  ,\n",
            "        0.19459  , -0.041291 ,  0.41252  ,  0.030331 , -0.81661  ,\n",
            "       -0.27705  , -0.023661 ,  0.72046  ,  0.21799  ,  0.38713  ,\n",
            "       -0.38547  , -0.30049  , -0.010166 ,  0.22605  , -0.09332  ,\n",
            "        0.39895  ,  0.45547  , -0.34048  , -0.016418 , -0.22542  ,\n",
            "        0.075017 ,  0.11923  , -0.69383  , -0.064248 ,  0.14825  ,\n",
            "       -0.32272  , -0.60902  , -0.32828  ,  0.34369  , -0.83633  ,\n",
            "       -0.046365 ,  0.22469  , -0.23675  ,  0.16287  , -0.19977  ,\n",
            "        0.26554  ,  0.37637  ,  0.43433  , -0.23304  , -0.076825 ,\n",
            "       -0.012174 ,  0.34471  ,  0.14235  , -0.47696  , -0.3786   ],\n",
            "      dtype=float32), array([-8.0353e-02, -2.4218e-01, -2.6267e-01, -3.3483e-01,  2.7804e-01,\n",
            "       -1.4549e-01, -2.7027e+00, -7.5670e-01,  2.8336e-01, -7.2944e-01,\n",
            "        2.1162e-01, -5.2950e-01,  3.1413e-01, -6.3769e-02, -1.4010e-01,\n",
            "       -1.5178e-01, -4.4237e-01, -4.8963e-01, -1.0900e-01,  2.0123e-01,\n",
            "        5.6939e-01, -1.3128e-01,  3.7606e-01,  2.9006e-01,  1.0225e-01,\n",
            "        7.1932e-02,  1.5706e-01,  3.3154e-01, -1.2658e-02,  7.3517e-01,\n",
            "        3.0506e-01,  1.6430e-01, -2.1664e-02,  2.9159e-01, -1.0311e-01,\n",
            "       -4.6601e-01,  1.0972e-01, -2.6945e-01,  3.7584e-01,  1.2056e-01,\n",
            "       -8.7698e-02, -5.1568e-02,  2.0632e-01, -2.7614e-02,  4.0971e-01,\n",
            "        1.4663e-01,  1.5049e-01, -4.4549e-01, -1.7702e-01,  1.1271e-01,\n",
            "        2.9409e-01,  7.5109e-02, -7.2456e-02, -1.7999e-01, -3.4524e-02,\n",
            "       -2.1329e-01, -4.4146e-01, -6.7343e-01,  6.3362e-01,  3.1155e-01,\n",
            "        1.0644e-01, -7.4109e-03,  2.6819e-01,  1.6563e-01, -2.4149e-01,\n",
            "       -3.6626e-01, -2.4001e-01, -2.1940e-01, -1.0674e-01, -5.2586e-01,\n",
            "        1.0235e-01, -3.6375e-01, -6.0832e-01,  3.4424e-02,  3.4668e-01,\n",
            "       -2.9090e-01,  2.1580e-01, -1.6574e-01,  2.4405e-01,  1.5225e-01,\n",
            "       -7.8449e-02, -1.6932e-01, -3.0232e-01,  1.8136e-02,  3.6239e-02,\n",
            "       -3.6096e-01, -7.1245e-03,  6.1051e-01, -9.8656e-02,  2.1805e-01,\n",
            "       -3.5874e-01,  4.2824e-02, -5.3259e-03,  3.3951e-01,  6.7011e-02,\n",
            "        1.9102e-02, -1.4930e+00, -3.2886e-01, -1.9263e-01,  1.5488e-01,\n",
            "       -1.3394e-01, -7.6664e-02,  2.1006e-03,  2.6518e-02,  9.2485e-02,\n",
            "        1.6418e-01, -8.5920e-02, -8.3575e-02,  2.9236e-02,  3.8007e-01,\n",
            "       -6.6097e-01,  8.0127e-02, -5.8193e-01,  1.5692e-01, -1.9185e-01,\n",
            "        7.7291e-01,  3.8412e-01,  9.8805e-02,  4.0793e-01,  9.2013e-02,\n",
            "        3.0912e-01,  3.4102e-02,  2.5688e-01,  7.0914e-01,  4.1107e-02,\n",
            "       -2.7394e-01,  2.9700e-01,  7.4942e-02, -6.6909e-03, -2.8219e-01,\n",
            "       -8.3884e-01, -1.0635e-01, -1.5715e-01, -2.9373e-01, -3.9934e-01,\n",
            "       -4.5894e-01,  4.2851e-02, -4.3856e-01,  3.2075e-01,  1.2214e+00,\n",
            "        1.2748e-01,  4.5359e-01,  7.4415e-02, -1.1432e-01, -5.2070e-01,\n",
            "       -2.4849e-01, -1.2202e-02,  2.6667e-01,  2.2034e-01, -5.9404e-01,\n",
            "        3.0770e-01,  1.4772e-01, -9.5911e-03,  2.2293e-01, -2.0404e-02,\n",
            "        1.0196e-01, -1.7569e-01,  1.1085e-01,  3.5609e-01,  1.3383e-01,\n",
            "       -3.3266e-02,  4.9192e-02,  4.1619e-01,  9.3482e-02,  2.1756e-01,\n",
            "        1.5825e-01, -2.4637e-01,  2.0161e-02,  2.2447e-01, -3.7260e-03,\n",
            "        3.3209e-01, -1.0354e-01,  2.6530e-01, -4.9483e-01,  1.7366e-01,\n",
            "       -2.2545e-01, -4.1306e-01, -3.7415e-01, -2.8561e-01, -4.9512e-01,\n",
            "        3.3037e-01,  2.9596e-01, -3.4866e-02,  1.6449e-01,  7.6361e-01,\n",
            "        9.4113e-02, -3.8189e-01, -3.2743e-01,  5.5906e-01,  2.1791e-01,\n",
            "        1.6357e-01, -3.9378e-01, -2.3332e-01, -1.2636e-01,  3.6002e-03,\n",
            "        6.9298e-01, -1.7612e-01, -3.1942e-02,  3.6175e-01,  1.7243e-01,\n",
            "       -3.8340e-01,  7.5150e-02,  9.8891e-02, -1.5738e-01, -9.1796e-01,\n",
            "        2.7077e-01, -2.8348e-01,  3.9699e-02,  6.5473e-01, -1.2977e-01,\n",
            "        3.3467e-02,  1.5329e-01, -5.0088e-04,  7.6293e-02,  1.0052e-01,\n",
            "        2.2982e-01, -1.7936e-01,  2.2203e-01, -3.6554e-01, -2.2515e-01,\n",
            "        2.1761e-01, -1.5113e-01, -3.4432e-01, -2.2501e-01, -2.6714e+00,\n",
            "       -5.6177e-01,  3.0785e-01, -3.1723e-02, -2.9997e-02,  2.1934e-01,\n",
            "       -1.4705e-01, -2.7103e-01, -3.9438e-01, -1.3505e-01,  9.8464e-02,\n",
            "        8.6147e-02, -4.2158e-01, -1.5188e-01, -1.7155e-01,  2.5262e-01,\n",
            "        4.8956e-01, -6.9486e-02, -1.3666e-01,  4.1522e-01, -5.7094e-01,\n",
            "        1.3250e-01, -3.2945e-01,  4.3328e-01, -7.7996e-02,  2.0719e-01,\n",
            "        1.2065e-01,  2.2492e-01,  2.6550e-01, -1.7528e-01,  5.0774e-01,\n",
            "       -4.3200e-03,  3.2793e-01, -3.3950e-02, -3.0817e-01, -6.3743e-01,\n",
            "       -2.3544e-02,  1.5409e-01,  9.2704e-02, -1.5318e-01,  2.1432e-01,\n",
            "       -1.2690e-01, -3.5146e-01, -1.3863e-02, -8.9824e-03,  2.3826e-03,\n",
            "       -4.0972e-01,  4.3164e-01, -1.9246e-01,  3.2302e-01, -4.5888e-01,\n",
            "       -3.0356e-01,  8.6874e-02,  6.0975e-02, -2.2511e-01,  2.2198e-01,\n",
            "        5.1727e-01, -4.8545e-02, -1.7231e-01,  3.1632e-01,  5.5598e-01,\n",
            "        2.8723e-01,  2.3520e-01,  1.2031e-01, -4.0517e-01, -2.7359e-02,\n",
            "       -1.7211e-01, -2.1797e-01, -3.8644e-01, -1.2192e-01,  3.1853e-01,\n",
            "       -5.4094e-02, -3.2662e-01, -3.8698e-03, -2.8152e-01,  1.7290e-01],\n",
            "      dtype=float32)]\n"
          ]
        }
      ],
      "source": [
        "#Try preprocessing and vectorization of asample text data using 'CleanText_and_Vectorize' function\n",
        "text = 'Definitely going to choose Prof. Looney\\'s class again! Interesting class and easy A. You can bring notes to exams so you don\\'t need to remember a lot. Lots of bonus points available and the observatory sessions are awesome!'\n",
        "print(CleanText_and_Vectorize(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h9DFSaZi1re6"
      },
      "outputs": [],
      "source": [
        "# preprocess and vectorize text data in the  comments column of the corpus 'RateMyProfessor_Sample data.csv'\n",
        "df['word_embeddings'] = df['comments'].apply(lambda text: CleanText_and_Vectorize(text))\n",
        "\n",
        "# save the modified dataframe as pickle file.\n",
        "#use pickle file format to save the daaframe to make sure to retrive the information about the datatypes of the data\n",
        "df.to_pickle('RateMyProfessor_WordEmbeddings.pkl')\n",
        "\n",
        "#load the previously saved pickle file as df\n",
        "df = pd.read_pickle('/content/RateMyProfessor_WordEmbeddings.pkl')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These lines of code preprocess and vectorize the text data in the comments column of the \"***RateMyProfessor_Sample data.csv***\" corpus by applying the CleanText_and_Vectorize function to each comment and storing the result in a new column called \"***word_embeddings***\". The modified dataframe is then saved as a pickle file and later loaded back into the notebook using the read_pickle method.\n",
        "\n",
        "***Note***: if you already have '***RateMyProfessor_WordEmbeddings.pkl'***, you can skip running the first two lines of codes and run the only the last line to load the file, and that definetly serves the purpose."
      ],
      "metadata": {
        "id": "v0FYOIAz_SQ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(1)"
      ],
      "metadata": {
        "id": "JoPAGdzK64FT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "outputId": "1c84275a-eb03-4284-94a2-906948d5508c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   student_star                                           comments  \\\n",
              "0           5.0  This class is hard, but its a two-in-one gen-e...   \n",
              "\n",
              "   student_difficult                                    word_embeddings  \n",
              "0                3.0  [[0.80308, -0.016776, 0.025788, -0.18749, 0.39...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-28f533d4-8e85-4e45-9b32-43509105ee3c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>student_star</th>\n",
              "      <th>comments</th>\n",
              "      <th>student_difficult</th>\n",
              "      <th>word_embeddings</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.0</td>\n",
              "      <td>This class is hard, but its a two-in-one gen-e...</td>\n",
              "      <td>3.0</td>\n",
              "      <td>[[0.80308, -0.016776, 0.025788, -0.18749, 0.39...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-28f533d4-8e85-4e45-9b32-43509105ee3c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-28f533d4-8e85-4e45-9b32-43509105ee3c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-28f533d4-8e85-4e45-9b32-43509105ee3c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.tail(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "x2Q3dvuFSZEi",
        "outputId": "fb3e3110-fbf6-4d8c-e5fd-91d1bb0ba7a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       student_star                                           comments  \\\n",
              "19992           5.0  Extremely easy lab teacher, quizzes are a litt...   \n",
              "\n",
              "       student_difficult                                    word_embeddings  \n",
              "19992                2.0  [[0.078935, -0.11888, -0.29625, -0.46044, 0.02...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9922a5e0-b471-4415-afdb-02597755adb0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>student_star</th>\n",
              "      <th>comments</th>\n",
              "      <th>student_difficult</th>\n",
              "      <th>word_embeddings</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>19992</th>\n",
              "      <td>5.0</td>\n",
              "      <td>Extremely easy lab teacher, quizzes are a litt...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>[[0.078935, -0.11888, -0.29625, -0.46044, 0.02...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9922a5e0-b471-4415-afdb-02597755adb0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9922a5e0-b471-4415-afdb-02597755adb0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9922a5e0-b471-4415-afdb-02597755adb0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get the word_embeddings of first comment in the data frame 'df'\n",
        "np.array(df['word_embeddings'][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TXL7pg-Au2TA",
        "outputId": "193cc432-2811-4e12-df08-11b4caeff3b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.80308  , -0.016776 ,  0.025788 , ...,  0.042583 , -0.39625  ,\n",
              "        -0.040408 ],\n",
              "       [-0.38065  ,  0.33245  ,  0.053904 , ...,  0.086626 ,  0.33142  ,\n",
              "         0.2014   ],\n",
              "       [-0.01004  , -0.41808  , -0.061757 , ..., -0.0073133,  0.12578  ,\n",
              "        -0.48272  ],\n",
              "       ...,\n",
              "       [ 0.11384  ,  0.42036  ,  0.23951  , ...,  0.055397 , -0.29187  ,\n",
              "        -0.35491  ],\n",
              "       [-0.40781  ,  0.1567   , -0.28539  , ...,  0.11763  , -0.0026115,\n",
              "         0.32068  ],\n",
              "       [-0.30338  ,  0.36641  ,  0.24148  , ..., -0.32577  , -0.011849 ,\n",
              "         0.17795  ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get the shape of the word_embeddings of first comment in the data frame 'df'\n",
        "np.array(df['word_embeddings'][0]).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lI_JD83vD8WV",
        "outputId": "65eead0e-ca18-46f4-9fe5-fd281cad0ba3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(17, 300)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the total number of comments in the dataframe\n",
        "total_comments= len(df['comments'])\n",
        "\n",
        "# Get the maximum length of word embeddings in the comments using list comprehension\n",
        "max_length = max([len(df.word_embeddings[i]) for i in range(total_comments) if i in df.index])\n",
        "\n",
        "# Pad the sequences of word embeddings to make them of equal length for all comments, using the Keras function 'pad_sequences'\n",
        "padded_embeddings = pad_sequences(df['word_embeddings'], maxlen=max_length, dtype='float32', padding='post', truncating='post')"
      ],
      "metadata": {
        "id": "ssm_9znKUl6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code calculates the maximum length of word embeddings in the dataframe and pads sequences of embeddings with zeros. This ensures that all sequences have the same length, which is necessary for training a neural network."
      ],
      "metadata": {
        "id": "j9UjNkW1A6s7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get max_length\n",
        "max_length"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-C8RKFiCFJVb",
        "outputId": "8c151c8f-0529-4f52-e267-69607e577118"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "46"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the 2D numpy array of padded embeddings to a pandas series for easier handling\n",
        "padded_embeddings_df = pd.Series([np.array(padded_embeddings[i]) for i in range(len(padded_embeddings))])"
      ],
      "metadata": {
        "id": "KDgcVaQE5wQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we create a pandas series where each element is an array of padded embeddings for the corresponding comment"
      ],
      "metadata": {
        "id": "M_c7ODNLBm7m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace the 'word_embeddings' column with 'padded_embeddings_df'\n",
        "df['word_embeddings'] = padded_embeddings_df"
      ],
      "metadata": {
        "id": "g-1MNEbW3Z_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This line of code assigns the padded word embeddings to the 'word_embeddings' column of the dataframe 'df', replacing the previously existing word embeddings."
      ],
      "metadata": {
        "id": "NDxr672tCABq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get the padded word_embeddings of first comment in the data frame 'df'\n",
        "df['word_embeddings'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tP_ea-t85P4y",
        "outputId": "dd1729cf-478e-472a-92cd-45e27124b22d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.80308  , -0.016776 ,  0.025788 , ...,  0.042583 , -0.39625  ,\n",
              "        -0.040408 ],\n",
              "       [-0.38065  ,  0.33245  ,  0.053904 , ...,  0.086626 ,  0.33142  ,\n",
              "         0.2014   ],\n",
              "       [-0.01004  , -0.41808  , -0.061757 , ..., -0.0073133,  0.12578  ,\n",
              "        -0.48272  ],\n",
              "       ...,\n",
              "       [ 0.       ,  0.       ,  0.       , ...,  0.       ,  0.       ,\n",
              "         0.       ],\n",
              "       [ 0.       ,  0.       ,  0.       , ...,  0.       ,  0.       ,\n",
              "         0.       ],\n",
              "       [ 0.       ,  0.       ,  0.       , ...,  0.       ,  0.       ,\n",
              "         0.       ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get the shape of the word_embeddings of first comment in the data frame 'df'\n",
        "df['word_embeddings'][0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSJYT9na0ZZ_",
        "outputId": "dda03081-c30b-4935-d331-3ad2dd756e15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(46, 300)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Input, LSTM, Dense, Dropout\n",
        "from keras.models import Model\n",
        "\n",
        "# Define input shape\n",
        "input_shape =df['word_embeddings'][0].shape\n",
        "\n",
        "# Define model architecture\n",
        "input_layer   =  Input(shape=input_shape)\n",
        "lstm_layer    =  LSTM(128)(input_layer)\n",
        "dense_layer   =  Dense(32, activation = 'relu')(lstm_layer)\n",
        "dropout_layer =  Dropout(0.2)(dense_layer)\n",
        "output_layer1 =  Dense(1, activation = 'relu', name = 'Quality')(dropout_layer)\n",
        "output_layer2 =  Dense(1, activation = 'relu', name = 'Difficulty')(dropout_layer)\n",
        "\n",
        "model = Model(inputs = input_layer, outputs = [output_layer1, output_layer2])\n",
        "\n"
      ],
      "metadata": {
        "id": "BsaagY_tWm97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code imports the necessary layers and model from the Keras library, defines the input shape for the model based on the shape of the word embeddings, sets up the architecture for a neural network model with an LSTM layer, a dense layer, a dropout layer, and two output layers, and then creates the model object using the input and output layers. The model has two output layers to predict the ***Quality*** and ***Difficulty*** of a professor based on the input word embeddings."
      ],
      "metadata": {
        "id": "QPiloXzfF8Xj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile model\n",
        "model.compile(loss={'Quality': 'mse', 'Difficulty': 'mse'},\n",
        "              optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "              metrics={'Quality': keras.metrics.RootMeanSquaredError(),\n",
        "                       'Difficulty': keras.metrics.RootMeanSquaredError()})\n"
      ],
      "metadata": {
        "id": "HjtTrdKeV9Rv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above code snippet compiles the previously defined model using the mean squared error (mse) loss function for both Quality and Difficulty outputs. It uses the Adam optimizer with a learning rate of 0.001 and sets the root mean squared error (RMSE) as the evaluation metric for both outputs."
      ],
      "metadata": {
        "id": "WZRgSUVNGZDU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print model summary\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77UpsZS9V-jk",
        "outputId": "1345f619-7506-481d-bff4-20837dadb516"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 46, 300)]    0           []                               \n",
            "                                                                                                  \n",
            " lstm (LSTM)                    (None, 128)          219648      ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 32)           4128        ['lstm[0][0]']                   \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 32)           0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " Quality (Dense)                (None, 1)            33          ['dropout[0][0]']                \n",
            "                                                                                                  \n",
            " Difficulty (Dense)             (None, 1)            33          ['dropout[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 223,842\n",
            "Trainable params: 223,842\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into train and test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# stack the embeddings to create a 3D array\n",
        "X = np.stack(df['word_embeddings'])\n",
        "\n",
        "# convert student rating for Quality to numpy array\n",
        "y_quality = np.array(df['student_star'])\n",
        "\n",
        "# convert student rating for Difficulty to numpy array\n",
        "y_difficulty = np.array(df['student_difficult'])\n",
        "\n",
        "# split the data into train and test sets with a test size of 20% and a random state of 42\n",
        "X_train, X_test, y_quality_train, y_quality_test, y_difficulty_train, y_difficulty_test = train_test_split(X, y_quality, y_difficulty, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "FvPyOzWJThig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we are splitting the data into training and testing sets using the ***train_test_split*** function from ***scikit-learn***. The features are stored in a NumPy array '***X***', while the target variables for ***Quality*** and ***Difficulty*** ratings are stored in separate NumPy arrays ***y_quality*** and ***y_difficulty***, respectively. The resulting variables ***X_train***, ***X_test***, ***y_quality_train***, ***y_quality_test***, ***y_difficulty_train***, and ***y_difficulty_test*** store the training and testing data"
      ],
      "metadata": {
        "id": "0YiExYPxGyB1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history = model.fit(X_train, {'Quality': y_quality_train, 'Difficulty': y_difficulty_train},epochs=20, batch_size=128,validation_split=0.2)\n",
        "\n",
        "# Evaluate the model on test data\n",
        "loss_error_matrix = model.evaluate(X_test, {'Quality': y_quality_test, 'Difficulty': y_difficulty_test})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHsCG558X8X4",
        "outputId": "944a0d6f-30a2-4e67-bfd5-70112edc7fcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "100/100 [==============================] - 12s 26ms/step - loss: 12.3765 - Quality_loss: 3.7796 - Difficulty_loss: 8.5969 - Quality_root_mean_squared_error: 1.9441 - Difficulty_root_mean_squared_error: 2.9320 - val_loss: 4.1993 - val_Quality_loss: 2.3418 - val_Difficulty_loss: 1.8576 - val_Quality_root_mean_squared_error: 1.5303 - val_Difficulty_root_mean_squared_error: 1.3629\n",
            "Epoch 2/20\n",
            "100/100 [==============================] - 2s 17ms/step - loss: 5.2428 - Quality_loss: 2.6674 - Difficulty_loss: 2.5754 - Quality_root_mean_squared_error: 1.6332 - Difficulty_root_mean_squared_error: 1.6048 - val_loss: 3.8031 - val_Quality_loss: 2.1009 - val_Difficulty_loss: 1.7022 - val_Quality_root_mean_squared_error: 1.4495 - val_Difficulty_root_mean_squared_error: 1.3047\n",
            "Epoch 3/20\n",
            "100/100 [==============================] - 1s 14ms/step - loss: 4.8831 - Quality_loss: 2.6233 - Difficulty_loss: 2.2597 - Quality_root_mean_squared_error: 1.6197 - Difficulty_root_mean_squared_error: 1.5032 - val_loss: 3.7899 - val_Quality_loss: 2.1019 - val_Difficulty_loss: 1.6880 - val_Quality_root_mean_squared_error: 1.4498 - val_Difficulty_root_mean_squared_error: 1.2992\n",
            "Epoch 4/20\n",
            "100/100 [==============================] - 1s 13ms/step - loss: 4.6666 - Quality_loss: 2.5648 - Difficulty_loss: 2.1018 - Quality_root_mean_squared_error: 1.6015 - Difficulty_root_mean_squared_error: 1.4498 - val_loss: 3.8218 - val_Quality_loss: 2.1297 - val_Difficulty_loss: 1.6921 - val_Quality_root_mean_squared_error: 1.4594 - val_Difficulty_root_mean_squared_error: 1.3008\n",
            "Epoch 5/20\n",
            "100/100 [==============================] - 3s 30ms/step - loss: 4.5687 - Quality_loss: 2.5354 - Difficulty_loss: 2.0333 - Quality_root_mean_squared_error: 1.5923 - Difficulty_root_mean_squared_error: 1.4259 - val_loss: 3.8492 - val_Quality_loss: 2.1452 - val_Difficulty_loss: 1.7040 - val_Quality_root_mean_squared_error: 1.4646 - val_Difficulty_root_mean_squared_error: 1.3054\n",
            "Epoch 6/20\n",
            "100/100 [==============================] - 2s 23ms/step - loss: 4.5292 - Quality_loss: 2.5342 - Difficulty_loss: 1.9950 - Quality_root_mean_squared_error: 1.5919 - Difficulty_root_mean_squared_error: 1.4124 - val_loss: 3.8156 - val_Quality_loss: 2.1142 - val_Difficulty_loss: 1.7014 - val_Quality_root_mean_squared_error: 1.4540 - val_Difficulty_root_mean_squared_error: 1.3044\n",
            "Epoch 7/20\n",
            "100/100 [==============================] - 2s 17ms/step - loss: 4.4782 - Quality_loss: 2.5109 - Difficulty_loss: 1.9673 - Quality_root_mean_squared_error: 1.5846 - Difficulty_root_mean_squared_error: 1.4026 - val_loss: 3.8111 - val_Quality_loss: 2.1194 - val_Difficulty_loss: 1.6916 - val_Quality_root_mean_squared_error: 1.4558 - val_Difficulty_root_mean_squared_error: 1.3006\n",
            "Epoch 8/20\n",
            "100/100 [==============================] - 1s 14ms/step - loss: 4.4504 - Quality_loss: 2.5070 - Difficulty_loss: 1.9434 - Quality_root_mean_squared_error: 1.5834 - Difficulty_root_mean_squared_error: 1.3940 - val_loss: 3.7050 - val_Quality_loss: 2.0185 - val_Difficulty_loss: 1.6866 - val_Quality_root_mean_squared_error: 1.4207 - val_Difficulty_root_mean_squared_error: 1.2987\n",
            "Epoch 9/20\n",
            "100/100 [==============================] - 1s 13ms/step - loss: 3.8051 - Quality_loss: 1.9483 - Difficulty_loss: 1.8568 - Quality_root_mean_squared_error: 1.3958 - Difficulty_root_mean_squared_error: 1.3626 - val_loss: 2.8930 - val_Quality_loss: 1.3251 - val_Difficulty_loss: 1.5678 - val_Quality_root_mean_squared_error: 1.1511 - val_Difficulty_root_mean_squared_error: 1.2521\n",
            "Epoch 10/20\n",
            "100/100 [==============================] - 1s 14ms/step - loss: 3.4896 - Quality_loss: 1.7019 - Difficulty_loss: 1.7876 - Quality_root_mean_squared_error: 1.3046 - Difficulty_root_mean_squared_error: 1.3370 - val_loss: 2.7187 - val_Quality_loss: 1.2142 - val_Difficulty_loss: 1.5046 - val_Quality_root_mean_squared_error: 1.1019 - val_Difficulty_root_mean_squared_error: 1.2266\n",
            "Epoch 11/20\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 3.3657 - Quality_loss: 1.6185 - Difficulty_loss: 1.7472 - Quality_root_mean_squared_error: 1.2722 - Difficulty_root_mean_squared_error: 1.3218 - val_loss: 2.7187 - val_Quality_loss: 1.2014 - val_Difficulty_loss: 1.5172 - val_Quality_root_mean_squared_error: 1.0961 - val_Difficulty_root_mean_squared_error: 1.2318\n",
            "Epoch 12/20\n",
            "100/100 [==============================] - 1s 14ms/step - loss: 3.2998 - Quality_loss: 1.5553 - Difficulty_loss: 1.7445 - Quality_root_mean_squared_error: 1.2471 - Difficulty_root_mean_squared_error: 1.3208 - val_loss: 2.6707 - val_Quality_loss: 1.1881 - val_Difficulty_loss: 1.4825 - val_Quality_root_mean_squared_error: 1.0900 - val_Difficulty_root_mean_squared_error: 1.2176\n",
            "Epoch 13/20\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 3.2278 - Quality_loss: 1.5122 - Difficulty_loss: 1.7156 - Quality_root_mean_squared_error: 1.2297 - Difficulty_root_mean_squared_error: 1.3098 - val_loss: 2.6137 - val_Quality_loss: 1.1667 - val_Difficulty_loss: 1.4470 - val_Quality_root_mean_squared_error: 1.0802 - val_Difficulty_root_mean_squared_error: 1.2029\n",
            "Epoch 14/20\n",
            "100/100 [==============================] - 2s 15ms/step - loss: 3.1709 - Quality_loss: 1.4979 - Difficulty_loss: 1.6731 - Quality_root_mean_squared_error: 1.2239 - Difficulty_root_mean_squared_error: 1.2935 - val_loss: 2.6075 - val_Quality_loss: 1.1370 - val_Difficulty_loss: 1.4705 - val_Quality_root_mean_squared_error: 1.0663 - val_Difficulty_root_mean_squared_error: 1.2127\n",
            "Epoch 15/20\n",
            "100/100 [==============================] - 2s 17ms/step - loss: 3.0929 - Quality_loss: 1.4519 - Difficulty_loss: 1.6410 - Quality_root_mean_squared_error: 1.2050 - Difficulty_root_mean_squared_error: 1.2810 - val_loss: 2.5460 - val_Quality_loss: 1.1559 - val_Difficulty_loss: 1.3901 - val_Quality_root_mean_squared_error: 1.0751 - val_Difficulty_root_mean_squared_error: 1.1790\n",
            "Epoch 16/20\n",
            "100/100 [==============================] - 2s 15ms/step - loss: 3.0317 - Quality_loss: 1.4250 - Difficulty_loss: 1.6067 - Quality_root_mean_squared_error: 1.1937 - Difficulty_root_mean_squared_error: 1.2676 - val_loss: 2.4791 - val_Quality_loss: 1.1097 - val_Difficulty_loss: 1.3695 - val_Quality_root_mean_squared_error: 1.0534 - val_Difficulty_root_mean_squared_error: 1.1702\n",
            "Epoch 17/20\n",
            "100/100 [==============================] - 1s 13ms/step - loss: 2.9864 - Quality_loss: 1.3960 - Difficulty_loss: 1.5903 - Quality_root_mean_squared_error: 1.1815 - Difficulty_root_mean_squared_error: 1.2611 - val_loss: 2.5411 - val_Quality_loss: 1.1451 - val_Difficulty_loss: 1.3960 - val_Quality_root_mean_squared_error: 1.0701 - val_Difficulty_root_mean_squared_error: 1.1815\n",
            "Epoch 18/20\n",
            "100/100 [==============================] - 1s 13ms/step - loss: 2.9153 - Quality_loss: 1.3545 - Difficulty_loss: 1.5607 - Quality_root_mean_squared_error: 1.1638 - Difficulty_root_mean_squared_error: 1.2493 - val_loss: 2.4791 - val_Quality_loss: 1.1328 - val_Difficulty_loss: 1.3463 - val_Quality_root_mean_squared_error: 1.0643 - val_Difficulty_root_mean_squared_error: 1.1603\n",
            "Epoch 19/20\n",
            "100/100 [==============================] - 1s 14ms/step - loss: 2.8810 - Quality_loss: 1.3350 - Difficulty_loss: 1.5460 - Quality_root_mean_squared_error: 1.1554 - Difficulty_root_mean_squared_error: 1.2434 - val_loss: 2.5048 - val_Quality_loss: 1.1605 - val_Difficulty_loss: 1.3443 - val_Quality_root_mean_squared_error: 1.0772 - val_Difficulty_root_mean_squared_error: 1.1594\n",
            "Epoch 20/20\n",
            "100/100 [==============================] - 1s 13ms/step - loss: 2.8296 - Quality_loss: 1.2941 - Difficulty_loss: 1.5355 - Quality_root_mean_squared_error: 1.1376 - Difficulty_root_mean_squared_error: 1.2391 - val_loss: 2.4278 - val_Quality_loss: 1.0943 - val_Difficulty_loss: 1.3335 - val_Quality_root_mean_squared_error: 1.0461 - val_Difficulty_root_mean_squared_error: 1.1548\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 2.3738 - Quality_loss: 1.0412 - Difficulty_loss: 1.3326 - Quality_root_mean_squared_error: 1.0204 - Difficulty_root_mean_squared_error: 1.1544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above piece of code fits the compiled model to the training data with 20 epochs and batch size of 128, using 20% of the training data as validation set. The losses are evaluated on the test set."
      ],
      "metadata": {
        "id": "VCIXw0kCJQHo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#save the model\n",
        "model.save('student_rating_regression_model.h5')"
      ],
      "metadata": {
        "id": "E6fvxX6XRSbx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get Quality Root mean squared error\n",
        "print('Root Mean Squared error Of Quality:', loss_error_matrix[3])\n",
        "\n",
        "# Get Difficulty Root mean squared error\n",
        "print('Root Mean Squared error Of Difficulty:', loss_error_matrix[4])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0lnZeCWx9jT",
        "outputId": "82f0571c-3397-4a98-88a6-7bfafdbcab7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Root Mean Squared error Of Quality: 1.0203790664672852\n",
            "Root Mean Squared error Of Difficulty: 1.154398798942566\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ask user for input\n",
        "user_input = input(\"Enter your comment: \")\n",
        "# Clean user input\n",
        "user_input = CleanText_and_Vectorize(user_input)\n",
        "user_input = np.expand_dims(user_input, axis=0)\n",
        "# Pad user input sequence to have the same length as the training data\n",
        "user_input = pad_sequences(user_input, maxlen=max_length,dtype='float32', padding='post', truncating='post')\n",
        "\n",
        "# Predict the rating using the trained model\n",
        "rating = model.predict(user_input)\n",
        "# Print the predicted rating\n",
        "print('Quality: %.1f' % min(rating[0],5))\n",
        "print('Difficulty: %.1f' % min(rating[1],5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PI_marOg0LXr",
        "outputId": "a3e710cd-1d77-41b5-baea-71cb4efe3b62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your comment: A brilliant proffessor, makes the topic interesting through class discussions. Although powerpoints would help in preparation for exams. His trivia questions are not hard if you pay attention in class. He is a very nice and understaning professor. The study guides he provides for exams are very helpful so long as you study well! Take him.\n",
            "1/1 [==============================] - 1s 565ms/step\n",
            "Quality: 4.6\n",
            "Difficulty: 3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **--- END OF SECTION 8 ---**"
      ],
      "metadata": {
        "id": "7Tf6vNpkRwEZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Approach 9**\n",
        "---\n",
        "## **REGRESSION MODEL - 3**\n",
        "---\n",
        "### This model treats the assignment as a regression problem and trains and utilizes a brand new word embedding, utilizing tensorflow.keras.layers.TextVectorization to both clean the data and generate the actual sequences, training on both the .csv and .json RateMyProfessor data together."
      ],
      "metadata": {
        "id": "t7Mi_DKyNPw4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the required libraries.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding, Dropout, TextVectorization"
      ],
      "metadata": {
        "id": "bBfLiQGS_Ieh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YjAfcFG1_JAR",
        "outputId": "b8d7c234-096c-46a5-9f84-594e99d01f14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the data from both .json and .csv to data frame.\n",
        "import json\n",
        "df = pd.read_csv(r'/content/drive/My Drive/GROUP_PROJECT_2_ANN/Big Data Set from RateMyProfessor.com for Professors Teaching Evaluation/RateMyProfessor_Sample data.csv')\n",
        "my_file = open('/content/drive/My Drive/GROUP_PROJECT_2_ANN/all_reviews.json')\n",
        "json_dict = json.load(my_file)\n",
        "my_file.close()\n",
        "lastindex = 20000\n",
        "for i in range(len(json_dict)):\n",
        "  for j in range(len(json_dict[i])):\n",
        "    d = json_dict[i][j]\n",
        "    tempDataFrame = pd.DataFrame({\n",
        "        \"comments\": d[\"Comment\"],\n",
        "        \"student_star\": float(d[\"Quality\"]),\n",
        "        \"student_difficult\": float(d[\"Difficulty\"]),\n",
        "        \"professor_name\": d[\"professor\"]\n",
        "    }, index = [lastindex])\n",
        "    df = pd.concat([df, tempDataFrame])\n",
        "    lastindex = lastindex + 1"
      ],
      "metadata": {
        "id": "P-EYp_eE_Tuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(np.array(df['comments'])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DT6LmrF9HXqa",
        "outputId": "e1524730-52ab-4882-c28a-817ec07ddefd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23374\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Remove 'nan' values from 'comments' column\n",
        "df = df.dropna(subset=['comments'])\n",
        "\n",
        "# # Remove stopwords and convert to lowercase\n",
        "nltk.download('stopwords')\n",
        "stop_words = stopwords.words('english')\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = \" \".join([word for word in text.split() if word not in stop_words])\n",
        "    return text\n",
        "\n",
        "df['comments'] = df['comments'].apply(clean_text)"
      ],
      "metadata": {
        "id": "sy6NrhrjGq3W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2448069c-a28f-4d78-8ef5-ee3f49a38851"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Some of this was tinkering around with how exactly TextVectorization works\n",
        "comments_nparr = np.array(df['comments'])\n",
        "unique_comments = np.unique(comments_nparr)\n",
        "unique_comments_hack = unique_comments[1:]\n",
        "print(len(unique_comments))\n",
        "print(len(unique_comments_hack))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVQQjwCFKtZF",
        "outputId": "ec0a16ae-ccd7-4ed1-a736-2a2a34848812"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22102\n",
            "22101\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set parameters for model\n",
        "MAX_NB_WORDS = 100000\n",
        "MAX_SEQUENCE_LENGTH = 250\n",
        "EMBEDDING_DIM = 300\n",
        "\n",
        "# Tokenize words in 'comments' column\n",
        "encoder = TextVectorization(\n",
        "    vocabulary = unique_comments_hack,\n",
        "    standardize=\"lower_and_strip_punctuation\",\n",
        "    split=\"whitespace\",\n",
        "    output_sequence_length = MAX_SEQUENCE_LENGTH\n",
        ")\n",
        "altcoder = TextVectorization(\n",
        "    standardize=\"lower\",\n",
        "    split=\"whitespace\",\n",
        "    output_sequence_length = MAX_SEQUENCE_LENGTH,\n",
        "    max_tokens = MAX_NB_WORDS\n",
        ")\n",
        "altcoder.adapt(unique_comments_hack)\n",
        "print('Found %s unique tokens in Altcoder sample' % len(altcoder.get_vocabulary()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMW-Gje1GrfV",
        "outputId": "0892890d-d3d6-42e6-ad04-8e6a91606be4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 22103 unique tokens. In Encoder sample\n",
            "Found 37543 unique tokens. In Altcoder sample\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note how there's way more tokens in the \"altcoder\" than \"encoder\" - this is because the former is treating entire strings as its vocabulary, as opposed to words"
      ],
      "metadata": {
        "id": "hkjj8xJx60JT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into training and testing sets\n",
        "VALIDATION_SPLIT = 0.2\n",
        "\n",
        "X = df['comments'].values\n",
        "y_star = df['student_star'].values\n",
        "y_difficult = df['student_difficult'].values\n",
        "y_star_category = pd.get_dummies(df['student_star']).values\n",
        "y_difficult_category = pd.get_dummies(df['student_difficult']).values\n",
        "\n",
        "indices = np.arange(X.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "X = X[indices]\n",
        "y_star = y_star[indices]\n",
        "y_difficult = y_difficult[indices]\n",
        "y_star_category = y_star_category[indices]\n",
        "y_difficult_category = y_difficult_category[indices]\n",
        "num_validation_samples = int(VALIDATION_SPLIT * X.shape[0])\n",
        "\n",
        "X_train = X[:-num_validation_samples]\n",
        "y_train_star = y_star[:-num_validation_samples]\n",
        "y_train_difficult = y_difficult[:-num_validation_samples]\n",
        "y_train_star_category = y_star_category[:-num_validation_samples]\n",
        "y_train_difficult_category = y_difficult_category[:-num_validation_samples]\n",
        "X_test = X[-num_validation_samples:]\n",
        "y_test_star = y_star[-num_validation_samples:]\n",
        "y_test_difficult = y_difficult[-num_validation_samples:]\n",
        "y_test_star_category = y_star_category[-num_validation_samples:]\n",
        "y_test_difficult_category = y_difficult_category[-num_validation_samples:]"
      ],
      "metadata": {
        "id": "2dRUobWaGwf6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Treat outputs as a regression problem\n",
        "from tensorflow.keras.layers import GRU, Bidirectional\n",
        "\n",
        "star_model_regression = Sequential([\n",
        "    altcoder,\n",
        "    Embedding(input_dim=len(altcoder.get_vocabulary()), output_dim=100, input_length=MAX_SEQUENCE_LENGTH),\n",
        "    Bidirectional(GRU(100, dropout=0.4)),\n",
        "    Dense(128, activation=\"sigmoid\"),\n",
        "    Dropout(0.4),\n",
        "    Dense(64, activation=\"sigmoid\"),\n",
        "    Dropout(0.4),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "star_model_regression.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
        "print(star_model_regression.summary())\n",
        "star_model_regression.fit(X_train, y_train_star, validation_data=(X_test, y_test_star), epochs=20, batch_size=128)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vEElEohOG2zZ",
        "outputId": "2a87e2d2-9f80-4340-a735-a51775f5fd0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " text_vectorization_1 (TextV  (None, 250)              0         \n",
            " ectorization)                                                   \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, 250, 100)          3754300   \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 200)              121200    \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               25728     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 128)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,909,549\n",
            "Trainable params: 3,909,549\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/20\n",
            "147/147 [==============================] - 42s 218ms/step - loss: 3.5950 - accuracy: 0.1167 - val_loss: 2.1223 - val_accuracy: 0.1220\n",
            "Epoch 2/20\n",
            "147/147 [==============================] - 17s 117ms/step - loss: 2.5302 - accuracy: 0.1220 - val_loss: 2.1092 - val_accuracy: 0.1220\n",
            "Epoch 3/20\n",
            "147/147 [==============================] - 11s 72ms/step - loss: 2.4582 - accuracy: 0.1220 - val_loss: 2.0930 - val_accuracy: 0.1220\n",
            "Epoch 4/20\n",
            "147/147 [==============================] - 11s 72ms/step - loss: 2.3009 - accuracy: 0.1220 - val_loss: 1.5598 - val_accuracy: 0.1220\n",
            "Epoch 5/20\n",
            "147/147 [==============================] - 9s 56ms/step - loss: 1.5151 - accuracy: 0.1220 - val_loss: 1.0907 - val_accuracy: 0.1220\n",
            "Epoch 6/20\n",
            "147/147 [==============================] - 8s 52ms/step - loss: 1.0965 - accuracy: 0.1220 - val_loss: 1.0488 - val_accuracy: 0.1220\n",
            "Epoch 7/20\n",
            "147/147 [==============================] - 6s 42ms/step - loss: 0.9295 - accuracy: 0.1220 - val_loss: 1.0696 - val_accuracy: 0.1220\n",
            "Epoch 8/20\n",
            "147/147 [==============================] - 7s 49ms/step - loss: 0.8288 - accuracy: 0.1220 - val_loss: 1.1086 - val_accuracy: 0.1220\n",
            "Epoch 9/20\n",
            "147/147 [==============================] - 7s 50ms/step - loss: 0.7438 - accuracy: 0.1220 - val_loss: 1.0418 - val_accuracy: 0.1220\n",
            "Epoch 10/20\n",
            "147/147 [==============================] - 7s 47ms/step - loss: 0.7199 - accuracy: 0.1220 - val_loss: 1.0835 - val_accuracy: 0.1220\n",
            "Epoch 11/20\n",
            "147/147 [==============================] - 6s 43ms/step - loss: 0.6759 - accuracy: 0.1220 - val_loss: 1.0825 - val_accuracy: 0.1220\n",
            "Epoch 12/20\n",
            "147/147 [==============================] - 7s 45ms/step - loss: 0.6329 - accuracy: 0.1219 - val_loss: 1.0678 - val_accuracy: 0.1220\n",
            "Epoch 13/20\n",
            "147/147 [==============================] - 6s 38ms/step - loss: 0.6047 - accuracy: 0.1220 - val_loss: 1.0618 - val_accuracy: 0.1220\n",
            "Epoch 14/20\n",
            "147/147 [==============================] - 6s 40ms/step - loss: 0.5711 - accuracy: 0.1220 - val_loss: 1.1067 - val_accuracy: 0.1220\n",
            "Epoch 15/20\n",
            "147/147 [==============================] - 7s 45ms/step - loss: 0.5595 - accuracy: 0.1220 - val_loss: 1.0858 - val_accuracy: 0.1220\n",
            "Epoch 16/20\n",
            "147/147 [==============================] - 7s 51ms/step - loss: 0.5274 - accuracy: 0.1220 - val_loss: 1.1083 - val_accuracy: 0.1220\n",
            "Epoch 17/20\n",
            "147/147 [==============================] - 6s 41ms/step - loss: 0.5118 - accuracy: 0.1220 - val_loss: 1.1123 - val_accuracy: 0.1220\n",
            "Epoch 18/20\n",
            "147/147 [==============================] - 6s 40ms/step - loss: 0.4925 - accuracy: 0.1220 - val_loss: 1.1204 - val_accuracy: 0.1220\n",
            "Epoch 19/20\n",
            "147/147 [==============================] - 6s 39ms/step - loss: 0.4794 - accuracy: 0.1220 - val_loss: 1.1105 - val_accuracy: 0.1220\n",
            "Epoch 20/20\n",
            "147/147 [==============================] - 5s 37ms/step - loss: 0.4571 - accuracy: 0.1220 - val_loss: 1.1113 - val_accuracy: 0.1220\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa09a471b10>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "difficulty_model_regression = Sequential([\n",
        "    altcoder,\n",
        "    Embedding(input_dim=len(altcoder.get_vocabulary()), output_dim=100, input_length=MAX_SEQUENCE_LENGTH),\n",
        "    Bidirectional(GRU(100, dropout=0.4)),\n",
        "    Dense(128, activation=\"sigmoid\"),\n",
        "    Dropout(0.4),\n",
        "    Dense(64, activation=\"sigmoid\"),\n",
        "    Dropout(0.4),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "difficulty_model_regression.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
        "print(difficulty_model_regression.summary())\n",
        "difficulty_model_regression.fit(X_train, y_train_difficult, validation_data=(X_test, y_test_difficult), epochs=20, batch_size=128)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_q3MH-6oG4Oi",
        "outputId": "26bff54f-c9f6-4345-db9d-00e3ee66ff17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " text_vectorization_1 (TextV  (None, 250)              0         \n",
            " ectorization)                                                   \n",
            "                                                                 \n",
            " embedding_8 (Embedding)     (None, 250, 100)          3754300   \n",
            "                                                                 \n",
            " bidirectional_8 (Bidirectio  (None, 200)              121200    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dense_24 (Dense)            (None, 128)               25728     \n",
            "                                                                 \n",
            " dropout_16 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_25 (Dense)            (None, 64)                8256      \n",
            "                                                                 \n",
            " dropout_17 (Dropout)        (None, 64)                0         \n",
            "                                                                 \n",
            " dense_26 (Dense)            (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,909,549\n",
            "Trainable params: 3,909,549\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/20\n",
            "147/147 [==============================] - 29s 166ms/step - loss: 2.1612 - accuracy: 0.1559 - val_loss: 1.4320 - val_accuracy: 0.1581\n",
            "Epoch 2/20\n",
            "147/147 [==============================] - 14s 97ms/step - loss: 1.4790 - accuracy: 0.1566 - val_loss: 1.2845 - val_accuracy: 0.1581\n",
            "Epoch 3/20\n",
            "147/147 [==============================] - 12s 83ms/step - loss: 1.1766 - accuracy: 0.1566 - val_loss: 1.3300 - val_accuracy: 0.1581\n",
            "Epoch 4/20\n",
            "147/147 [==============================] - 7s 50ms/step - loss: 0.9892 - accuracy: 0.1566 - val_loss: 1.3995 - val_accuracy: 0.1581\n",
            "Epoch 5/20\n",
            "147/147 [==============================] - 7s 49ms/step - loss: 0.8425 - accuracy: 0.1566 - val_loss: 1.3977 - val_accuracy: 0.1581\n",
            "Epoch 6/20\n",
            "147/147 [==============================] - 7s 47ms/step - loss: 0.7408 - accuracy: 0.1566 - val_loss: 1.5339 - val_accuracy: 0.1581\n",
            "Epoch 7/20\n",
            "147/147 [==============================] - 7s 46ms/step - loss: 0.6760 - accuracy: 0.1566 - val_loss: 1.5854 - val_accuracy: 0.1581\n",
            "Epoch 8/20\n",
            "147/147 [==============================] - 7s 45ms/step - loss: 0.6261 - accuracy: 0.1565 - val_loss: 1.5701 - val_accuracy: 0.1581\n",
            "Epoch 9/20\n",
            "147/147 [==============================] - 7s 47ms/step - loss: 0.5746 - accuracy: 0.1566 - val_loss: 1.6095 - val_accuracy: 0.1581\n",
            "Epoch 10/20\n",
            "147/147 [==============================] - 7s 45ms/step - loss: 0.5377 - accuracy: 0.1566 - val_loss: 1.5825 - val_accuracy: 0.1581\n",
            "Epoch 11/20\n",
            "147/147 [==============================] - 6s 38ms/step - loss: 0.5205 - accuracy: 0.1566 - val_loss: 1.5544 - val_accuracy: 0.1581\n",
            "Epoch 12/20\n",
            "147/147 [==============================] - 6s 40ms/step - loss: 0.4927 - accuracy: 0.1566 - val_loss: 1.6004 - val_accuracy: 0.1581\n",
            "Epoch 13/20\n",
            "147/147 [==============================] - 6s 44ms/step - loss: 0.4722 - accuracy: 0.1566 - val_loss: 1.6864 - val_accuracy: 0.1581\n",
            "Epoch 14/20\n",
            "147/147 [==============================] - 6s 39ms/step - loss: 0.4479 - accuracy: 0.1566 - val_loss: 1.6074 - val_accuracy: 0.1581\n",
            "Epoch 15/20\n",
            "147/147 [==============================] - 6s 38ms/step - loss: 0.4281 - accuracy: 0.1566 - val_loss: 1.5792 - val_accuracy: 0.1581\n",
            "Epoch 16/20\n",
            "147/147 [==============================] - 6s 39ms/step - loss: 0.4128 - accuracy: 0.1566 - val_loss: 1.5801 - val_accuracy: 0.1581\n",
            "Epoch 17/20\n",
            "147/147 [==============================] - 6s 38ms/step - loss: 0.3964 - accuracy: 0.1566 - val_loss: 1.6782 - val_accuracy: 0.1581\n",
            "Epoch 18/20\n",
            "147/147 [==============================] - 7s 46ms/step - loss: 0.3887 - accuracy: 0.1566 - val_loss: 1.6281 - val_accuracy: 0.1581\n",
            "Epoch 19/20\n",
            "147/147 [==============================] - 5s 36ms/step - loss: 0.3644 - accuracy: 0.1566 - val_loss: 1.5779 - val_accuracy: 0.1581\n",
            "Epoch 20/20\n",
            "147/147 [==============================] - 7s 45ms/step - loss: 0.3662 - accuracy: 0.1566 - val_loss: 1.6412 - val_accuracy: 0.1581\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa092f743a0>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Alternate Model - 2\n",
        "\n",
        "### **This model treats the problem as a classification problem. Otherwise, it is exactly the same as before**\n",
        "\n"
      ],
      "metadata": {
        "id": "c3FPgkNz6KJM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import GRU, Bidirectional\n",
        "\n",
        "star_model_classification = Sequential([\n",
        "    altcoder,\n",
        "    Embedding(input_dim=len(altcoder.get_vocabulary()), output_dim=100, input_length=MAX_SEQUENCE_LENGTH),\n",
        "    Bidirectional(GRU(100, dropout=0.4)),\n",
        "    Dense(128, activation=\"sigmoid\"),\n",
        "    Dropout(0.4),\n",
        "    Dense(64, activation=\"sigmoid\"),\n",
        "    Dropout(0.4),\n",
        "    Dense(9, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "star_model_classification.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(star_model_classification.summary())\n",
        "star_model_classification.fit(X_train, y_train_star_category, validation_data=(X_test, y_test_star_category), epochs=20, batch_size=128)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQ2oXT_7DLcJ",
        "outputId": "bd370c39-e249-4482-ea39-f83bad6afef3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " text_vectorization_5 (TextV  (None, 250)              0         \n",
            " ectorization)                                                   \n",
            "                                                                 \n",
            " embedding_6 (Embedding)     (None, 250, 100)          3754300   \n",
            "                                                                 \n",
            " bidirectional_6 (Bidirectio  (None, 200)              121200    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dense_18 (Dense)            (None, 128)               25728     \n",
            "                                                                 \n",
            " dropout_12 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_19 (Dense)            (None, 64)                8256      \n",
            "                                                                 \n",
            " dropout_13 (Dropout)        (None, 64)                0         \n",
            "                                                                 \n",
            " dense_20 (Dense)            (None, 9)                 585       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,910,069\n",
            "Trainable params: 3,910,069\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/20\n",
            "147/147 [==============================] - 28s 154ms/step - loss: 1.9305 - accuracy: 0.3535 - val_loss: 1.7144 - val_accuracy: 0.4175\n",
            "Epoch 2/20\n",
            "147/147 [==============================] - 15s 101ms/step - loss: 1.6369 - accuracy: 0.4405 - val_loss: 1.6412 - val_accuracy: 0.4278\n",
            "Epoch 3/20\n",
            "147/147 [==============================] - 10s 67ms/step - loss: 1.5143 - accuracy: 0.4619 - val_loss: 1.6298 - val_accuracy: 0.4216\n",
            "Epoch 4/20\n",
            "147/147 [==============================] - 11s 73ms/step - loss: 1.4088 - accuracy: 0.4801 - val_loss: 1.6526 - val_accuracy: 0.4074\n",
            "Epoch 5/20\n",
            "147/147 [==============================] - 8s 53ms/step - loss: 1.3175 - accuracy: 0.5125 - val_loss: 1.7635 - val_accuracy: 0.4023\n",
            "Epoch 6/20\n",
            "147/147 [==============================] - 8s 52ms/step - loss: 1.2331 - accuracy: 0.5401 - val_loss: 1.8044 - val_accuracy: 0.4030\n",
            "Epoch 7/20\n",
            "147/147 [==============================] - 6s 44ms/step - loss: 1.1535 - accuracy: 0.5707 - val_loss: 1.8457 - val_accuracy: 0.3961\n",
            "Epoch 8/20\n",
            "147/147 [==============================] - 5s 37ms/step - loss: 1.0763 - accuracy: 0.6014 - val_loss: 1.9810 - val_accuracy: 0.3929\n",
            "Epoch 9/20\n",
            "147/147 [==============================] - 6s 39ms/step - loss: 1.0259 - accuracy: 0.6177 - val_loss: 1.9998 - val_accuracy: 0.3621\n",
            "Epoch 10/20\n",
            "147/147 [==============================] - 6s 38ms/step - loss: 0.9745 - accuracy: 0.6370 - val_loss: 2.1038 - val_accuracy: 0.3642\n",
            "Epoch 11/20\n",
            "147/147 [==============================] - 6s 42ms/step - loss: 0.9359 - accuracy: 0.6499 - val_loss: 2.1808 - val_accuracy: 0.3533\n",
            "Epoch 12/20\n",
            "147/147 [==============================] - 7s 47ms/step - loss: 0.8870 - accuracy: 0.6651 - val_loss: 2.2982 - val_accuracy: 0.3694\n",
            "Epoch 13/20\n",
            "147/147 [==============================] - 6s 40ms/step - loss: 0.8600 - accuracy: 0.6773 - val_loss: 2.4207 - val_accuracy: 0.3587\n",
            "Epoch 14/20\n",
            "147/147 [==============================] - 6s 44ms/step - loss: 0.8200 - accuracy: 0.6894 - val_loss: 2.4418 - val_accuracy: 0.3584\n",
            "Epoch 15/20\n",
            "147/147 [==============================] - 6s 38ms/step - loss: 0.7883 - accuracy: 0.7036 - val_loss: 2.5732 - val_accuracy: 0.3385\n",
            "Epoch 16/20\n",
            "147/147 [==============================] - 6s 42ms/step - loss: 0.7666 - accuracy: 0.7122 - val_loss: 2.6273 - val_accuracy: 0.3340\n",
            "Epoch 17/20\n",
            "147/147 [==============================] - 6s 40ms/step - loss: 0.7558 - accuracy: 0.7169 - val_loss: 2.7551 - val_accuracy: 0.3432\n",
            "Epoch 18/20\n",
            "147/147 [==============================] - 6s 39ms/step - loss: 0.7248 - accuracy: 0.7277 - val_loss: 2.8292 - val_accuracy: 0.3407\n",
            "Epoch 19/20\n",
            "147/147 [==============================] - 5s 37ms/step - loss: 0.6980 - accuracy: 0.7382 - val_loss: 2.8978 - val_accuracy: 0.3373\n",
            "Epoch 20/20\n",
            "147/147 [==============================] - 6s 39ms/step - loss: 0.6803 - accuracy: 0.7490 - val_loss: 2.9814 - val_accuracy: 0.3238\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa09fa1a1d0>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "difficulty_model_classification = Sequential([\n",
        "    altcoder,\n",
        "    Embedding(input_dim=len(altcoder.get_vocabulary()), output_dim=100, input_length=MAX_SEQUENCE_LENGTH),\n",
        "    Bidirectional(GRU(100, dropout=0.4)),\n",
        "    Dense(128, activation=\"sigmoid\"),\n",
        "    Dropout(0.4),\n",
        "    Dense(64, activation=\"sigmoid\"),\n",
        "    Dropout(0.4),\n",
        "    Dense(5, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "difficulty_model_classification.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(difficulty_model_classification.summary())\n",
        "difficulty_model_classification.fit(X_train, y_train_difficult_category, validation_data=(X_test, y_test_difficult_category), epochs=20, batch_size=128)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8IUsAZ6G4dO",
        "outputId": "a27166d2-2ec7-4d43-9021-9788e78821cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " text_vectorization_1 (TextV  (None, 250)              0         \n",
            " ectorization)                                                   \n",
            "                                                                 \n",
            " embedding_10 (Embedding)    (None, 250, 100)          3754300   \n",
            "                                                                 \n",
            " bidirectional_10 (Bidirecti  (None, 200)              121200    \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dense_30 (Dense)            (None, 128)               25728     \n",
            "                                                                 \n",
            " dropout_20 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_31 (Dense)            (None, 64)                8256      \n",
            "                                                                 \n",
            " dropout_21 (Dropout)        (None, 64)                0         \n",
            "                                                                 \n",
            " dense_32 (Dense)            (None, 5)                 325       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,909,809\n",
            "Trainable params: 3,909,809\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/20\n",
            "147/147 [==============================] - 29s 156ms/step - loss: 1.6545 - accuracy: 0.2432 - val_loss: 1.5854 - val_accuracy: 0.2731\n",
            "Epoch 2/20\n",
            "147/147 [==============================] - 15s 101ms/step - loss: 1.5918 - accuracy: 0.2631 - val_loss: 1.5837 - val_accuracy: 0.2731\n",
            "Epoch 3/20\n",
            "147/147 [==============================] - 12s 79ms/step - loss: 1.5349 - accuracy: 0.3020 - val_loss: 1.4901 - val_accuracy: 0.3144\n",
            "Epoch 4/20\n",
            "147/147 [==============================] - 9s 61ms/step - loss: 1.4082 - accuracy: 0.3561 - val_loss: 1.4756 - val_accuracy: 0.3300\n",
            "Epoch 5/20\n",
            "147/147 [==============================] - 7s 46ms/step - loss: 1.2818 - accuracy: 0.4249 - val_loss: 1.5160 - val_accuracy: 0.3409\n",
            "Epoch 6/20\n",
            "147/147 [==============================] - 8s 54ms/step - loss: 1.1582 - accuracy: 0.5039 - val_loss: 1.5923 - val_accuracy: 0.3501\n",
            "Epoch 7/20\n",
            "147/147 [==============================] - 6s 44ms/step - loss: 1.0415 - accuracy: 0.5751 - val_loss: 1.7081 - val_accuracy: 0.3520\n",
            "Epoch 8/20\n",
            "147/147 [==============================] - 7s 49ms/step - loss: 0.9119 - accuracy: 0.6422 - val_loss: 1.8103 - val_accuracy: 0.3548\n",
            "Epoch 9/20\n",
            "147/147 [==============================] - 6s 41ms/step - loss: 0.8162 - accuracy: 0.6915 - val_loss: 1.9712 - val_accuracy: 0.3475\n",
            "Epoch 10/20\n",
            "147/147 [==============================] - 6s 44ms/step - loss: 0.7187 - accuracy: 0.7400 - val_loss: 2.0571 - val_accuracy: 0.3471\n",
            "Epoch 11/20\n",
            "147/147 [==============================] - 5s 37ms/step - loss: 0.6543 - accuracy: 0.7656 - val_loss: 2.2483 - val_accuracy: 0.3313\n",
            "Epoch 12/20\n",
            "147/147 [==============================] - 6s 42ms/step - loss: 0.5816 - accuracy: 0.7964 - val_loss: 2.3543 - val_accuracy: 0.3381\n",
            "Epoch 13/20\n",
            "147/147 [==============================] - 6s 43ms/step - loss: 0.5420 - accuracy: 0.8111 - val_loss: 2.4325 - val_accuracy: 0.3385\n",
            "Epoch 14/20\n",
            "147/147 [==============================] - 6s 38ms/step - loss: 0.4919 - accuracy: 0.8320 - val_loss: 2.5360 - val_accuracy: 0.3306\n",
            "Epoch 15/20\n",
            "147/147 [==============================] - 6s 41ms/step - loss: 0.4564 - accuracy: 0.8413 - val_loss: 2.6723 - val_accuracy: 0.3289\n",
            "Epoch 16/20\n",
            "147/147 [==============================] - 6s 40ms/step - loss: 0.4328 - accuracy: 0.8506 - val_loss: 2.7033 - val_accuracy: 0.3366\n",
            "Epoch 17/20\n",
            "147/147 [==============================] - 6s 40ms/step - loss: 0.4097 - accuracy: 0.8579 - val_loss: 2.7492 - val_accuracy: 0.3360\n",
            "Epoch 18/20\n",
            "147/147 [==============================] - 6s 39ms/step - loss: 0.3833 - accuracy: 0.8656 - val_loss: 2.9755 - val_accuracy: 0.3272\n",
            "Epoch 19/20\n",
            "147/147 [==============================] - 5s 36ms/step - loss: 0.3647 - accuracy: 0.8711 - val_loss: 2.9930 - val_accuracy: 0.3296\n",
            "Epoch 20/20\n",
            "147/147 [==============================] - 5s 34ms/step - loss: 0.3422 - accuracy: 0.8799 - val_loss: 3.0163 - val_accuracy: 0.3293\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa07e0b4b80>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Alternate Model - 3\n",
        "\n",
        "### **This model is exactly the same as the preceding model, only it utilizes tanh activation.**\n"
      ],
      "metadata": {
        "id": "t0AjtLAF6LyN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "star_model_final = Sequential([\n",
        "    altcoder,\n",
        "    Embedding(input_dim=len(altcoder.get_vocabulary()), output_dim=100, input_length=MAX_SEQUENCE_LENGTH),\n",
        "    Bidirectional(GRU(100, dropout=0.4)),\n",
        "    Dense(128, activation=\"tanh\"),\n",
        "    Dropout(0.4),\n",
        "    Dense(64, activation=\"tanh\"),\n",
        "    Dropout(0.4),\n",
        "    Dense(9, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "star_model_final.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(star_model_final.summary())\n",
        "star_model_final.fit(X_train, y_train_star_category, validation_data=(X_test, y_test_star_category), epochs=20, batch_size=128)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qCXaATFKm4r",
        "outputId": "b1132adf-d512-432b-f067-6347c97e8919"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " text_vectorization_1 (TextV  (None, 250)              0         \n",
            " ectorization)                                                   \n",
            "                                                                 \n",
            " embedding_11 (Embedding)    (None, 250, 100)          3754300   \n",
            "                                                                 \n",
            " bidirectional_11 (Bidirecti  (None, 200)              121200    \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dense_33 (Dense)            (None, 128)               25728     \n",
            "                                                                 \n",
            " dropout_22 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_34 (Dense)            (None, 64)                8256      \n",
            "                                                                 \n",
            " dropout_23 (Dropout)        (None, 64)                0         \n",
            "                                                                 \n",
            " dense_35 (Dense)            (None, 9)                 585       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,910,069\n",
            "Trainable params: 3,910,069\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/20\n",
            "147/147 [==============================] - 30s 167ms/step - loss: 1.7776 - accuracy: 0.4025 - val_loss: 1.6116 - val_accuracy: 0.4385\n",
            "Epoch 2/20\n",
            "147/147 [==============================] - 14s 95ms/step - loss: 1.4914 - accuracy: 0.4688 - val_loss: 1.5690 - val_accuracy: 0.4443\n",
            "Epoch 3/20\n",
            "147/147 [==============================] - 10s 65ms/step - loss: 1.3156 - accuracy: 0.5202 - val_loss: 1.6794 - val_accuracy: 0.4395\n",
            "Epoch 4/20\n",
            "147/147 [==============================] - 8s 57ms/step - loss: 1.1730 - accuracy: 0.5626 - val_loss: 1.7421 - val_accuracy: 0.4293\n",
            "Epoch 5/20\n",
            "147/147 [==============================] - 7s 49ms/step - loss: 1.0403 - accuracy: 0.6025 - val_loss: 1.9089 - val_accuracy: 0.3591\n",
            "Epoch 6/20\n",
            "147/147 [==============================] - 7s 51ms/step - loss: 0.9421 - accuracy: 0.6451 - val_loss: 2.1190 - val_accuracy: 0.3824\n",
            "Epoch 7/20\n",
            "147/147 [==============================] - 6s 42ms/step - loss: 0.8318 - accuracy: 0.6912 - val_loss: 2.2323 - val_accuracy: 0.3516\n",
            "Epoch 8/20\n",
            "147/147 [==============================] - 6s 44ms/step - loss: 0.7439 - accuracy: 0.7312 - val_loss: 2.4461 - val_accuracy: 0.3332\n",
            "Epoch 9/20\n",
            "147/147 [==============================] - 6s 39ms/step - loss: 0.6647 - accuracy: 0.7659 - val_loss: 2.6351 - val_accuracy: 0.3625\n",
            "Epoch 10/20\n",
            "147/147 [==============================] - 6s 44ms/step - loss: 0.5852 - accuracy: 0.7998 - val_loss: 2.7590 - val_accuracy: 0.3507\n",
            "Epoch 11/20\n",
            "147/147 [==============================] - 5s 37ms/step - loss: 0.5151 - accuracy: 0.8321 - val_loss: 3.0046 - val_accuracy: 0.3522\n",
            "Epoch 12/20\n",
            "147/147 [==============================] - 6s 43ms/step - loss: 0.4688 - accuracy: 0.8440 - val_loss: 3.0550 - val_accuracy: 0.3529\n",
            "Epoch 13/20\n",
            "147/147 [==============================] - 6s 39ms/step - loss: 0.4259 - accuracy: 0.8586 - val_loss: 3.2132 - val_accuracy: 0.3281\n",
            "Epoch 14/20\n",
            "147/147 [==============================] - 6s 40ms/step - loss: 0.3868 - accuracy: 0.8722 - val_loss: 3.3421 - val_accuracy: 0.3507\n",
            "Epoch 15/20\n",
            "147/147 [==============================] - 6s 43ms/step - loss: 0.3513 - accuracy: 0.8858 - val_loss: 3.3583 - val_accuracy: 0.3602\n",
            "Epoch 16/20\n",
            "147/147 [==============================] - 6s 40ms/step - loss: 0.3474 - accuracy: 0.8863 - val_loss: 3.4274 - val_accuracy: 0.3409\n",
            "Epoch 17/20\n",
            "147/147 [==============================] - 6s 43ms/step - loss: 0.3045 - accuracy: 0.9010 - val_loss: 3.6433 - val_accuracy: 0.3445\n",
            "Epoch 18/20\n",
            "147/147 [==============================] - 6s 41ms/step - loss: 0.2927 - accuracy: 0.9037 - val_loss: 3.5887 - val_accuracy: 0.3439\n",
            "Epoch 19/20\n",
            "147/147 [==============================] - 6s 39ms/step - loss: 0.2740 - accuracy: 0.9110 - val_loss: 3.7180 - val_accuracy: 0.3381\n",
            "Epoch 20/20\n",
            "147/147 [==============================] - 6s 40ms/step - loss: 0.2694 - accuracy: 0.9108 - val_loss: 3.8026 - val_accuracy: 0.3358\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa06063f610>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **--- END OF SECTION 9 ---**"
      ],
      "metadata": {
        "id": "t-30ZheHNPlY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ACCURACY COMPARISON TABLE**"
      ],
      "metadata": {
        "id": "4n9LJS0BB55V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Classification Models**\n",
        "\n",
        "All values are percentages and the highest seen during training or at some subsequent evaluate() step. Training accuracies may not reflect actual values due to presence of dropout within models, but are included for reference.\n",
        "\n",
        "| Approaches  | Quality Train Accuracy | Quality Test Accuracy | Difficulty Train Accuracy  | Difficulty Test Accuracy | JSON data | Embedding Dimension |  Embedding Trained |\n",
        "|:----------:|:---------:|:--------:|:----------:|:---------:|:--------:|:--------:|:--------:|\n",
        "|   1 - GloVe, LSTM  |   49.35   |  44.52   |   N/A |   N/A   |     | 100   | |  \n",
        "|   2 - GloVe, Deep GRU  |   42.19   |   43.34  |   N/A  |   N/A   |     | 100   | |  \n",
        "|   3 - GloVe, LSTM  |   81.95   |    46.01  |   N/A  |   N/A   |  X   | 100   |X |  \n",
        "|   4.1 - GloVe, Deep Bidirectional LSTM  |   76.25    |  46.97  |   82.56  |   38.03   |  X   |300  | X |\n",
        "|   4.2 - GloVe, Deep Bidirectional LSTM  |   53.98    |   46.44  |   N/A  |   N/A   |  X   | 300   |  |\n",
        "|   4.3 - GloVe, Deep Bidirectional LSTM  |   50.05   |  46.57  |   N/A  |   N/A   |  X   | 300   |  |\n",
        "|   4.4 - GloVe, Deep Bidirectional LSTM |   55.57   |  46.82 |   N/A  |   N/A   |  X   |300   |  |  \n",
        "|   4.5 - GloVe, Deep Bidirectional GRU |   49.49   |  47.06 |   N/A  |   N/A   |  X   |300   |  |  \n",
        "|   6 - One-Hot Encoding, Dense Network *(See note)|   N/A   |  N/A |   55.58  |   27.58   |    |N/A   | N/A |  N/A\n",
        "|   7 - GloVe, Bidirectional LSTM, Relative  **(See note) |   80.97   |  80.09 |   80.97  |   80.09   |     |300   |  |  \n",
        "|   9.2 - Custom Embedding, Bidirectional GRU |   74.90   |  42.78 |   87.99  |  35.48   |  X   | 100   | X |\n",
        "|   9.3 - Custom Embedding, Bidirectional GRU |   91.10   |  44.43 |    N/A |   N/A   |  X   | 100   | X |  \n",
        "\n",
        "*Note: While this is not an RNN, it is included as a baseline to demonstrate the relative accuracy of dense and recurrent networks\n",
        "\n",
        "**Note: This model actually predicts whether or not the difficulty or quality values are higher for a given comment. This is included as reference and to demonstrate how different questions lead to vastly different accuracies."
      ],
      "metadata": {
        "id": "u63qms8j234E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Regression Models**\n",
        "\n",
        "| Approaches  | Quality Train Loss | Quality Test Loss | Difficulty Train Loss | Difficulty Test Loss | JSON data | Embedding Dimension | Embedding Trained\n",
        "|:----------:|:---------:|:--------:|:----------:|:---------:|:--------:|:--------:|:--------:|\n",
        "|   5 - GloVe, Deep Bidirectional GRU |   0.8161   |  0.8127   |   N/A |   N/A   |  X   | 300   |\n",
        "|   8 - GloVe, LSTM, User Input Support |   1.2941   |   1.0412  |   1.5355 |    1.3326   |     | 300   |\n",
        "|   9.1 - Custom Embedding, Bidirectional GRU  |   0.4571   |   1.0418  |    0.3644  |   1.2845  |  X   | 100   | X\n"
      ],
      "metadata": {
        "id": "gMgcHubU26wH"
      }
    }
  ]
}